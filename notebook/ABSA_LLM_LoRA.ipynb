{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"ABSA_LLM_LoRA","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/absa-llm-lora.41987b78-d4d8-4870-a0a4-f38a355784ae.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250626/auto/storage/goog4_request&X-Goog-Date=20250626T123532Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3c29be67e828430be8a3080bc1f957f9ecc09df6b8ba1279109f3f34a2f2c2bf17736dcb1f8fbb79a64a26b010060c8452ba4e1b97a147686b8ade683fd577ed3ca44c1e068b14043e0e7c13a8e836367d1963c344f01087b5a05a724e3b199289e3cf857ac0f065329c03276ce2277d5051dad724a5436fc2ac80186e5c8499afc7c57a3cbde62d4f173259462b190326649801b53bd28b93e67463d1a8773f96725023ae0a28c87227ff87cc0cbcd144b7b7ba00d37234fd4ffc6319d3e5868e746bf16acd9405ab8d32302e93cbd38221cd5780a43b0c18ee6afc2999fa671fcd0f6895354c6a3ea6aefbf126f3a8ea8ead702a386c88e10250c5609a9f50","timestamp":1751043497204}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install torch transformers peft datasets scikit-learn matplotlib seaborn textblob pandas"],"metadata":{"trusted":true,"id":"ug8r6Xh-QWbE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    TrainingArguments, Trainer\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    classification_report, precision_recall_fscore_support, accuracy_score\n",")\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textblob import TextBlob\n","import re\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","try:\n","    from peft import LoraConfig, get_peft_model, TaskType\n","    PEFT_AVAILABLE = True\n","except ImportError:\n","    print(\"PEFT not available. Using standard fine-tuning.\")\n","    PEFT_AVAILABLE = False"],"metadata":{"id":"RgQajHGMRG54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# EDA Functions\n","# =========================\n","def executive_summary(df):\n","    print(\"Executive Summary of E-commerce Reviews\")\n","    print(f\"Total reviews analyzed: {len(df)}\")\n","    pos = (df['sentiment'] == 'positive').mean()\n","    neu = (df['sentiment'] == 'neutral').mean()\n","    neg = (df['sentiment'] == 'negative').mean()\n","    print(f\"Positive reviews: {pos:.1%}\")\n","    print(f\"Neutral reviews: {neu:.1%}\")\n","    print(f\"Negative reviews: {neg:.1%}\")\n","    aspects = ['aspect_fit', 'aspect_quality', 'aspect_style', 'aspect_comfort', 'aspect_price']\n","    negative_df = df[df['sentiment'] == 'negative']\n","    if len(negative_df) > 0:\n","        top_aspects = negative_df[aspects].sum().sort_values(ascending=False)\n","        print(f\"Top reasons for negative reviews: {list(top_aspects.index.str.replace('aspect_', '').str.title())[:3]}\")\n","    print(\"\\nActionable Insights:\")\n","    print(\"- Focus on fit and quality issues that drive most negative sentiment\")\n","    print(\"- Customers appreciate well-fitting, comfortable clothes—highlight these in marketing\")\n","    print(\"- Addressing sizing and quality complaints can reduce negative reviews significantly\\n\")\n","\n","def plot_overall_sentiment(df):\n","    plt.figure(figsize=(7, 4))\n","    sns.countplot(data=df, x='sentiment', order=['positive', 'neutral', 'negative'], palette='viridis')\n","    plt.title(\"Overall Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Number of Reviews\")\n","    plt.show()\n","\n","def plot_negative_aspect_breakdown(df):\n","    aspects = ['aspect_fit', 'aspect_quality', 'aspect_style', 'aspect_comfort', 'aspect_price']\n","    negative_df = df[df['sentiment'] == 'negative']\n","    if len(negative_df) > 0:\n","        aspect_counts = negative_df[aspects].sum().sort_values(ascending=False)\n","        plt.figure(figsize=(8, 5))\n","        sns.barplot(x=aspect_counts.index.str.replace('aspect_', '').str.title(),\n","                    y=aspect_counts.values, palette=\"mako\")\n","        plt.title(\"Top Aspects in Negative Reviews\")\n","        plt.xlabel(\"Aspect\")\n","        plt.ylabel(\"Count\")\n","        plt.xticks(rotation=45)\n","        plt.tight_layout()\n","        plt.show()\n","\n","def plot_aspect_sentiment_heatmap(df):\n","    aspects = ['aspect_fit', 'aspect_quality', 'aspect_style', 'aspect_comfort', 'aspect_price']\n","    aspect_sentiment = {}\n","    for aspect in aspects:\n","        aspect_sentiment[aspect] = df.groupby('sentiment')[aspect].sum()\n","    aspect_sentiment_df = pd.DataFrame(aspect_sentiment)\n","    aspect_sentiment_df = aspect_sentiment_df.rename(columns=lambda x: x.replace('aspect_', '').title())\n","    plt.figure(figsize=(9, 4))\n","    sns.heatmap(aspect_sentiment_df.T, annot=True, fmt='d', cmap='coolwarm')\n","    plt.title('Mentions of Each Aspect per Sentiment')\n","    plt.xlabel('Sentiment')\n","    plt.ylabel('Aspect')\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"gbeeH2tuRNHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Custom Dataset Class\n","# =========================\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(\n","            text,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n"],"metadata":{"id":"wsKa7JEQRrud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Business Insights/Aspect Extraction\n","# =========================\n","\n","class BusinessInsightExtractor:\n","    def __init__(self):\n","        self.aspects = {\n","            'fit': ['fit', 'fits', 'fitting', 'size', 'sized', 'sizing', 'tight', 'loose', 'small', 'large', 'runs'],\n","            'quality': ['quality', 'material', 'fabric', 'cotton', 'polyester', 'durable', 'cheap', 'flimsy', 'well made', 'poorly made'],\n","            'style': ['style', 'design', 'look', 'appearance', 'fashionable', 'trendy', 'color', 'pattern', 'cute', 'ugly'],\n","            'comfort': ['comfort', 'comfortable', 'soft', 'cozy', 'breathable', 'itchy', 'scratchy', 'harsh'],\n","            'price': ['price', 'cost', 'expensive', 'cheap', 'affordable', 'value', 'worth', 'money', 'overpriced']\n","        }\n","    def extract_opinion_phrases(self, text):\n","        contrast_pattern = r'\\\\b(but|however|although|though|yet|while|whereas)\\\\b'\n","        parts = re.split(contrast_pattern, text, flags=re.IGNORECASE)\n","        phrases = []\n","        for i in range(0, len(parts), 2):\n","            if i < len(parts):\n","                phrase = parts[i].strip()\n","                if phrase:\n","                    phrases.append(phrase)\n","        return phrases\n","    def analyze_aspect_sentiment(self, text):\n","        text_lower = text.lower()\n","        aspect_sentiments = {}\n","        for aspect, keywords in self.aspects.items():\n","            aspect_mentioned = any(keyword in text_lower for keyword in keywords)\n","            if aspect_mentioned:\n","                blob = TextBlob(text)\n","                sentiment_score = blob.sentiment.polarity\n","                if sentiment_score > 0.1:\n","                    sentiment = 'positive'\n","                elif sentiment_score < -0.1:\n","                    sentiment = 'negative'\n","                else:\n","                    sentiment = 'neutral'\n","                aspect_sentiments[aspect] = {\n","                    'sentiment': sentiment,\n","                    'score': sentiment_score,\n","                    'mentioned_keywords': [kw for kw in keywords if kw in text_lower]\n","                }\n","        return aspect_sentiments\n","\n","    def extract_mixed_reviews(self, df):\n","        mixed_reviews = []\n","        for idx, row in df.iterrows():\n","            text = row['review_text']\n","            rating = row['rating']\n","            if rating >= 4:\n","                phrases = self.extract_opinion_phrases(text)\n","                if len(phrases) > 1:\n","                    aspect_analysis = self.analyze_aspect_sentiment(text)\n","                    sentiments = [asp['sentiment'] for asp in aspect_analysis.values()]\n","                    if 'positive' in sentiments and 'negative' in sentiments:\n","                        mixed_reviews.append({\n","                            'review_id': idx,\n","                            'rating': rating,\n","                            'text': text,\n","                            'phrases': phrases,\n","                            'aspect_analysis': aspect_analysis\n","                        })\n","        return mixed_reviews\n","\n","    def generate_business_recommendations(self, df):\n","        recommendations = {}\n","        negative_reviews = df[df['rating'] <= 2]\n","        if len(negative_reviews) == 0:\n","            return recommendations\n","        for aspect in self.aspects.keys():\n","            aspect_issues = []\n","            aspect_count = 0\n","            for _, row in negative_reviews.iterrows():\n","                text = row['review_text']\n","                aspect_analysis = self.analyze_aspect_sentiment(text)\n","                if aspect in aspect_analysis and aspect_analysis[aspect]['sentiment'] == 'negative':\n","                    aspect_count += 1\n","                    aspect_issues.append({\n","                        'text': text,\n","                        'keywords': aspect_analysis[aspect]['mentioned_keywords']\n","                    })\n","            if aspect_count > 0:\n","                recs = self._generate_aspect_recommendations(aspect)\n","                recommendations[aspect] = {\n","                    'issue_count': aspect_count,\n","                    'percentage': (aspect_count / len(negative_reviews)) * 100,\n","                    'recommendations': recs,\n","                    'sample_issues': aspect_issues[:3]\n","                }\n","        return recommendations\n","    def _generate_aspect_recommendations(self, aspect):\n","        recommendations = {\n","            'fit': [\n","                \"Update sizing charts with detailed measurements\",\n","                \"Add customer fit photos and reviews\",\n","                \"Implement virtual try-on technology\",\n","                \"Offer free returns for sizing issues\",\n","                \"Create size comparison guides\"\n","            ],\n","            'quality': [\n","                \"Review material sourcing and supplier quality\",\n","                \"Implement stricter quality control processes\",\n","                \"Add detailed material descriptions to product pages\",\n","                \"Offer extended warranties\",\n","                \"Provide care instructions to extend product life\"\n","            ],\n","            'style': [\n","                \"Conduct customer style preference surveys\",\n","                \"A/B test new designs before full production\",\n","                \"Expand color and pattern options\",\n","                \"Create seasonal trend reports\",\n","                \"Collaborate with fashion influencers for feedback\"\n","            ],\n","            'comfort': [\n","                \"Test products with focus groups for comfort\",\n","                \"Use softer, more breathable materials\",\n","                \"Add comfort features (padding, seamless construction)\",\n","                \"Provide comfort guarantees\",\n","                \"Create comfort-focused product lines\"\n","            ],\n","            'price': [\n","                \"Conduct competitive pricing analysis\",\n","                \"Create value proposition messaging\",\n","                \"Offer loyalty discounts and rewards\",\n","                \"Bundle products for better value\",\n","                \"Implement dynamic pricing strategies\"\n","            ]\n","        }\n","        return recommendations.get(aspect, [\"Review and improve this aspect based on customer feedback\"])\n"],"metadata":{"id":"QZ6mE6PHSTdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Aspect Bias Sentimental Analysis LLM Pipeline\n","# =========================\n","\n","class ABSALLMPipeline:\n","    def __init__(self, model_name='distilbert-base-uncased', use_lora=True):\n","        self.model_name = model_name\n","        self.use_lora = use_lora and PEFT_AVAILABLE\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        print(f\"Using device: {self.device}\")\n","        print(f\"Fine-tuning method: {'LoRA' if self.use_lora else 'Full Fine-tuning'}\")\n","        self.tokenizer = None\n","        self.model = None\n","        self.peft_model = None\n","        self.label_encoder = LabelEncoder()\n","        self.trainer = None\n","        self.insight_extractor = BusinessInsightExtractor()\n","        if self.use_lora:\n","            self.lora_config = LoraConfig(\n","                task_type=TaskType.SEQ_CLS,\n","                inference_mode=False,\n","                r=16,\n","                lora_alpha=32,\n","                lora_dropout=0.1,\n","                target_modules=self._get_target_modules(model_name),\n","                bias=\"none\",\n","            )\n","    def _get_target_modules(self, model_name):\n","        if 'distilbert' in model_name.lower():\n","            return [\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]\n","        elif 'roberta' in model_name.lower() or 'bert' in model_name.lower():\n","            return [\"query\", \"value\", \"key\", \"dense\"]\n","        else:\n","            return [\"query\", \"value\"]\n","    def load_and_preprocess_data(self):\n","        print(\"Loading real dataset...\")\n","        ds = load_dataset(\"Censius-AI/ECommerce-Women-Clothing-Reviews\")\n","        df = pd.DataFrame(ds[\"train\"])\n","        df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]\n","        df = df.dropna(subset=['review_text'])\n","        df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n","        # 80% train, 20% test\n","        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating'])\n","        print(f\"Loaded {len(train_df)} training and {len(test_df)} testing samples.\")\n","        def create_sentiment_label(rating):\n","            if rating <= 2:\n","                return 'negative'\n","            elif rating == 3:\n","                return 'neutral'\n","            else:\n","                return 'positive'\n","        train_df['sentiment'] = train_df['rating'].apply(create_sentiment_label)\n","        test_df['sentiment'] = test_df['rating'].apply(create_sentiment_label)\n","        return train_df, test_df\n","    def extract_aspects_and_enhance_text(self, df):\n","        aspects = self.insight_extractor.aspects\n","        for aspect, keywords in aspects.items():\n","            pattern = '|'.join([re.escape(kw) for kw in keywords])\n","            df[f'aspect_{aspect}'] = df['review_text'].str.lower().str.contains(pattern, na=False, regex=True)\n","        def enhance_text_with_aspects(row):\n","            text = row['review_text']\n","            mentioned_aspects = []\n","            for aspect in aspects.keys():\n","                if row[f'aspect_{aspect}']:\n","                    mentioned_aspects.append(aspect)\n","            if mentioned_aspects:\n","                aspect_info = f\"[ASPECTS: {', '.join(mentioned_aspects)}] \"\n","                return aspect_info + text\n","            return text\n","        df['enhanced_text'] = df.apply(enhance_text_with_aspects, axis=1)\n","        return df\n","    def prepare_model_and_tokenizer(self, num_labels):\n","        print(f\"Loading {self.model_name} model and tokenizer...\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.model = AutoModelForSequenceClassification.from_pretrained(\n","            self.model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n","        self.model.resize_token_embeddings(len(self.tokenizer))\n","        if self.use_lora:\n","            print(\"Applying LoRA configuration...\")\n","            self.peft_model = get_peft_model(self.model, self.lora_config)\n","            self.peft_model.print_trainable_parameters()\n","            self.peft_model.to(self.device)\n","            self.model = self.peft_model\n","        else:\n","            print(\"Using full fine-tuning...\")\n","            self.model.to(self.device)\n","            total_params = sum(p.numel() for p in self.model.parameters())\n","            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","            print(f\"Total parameters: {total_params:,}\")\n","            print(f\"Trainable parameters: {trainable_params:,}\")\n","    def create_datasets(self, texts, labels):\n","        return ReviewDataset(texts, labels, self.tokenizer)\n","    def compute_metrics(self, eval_pred):\n","        predictions, labels = eval_pred\n","        predictions = np.argmax(predictions, axis=1)\n","        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","        accuracy = accuracy_score(labels, predictions)\n","        return {\n","            'accuracy': accuracy,\n","            'f1': f1,\n","            'precision': precision,\n","            'recall': recall\n","        }\n","    def train_model(self, train_dataset, output_dir='./results'):\n","        print(\"Training model...\")\n","        training_args = TrainingArguments(\n","            output_dir=output_dir,\n","            num_train_epochs=3 if self.use_lora else 2,\n","            per_device_train_batch_size=16 if self.use_lora else 8,\n","            per_device_eval_batch_size=32 if self.use_lora else 16,\n","            learning_rate=1e-4 if self.use_lora else 2e-5,\n","            warmup_steps=100,\n","            weight_decay=0.01,\n","            logging_dir='./logs',\n","            logging_steps=50,\n","            report_to=None,\n","            dataloader_pin_memory=False,\n","            fp16=torch.cuda.is_available(),\n","        )\n","        self.trainer = Trainer(\n","            model=self.model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            compute_metrics=self.compute_metrics\n","        )\n","        self.trainer.train()\n","        return self.trainer\n","    def evaluate_model(self, test_dataset, test_labels):\n","        print(\"Evaluating model...\")\n","        predictions = self.trainer.predict(test_dataset)\n","        y_pred = np.argmax(predictions.predictions, axis=1)\n","        y_true = test_labels\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nTest Results:\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall: {recall:.4f}\")\n","        print(f\"F1-Score: {f1:.4f}\")\n","        print(f\"\\nDetailed Classification Report:\")\n","        target_names = self.label_encoder.classes_\n","        print(classification_report(y_true, y_pred, target_names=target_names))\n","        return {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'y_true': y_true,\n","            'y_pred': y_pred,\n","            'predictions': predictions\n","        }\n","    def generate_business_insights(self, df):\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"BUSINESS INSIGHTS EXTRACTION\")\n","        print(\"=\"*60)\n","        mixed_reviews = self.insight_extractor.extract_mixed_reviews(df)\n","        print(f\"\\nFound {len(mixed_reviews)} mixed sentiment reviews\")\n","        recommendations = self.insight_extractor.generate_business_recommendations(df)\n","        print(\"\\n\" + \"=\"*50)\n","        print(\"KEY BUSINESS INSIGHTS\")\n","        print(\"=\"*50)\n","        if mixed_reviews:\n","            print(f\"\\n1. MIXED SENTIMENT ANALYSIS:\")\n","            print(f\"   Found {len(mixed_reviews)} reviews with mixed sentiments\")\n","            print(f\"   These are high-rated reviews that still contain negative feedback\")\n","            for i, review in enumerate(mixed_reviews[:3]):\n","                print(f\"\\n   Example {i+1} (Rating: {review['rating']}/5):\")\n","                print(f\"   Text: \\\"{review['text'][:100]}...\\\"\")\n","                print(f\"   Aspects: {list(review['aspect_analysis'].keys())}\")\n","        print(f\"\\n2. ASPECT-BASED RECOMMENDATIONS:\")\n","        for aspect, data in recommendations.items():\n","            print(f\"\\n   {aspect.upper()} Issues:\")\n","            print(f\"   - {data['issue_count']} complaints ({data['percentage']:.1f}% of negative reviews)\")\n","            print(f\"   - Top Recommendations:\")\n","            for rec in data['recommendations'][:3]:\n","                print(f\"     • {rec}\")\n","        return {\n","            'mixed_reviews': mixed_reviews,\n","            'recommendations': recommendations\n","        }\n","    def run_pipeline(self):\n","        print(\"Starting ABSA Pipeline...\")\n","        train_df, test_df = self.load_and_preprocess_data()\n","        train_df = self.extract_aspects_and_enhance_text(train_df)\n","        test_df = self.extract_aspects_and_enhance_text(test_df)\n","\n","        # EDA on training set\n","        print(\"\\n[EDA on Training Set]\")\n","        executive_summary(train_df)\n","        plot_overall_sentiment(train_df)\n","        plot_negative_aspect_breakdown(train_df)\n","        plot_aspect_sentiment_heatmap(train_df)\n","\n","        # Generate business insights from training data\n","        insights = self.generate_business_insights(train_df)\n","\n","        # Prepare label encoding and texts\n","        y_train = self.label_encoder.fit_transform(train_df['sentiment'])\n","        X_train = train_df['enhanced_text'].values\n","        y_test = self.label_encoder.transform(test_df['sentiment'])\n","        X_test = test_df['enhanced_text'].values\n","\n","        # Prepare model and tokenizer\n","        self.prepare_model_and_tokenizer(len(self.label_encoder.classes_))\n","\n","        # Datasets\n","        train_dataset = self.create_datasets(X_train, y_train)\n","        test_dataset = self.create_datasets(X_test, y_test)\n","\n","        # Train model\n","        self.train_model(train_dataset)\n","\n","        # Evaluate on test set\n","        test_results = self.evaluate_model(test_dataset, y_test)\n","\n","        return {\n","          'test_results': test_results,\n","          'business_insights': insights,\n","          'model': self.model,\n","          'tokenizer': self.tokenizer,\n","          'train_data': train_df,\n","          'test_data': test_df,\n","          'label_encoder': self.label_encoder\n","         }\n"],"metadata":{"id":"gRUgcqPMSXUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Model Training, Evaluation & Main\n","# =========================\n","\n","if __name__ == \"__main__\":\n","    # Instantiate the pipeline (choose model, and LoRA or full FT as desired)\n","    pipeline = ABSALLMPipeline(\n","        model_name='distilbert-base-uncased',\n","        use_lora=True  # or False if you do not want LoRA\n","    )\n","\n","    # Run the pipeline end-to-end (this includes data load, EDA, insights, training, evaluation)\n","    results = pipeline.run_pipeline()\n","\n","    # Show model performance metrics on test set\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"FINAL RESULTS SUMMARY\")\n","    print(\"=\"*50)\n","    print(f\"Model Performance on Test Set (20% of the real data):\")\n","    print(f\"  Accuracy: {results['test_results']['accuracy']:.4f}\")\n","    print(f\"  Precision: {results['test_results']['precision']:.4f}\")\n","    print(f\"  Recall: {results['test_results']['recall']:.4f}\")\n","    print(f\"  F1-Score: {results['test_results']['f1']:.4f}\")\n","\n","    print(\"\\nBusiness Insights Generated:\")\n","    print(f\"  Mixed Reviews Found: {len(results['business_insights']['mixed_reviews'])}\")\n","    print(f\"  Aspects with Recommendations: {len(results['business_insights']['recommendations'])}\")\n","    print(\"\\nExample Recommendations:\")\n","    for aspect, data in results['business_insights']['recommendations'].items():\n","        print(f\"- {aspect.title()}: {data['recommendations'][:2]}\")\n","\n","    # Example: Extract and print detailed business insights for a sample review\n","    extractor = BusinessInsightExtractor()\n","    sample_review = \"The fabric is soft but the sizing is off.\"\n","    print(\"\\n--- Example: Deep Opinion Extraction ---\")\n","    print(\"Review:\", sample_review)\n","    phrases = extractor.extract_opinion_phrases(sample_review)\n","    for p in phrases:\n","        asp = extractor.analyze_aspect_sentiment(p)\n","        print(f\"  Phrase: {p}\\n  Aspect Sentiment: {asp}\")\n"],"metadata":{"id":"jNeiMVB6Sk-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer, AutoModelForSequenceClassification,\n","    TrainingArguments, Trainer, EarlyStoppingCallback\n",")\n","from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    precision_recall_fscore_support, accuracy_score\n",")\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textblob import TextBlob\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(\n","            text,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","class ABSALLMPipeline:\n","    def __init__(self, model_name='distilbert-base-uncased', use_lora=True, lora_config=None):\n","        \"\"\"\n","        Initialize ABSA Pipeline with option for LoRA fine-tuning\n","\n","        Args:\n","            model_name: Model to use ('distilbert-base-uncased', 'roberta-base', 'bert-base-uncased')\n","            use_lora: Whether to use LoRA fine-tuning (True) or full fine-tuning (False)\n","            lora_config: Custom LoRA configuration\n","        \"\"\"\n","        self.model_name = model_name\n","        self.use_lora = use_lora\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        print(f\"Using device: {self.device}\")\n","        print(f\"Fine-tuning method: {'LoRA' if use_lora else 'Full Fine-tuning'}\")\n","\n","        self.tokenizer = None\n","        self.model = None\n","        self.peft_model = None\n","        self.label_encoder = LabelEncoder()\n","        self.trainer = None\n","\n","        # Default LoRA configuration\n","        self.lora_config = lora_config or LoraConfig(\n","            task_type=TaskType.SEQ_CLS,  # Sequence Classification\n","            inference_mode=False,\n","            r=16,                        # Rank - higher = more parameters but better performance\n","            lora_alpha=32,               # LoRA scaling parameter\n","            lora_dropout=0.1,            # Dropout for LoRA layers\n","            target_modules=self._get_target_modules(model_name),  # Which layers to adapt\n","            bias=\"none\",                 # Whether to adapt bias parameters\n","        )\n","\n","    def _get_target_modules(self, model_name):\n","        \"\"\"Get target modules for LoRA based on model architecture\"\"\"\n","        if 'distilbert' in model_name.lower():\n","            return [\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]\n","        elif 'roberta' in model_name.lower() or 'bert' in model_name.lower():\n","            return [\"query\", \"value\", \"key\", \"dense\"]\n","        else:\n","            # Default for most transformer models\n","            return [\"query\", \"value\"]\n","\n","    def load_and_preprocess_data(self):\n","        \"\"\"Load and preprocess the dataset\"\"\"\n","        print(\"Loading dataset...\")\n","        ds = load_dataset(\"Censius-AI/ECommerce-Women-Clothing-Reviews\")\n","        df = pd.DataFrame(ds[\"train\"])\n","\n","        print(f\"Dataset shape: {df.shape}\")\n","\n","        # Remove rows with missing review text\n","        df = df.dropna(subset=['Review Text'])\n","\n","        # Sample data for demonstration (remove this line for full dataset)\n","        df = df.sample(n=min(15000, len(df)), random_state=42).reset_index(drop=True)\n","        print(f\"Sampled dataset shape: {df.shape}\")\n","\n","        # Create sentiment labels from rating\n","        def create_sentiment_label(rating):\n","            if rating <= 2:\n","                return 'negative'\n","            elif rating == 3:\n","                return 'neutral'\n","            else:\n","                return 'positive'\n","\n","        df['sentiment'] = df['Rating'].apply(create_sentiment_label)\n","\n","        print(f\"Sentiment distribution:\")\n","        print(df['sentiment'].value_counts())\n","\n","        return df\n","\n","    def extract_aspects_and_enhance_text(self, df):\n","        \"\"\"Extract aspects and enhance text with aspect information\"\"\"\n","        print(\"Extracting aspects and enhancing text...\")\n","\n","        # Define common aspects for clothing reviews\n","        aspects = {\n","            'fit': ['fit', 'fits', 'fitting', 'size', 'sized', 'sizing', 'tight', 'loose', 'small', 'large'],\n","            'quality': ['quality', 'material', 'fabric', 'cotton', 'polyester', 'durable', 'cheap', 'flimsy'],\n","            'style': ['style', 'design', 'look', 'appearance', 'fashionable', 'trendy', 'color', 'pattern'],\n","            'comfort': ['comfort', 'comfortable', 'soft', 'cozy', 'breathable', 'itchy', 'scratchy'],\n","            'price': ['price', 'cost', 'expensive', 'cheap', 'affordable', 'value', 'worth', 'money']\n","        }\n","\n","        # Extract aspect mentions\n","        for aspect, keywords in aspects.items():\n","            df[f'aspect_{aspect}'] = df['Review Text'].str.lower().str.contains('|'.join(keywords), na=False)\n","\n","        # Create enhanced text with aspect information\n","        def enhance_text_with_aspects(row):\n","            text = row['Review Text']\n","            mentioned_aspects = []\n","\n","            for aspect in aspects.keys():\n","                if row[f'aspect_{aspect}']:\n","                    mentioned_aspects.append(aspect)\n","\n","            if mentioned_aspects:\n","                aspect_info = f\"[ASPECTS: {', '.join(mentioned_aspects)}] \"\n","                return aspect_info + text\n","            return text\n","\n","        df['enhanced_text'] = df.apply(enhance_text_with_aspects, axis=1)\n","\n","        return df\n","\n","    def prepare_model_and_tokenizer(self, num_labels):\n","        \"\"\"Initialize tokenizer and model with optional LoRA\"\"\"\n","        print(f\"Loading {self.model_name} model and tokenizer...\")\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","\n","        # Load base model\n","        self.model = AutoModelForSequenceClassification.from_pretrained(\n","            self.model_name,\n","            num_labels=num_labels\n","        )\n","\n","        if self.use_lora:\n","            print(\"Applying LoRA configuration...\")\n","            print(f\"LoRA Config: {self.lora_config}\")\n","\n","            # Apply LoRA to the model\n","            self.peft_model = get_peft_model(self.model, self.lora_config)\n","\n","            # Print trainable parameters\n","            self.peft_model.print_trainable_parameters()\n","\n","            # Move to device\n","            self.peft_model.to(self.device)\n","\n","            # Use peft_model for training\n","            self.model = self.peft_model\n","        else:\n","            print(\"Using full fine-tuning...\")\n","            # Move model to device\n","            self.model.to(self.device)\n","\n","            total_params = sum(p.numel() for p in self.model.parameters())\n","            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","            print(f\"Total parameters: {total_params:,}\")\n","            print(f\"Trainable parameters: {trainable_params:,}\")\n","\n","    def create_datasets(self, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n","        \"\"\"Create PyTorch datasets\"\"\"\n","        train_dataset = ReviewDataset(train_texts, train_labels, self.tokenizer)\n","        val_dataset = ReviewDataset(val_texts, val_labels, self.tokenizer)\n","        test_dataset = ReviewDataset(test_texts, test_labels, self.tokenizer)\n","\n","        return train_dataset, val_dataset, test_dataset\n","\n","    def compute_metrics(self, eval_pred):\n","        \"\"\"Compute metrics for evaluation\"\"\"\n","        predictions, labels = eval_pred\n","        predictions = np.argmax(predictions, axis=1)\n","\n","        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","        accuracy = accuracy_score(labels, predictions)\n","\n","        return {\n","            'accuracy': accuracy,\n","            'f1': f1,\n","            'precision': precision,\n","            'recall': recall\n","        }\n","\n","    def train_model(self, train_dataset, val_dataset, output_dir='./results'):\n","        \"\"\"Train the model using Hugging Face Trainer\"\"\"\n","        print(\"Training model...\")\n","\n","        # Adjust training arguments based on fine-tuning method\n","        if self.use_lora:\n","            # LoRA settings - can use higher batch sizes and learning rates\n","            training_args = TrainingArguments(\n","                output_dir=output_dir,\n","                num_train_epochs=5,\n","                per_device_train_batch_size=32,  # Higher batch size for LoRA\n","                per_device_eval_batch_size=64,\n","                learning_rate=1e-4,              # Higher learning rate for LoRA\n","                warmup_steps=100,\n","                weight_decay=0.01,\n","                logging_dir='./logs',\n","                logging_steps=50,\n","                evaluation_strategy=\"steps\",\n","                eval_steps=200,\n","                save_strategy=\"steps\",\n","                save_steps=200,\n","                load_best_model_at_end=True,\n","                metric_for_best_model=\"f1\",\n","                greater_is_better=True,\n","                report_to=None,\n","                save_total_limit=2,\n","                dataloader_pin_memory=False,\n","                fp16=torch.cuda.is_available(),\n","                gradient_checkpointing=True,     # Memory efficient\n","            )\n","        else:\n","            # Full fine-tuning settings - more conservative\n","            training_args = TrainingArguments(\n","                output_dir=output_dir,\n","                num_train_epochs=3,\n","                per_device_train_batch_size=16,  # Lower batch size for full fine-tuning\n","                per_device_eval_batch_size=32,\n","                learning_rate=2e-5,              # Lower learning rate for full fine-tuning\n","                warmup_steps=500,\n","                weight_decay=0.01,\n","                logging_dir='./logs',\n","                logging_steps=100,\n","                evaluation_strategy=\"steps\",\n","                eval_steps=500,\n","                save_strategy=\"steps\",\n","                save_steps=500,\n","                load_best_model_at_end=True,\n","                metric_for_best_model=\"f1\",\n","                greater_is_better=True,\n","                report_to=None,\n","                save_total_limit=2,\n","                dataloader_pin_memory=False,\n","                fp16=torch.cuda.is_available(),\n","            )\n","\n","        self.trainer = Trainer(\n","            model=self.model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            compute_metrics=self.compute_metrics,\n","            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","        )\n","\n","        # Train the model\n","        self.trainer.train()\n","\n","        return self.trainer\n","\n","    def evaluate_model(self, test_dataset, test_labels):\n","        \"\"\"Evaluate the model and return detailed metrics\"\"\"\n","        print(\"Evaluating model...\")\n","\n","        # Get predictions\n","        predictions = self.trainer.predict(test_dataset)\n","        y_pred = np.argmax(predictions.predictions, axis=1)\n","        y_true = test_labels\n","\n","        # Calculate metrics\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n","        accuracy = accuracy_score(y_true, y_pred)\n","\n","        print(f\"\\nTest Results:\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall: {recall:.4f}\")\n","        print(f\"F1-Score: {f1:.4f}\")\n","\n","        # Detailed classification report\n","        print(f\"\\nDetailed Classification Report:\")\n","        target_names = self.label_encoder.classes_\n","        print(classification_report(y_true, y_pred, target_names=target_names))\n","\n","        return {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'y_true': y_true,\n","            'y_pred': y_pred,\n","            'predictions': predictions\n","        }\n","\n","    def cross_validate(self, df, cv_folds=5):\n","        \"\"\"Perform k-fold cross-validation\"\"\"\n","        print(f\"Performing {cv_folds}-fold cross-validation...\")\n","\n","        X = df['enhanced_text'].values\n","        y = self.label_encoder.fit_transform(df['sentiment'])\n","\n","        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n","\n","        cv_results = {\n","            'accuracy': [],\n","            'precision': [],\n","            'recall': [],\n","            'f1': []\n","        }\n","\n","        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n","            print(f\"\\nFold {fold + 1}/{cv_folds}\")\n","\n","            # Split data\n","            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n","            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n","\n","            # Create datasets\n","            train_dataset = ReviewDataset(X_train_fold, y_train_fold, self.tokenizer)\n","            val_dataset = ReviewDataset(X_val_fold, y_val_fold, self.tokenizer)\n","\n","            # Re-initialize model for each fold\n","            base_model = AutoModelForSequenceClassification.from_pretrained(\n","                self.model_name,\n","                num_labels=len(self.label_encoder.classes_)\n","            )\n","\n","            if self.use_lora:\n","                fold_model = get_peft_model(base_model, self.lora_config)\n","            else:\n","                fold_model = base_model\n","\n","            fold_model.to(self.device)\n","\n","            # Train model\n","            trainer = self.train_fold_model(fold_model, train_dataset, val_dataset, fold)\n","\n","            # Evaluate\n","            predictions = trainer.predict(val_dataset)\n","            y_pred_fold = np.argmax(predictions.predictions, axis=1)\n","\n","            # Calculate metrics\n","            accuracy = accuracy_score(y_val_fold, y_pred_fold)\n","            precision, recall, f1, _ = precision_recall_fscore_support(\n","                y_val_fold, y_pred_fold, average='weighted'\n","            )\n","\n","            cv_results['accuracy'].append(accuracy)\n","            cv_results['precision'].append(precision)\n","            cv_results['recall'].append(recall)\n","            cv_results['f1'].append(f1)\n","\n","            print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n","\n","            # Clean up\n","            del fold_model, trainer\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","        # Print cross-validation results\n","        print(f\"\\nCross-Validation Results ({cv_folds} folds):\")\n","        for metric, scores in cv_results.items():\n","            mean_score = np.mean(scores)\n","            std_score = np.std(scores)\n","            print(f\"{metric.title()}: {mean_score:.4f} (+/- {std_score * 2:.4f})\")\n","\n","        return cv_results\n","\n","    def train_fold_model(self, model, train_dataset, val_dataset, fold):\n","        \"\"\"Train model for a specific fold\"\"\"\n","        if self.use_lora:\n","            training_args = TrainingArguments(\n","                output_dir=f'./results_fold_{fold}',\n","                num_train_epochs=3,\n","                per_device_train_batch_size=32,\n","                per_device_eval_batch_size=64,\n","                learning_rate=1e-4,\n","                warmup_steps=50,\n","                weight_decay=0.01,\n","                logging_steps=25,\n","                evaluation_strategy=\"no\",\n","                save_strategy=\"no\",\n","                report_to=None,\n","                fp16=torch.cuda.is_available(),\n","                gradient_checkpointing=True,\n","            )\n","        else:\n","            training_args = TrainingArguments(\n","                output_dir=f'./results_fold_{fold}',\n","                num_train_epochs=2,\n","                per_device_train_batch_size=16,\n","                per_device_eval_batch_size=32,\n","                learning_rate=2e-5,\n","                warmup_steps=100,\n","                weight_decay=0.01,\n","                logging_steps=50,\n","                evaluation_strategy=\"no\",\n","                save_strategy=\"no\",\n","                report_to=None,\n","                fp16=torch.cuda.is_available(),\n","            )\n","\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            compute_metrics=self.compute_metrics,\n","        )\n","\n","        trainer.train()\n","        return trainer\n","\n","    def plot_confusion_matrix(self, y_true, y_pred, title=\"Confusion Matrix\"):\n","        \"\"\"Plot confusion matrix\"\"\"\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                   xticklabels=self.label_encoder.classes_,\n","                   yticklabels=self.label_encoder.classes_,\n","                   cbar_kws={'label': 'Count'})\n","\n","        method = \"LoRA\" if self.use_lora else \"Full Fine-tuning\"\n","        plt.title(f'{title}\\nModel: {self.model_name} ({method})', fontsize=16, fontweight='bold')\n","        plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n","        plt.ylabel('True Label', fontsize=12, fontweight='bold')\n","\n","        # Add accuracy to the plot\n","        accuracy = accuracy_score(y_true, y_pred) * 100\n","        plt.figtext(0.02, 0.02, f'Overall Accuracy: {accuracy:.2f}%',\n","                   fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Print per-class metrics\n","        print(f\"\\nPer-class metrics:\")\n","        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n","\n","        for i, class_name in enumerate(self.label_encoder.classes_):\n","            print(f\"{class_name}:\")\n","            print(f\"  Precision: {precision[i]:.4f}\")\n","            print(f\"  Recall: {recall[i]:.4f}\")\n","            print(f\"  F1-Score: {f1[i]:.4f}\")\n","            print(f\"  Support: {support[i]}\")\n","\n","    def plot_training_history(self):\n","        \"\"\"Plot training history if available\"\"\"\n","        if self.trainer and hasattr(self.trainer.state, 'log_history'):\n","            history = self.trainer.state.log_history\n","\n","            train_loss = []\n","            eval_loss = []\n","            eval_f1 = []\n","\n","            for log in history:\n","                if 'loss' in log:\n","                    train_loss.append(log['loss'])\n","                if 'eval_loss' in log:\n","                    eval_loss.append(log['eval_loss'])\n","                if 'eval_f1' in log:\n","                    eval_f1.append(log['eval_f1'])\n","\n","            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","            method = \"LoRA\" if self.use_lora else \"Full Fine-tuning\"\n","\n","            # Loss plot\n","            ax1.plot(train_loss, label='Training Loss', color='blue')\n","            if eval_loss:\n","                ax1.plot(eval_loss, label='Validation Loss', color='red')\n","            ax1.set_title(f'Training History - {method}\\nModel: {self.model_name}')\n","            ax1.set_xlabel('Steps')\n","            ax1.set_ylabel('Loss')\n","            ax1.legend()\n","            ax1.grid(True)\n","\n","            # F1 plot\n","            if eval_f1:\n","                ax2.plot(eval_f1, label='Validation F1', color='green')\n","                ax2.set_title(f'Validation F1 Score - {method}')\n","                ax2.set_xlabel('Steps')\n","                ax2.set_ylabel('F1 Score')\n","                ax2.legend()\n","                ax2.grid(True)\n","\n","            plt.tight_layout()\n","            plt.show()\n","\n","    def save_model(self, output_dir=\"./final_model\"):\n","        \"\"\"Save the trained model\"\"\"\n","        if self.use_lora:\n","            self.peft_model.save_pretrained(output_dir)\n","            print(f\"LoRA model saved to {output_dir}\")\n","        else:\n","            self.model.save_pretrained(output_dir)\n","            print(f\"Full model saved to {output_dir}\")\n","\n","        self.tokenizer.save_pretrained(output_dir)\n","\n","    def run_pipeline(self, include_cv=False):\n","        \"\"\"Run the complete ABSA pipeline\"\"\"\n","        # Load and preprocess data\n","        df = self.load_and_preprocess_data()\n","\n","        # Extract aspects and enhance text\n","        df = self.extract_aspects_and_enhance_text(df)\n","\n","        # Encode labels\n","        y = self.label_encoder.fit_transform(df['sentiment'])\n","        X = df['enhanced_text'].values\n","\n","        # Prepare model and tokenizer\n","        self.prepare_model_and_tokenizer(len(self.label_encoder.classes_))\n","\n","        # Train-test split (80-20)\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X, y, test_size=0.2, random_state=42, stratify=y\n","        )\n","\n","        # Further split training data for validation\n","        X_train_final, X_val, y_train_final, y_val = train_test_split(\n","            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n","        )\n","\n","        print(f\"Final training set size: {len(X_train_final)}\")\n","        print(f\"Validation set size: {len(X_val)}\")\n","        print(f\"Test set size: {len(X_test)}\")\n","\n","        # Create datasets\n","        train_dataset, val_dataset, test_dataset = self.create_datasets(\n","            X_train_final, y_train_final, X_val, y_val, X_test, y_test\n","        )\n","\n","        # Train model\n","        self.train_model(train_dataset, val_dataset)\n","\n","        # Plot training history\n","        self.plot_training_history()\n","\n","        # Evaluate on test set\n","        test_results = self.evaluate_model(test_dataset, y_test)\n","\n","        # Plot confusion matrix\n","        self.plot_confusion_matrix(\n","            test_results['y_true'],\n","            test_results['y_pred'],\n","            f\"Test Set Confusion Matrix\"\n","        )\n","\n","        # Cross-validation (optional)\n","        cv_results = None\n","        if include_cv:\n","            cv_results = self.cross_validate(df, cv_folds=3)  # Reduced folds for LoRA\n","\n","        # Save model\n","        self.save_model()\n","\n","        return {\n","            'test_results': test_results,\n","            'cv_results': cv_results,\n","            'model': self.model,\n","            'tokenizer': self.tokenizer\n","        }\n","\n","def compare_lora_vs_full_finetuning():\n","    \"\"\"Compare LoRA vs Full Fine-tuning\"\"\"\n","    print(\"Comparing LoRA vs Full Fine-tuning...\")\n","\n","    results_comparison = {}\n","\n","    # Test LoRA\n","    print(f\"\\n{'='*60}\")\n","    print(f\"TESTING LoRA FINE-TUNING\")\n","    print(f\"{'='*60}\")\n","\n","    pipeline_lora = ABSALLMPipeline(model_name='distilbert-base-uncased', use_lora=True)\n","    results_lora = pipeline_lora.run_pipeline(include_cv=False)\n","\n","    results_comparison['LoRA'] = {\n","        'accuracy': results_lora['test_results']['accuracy'],\n","        'precision': results_lora['test_results']['precision'],\n","        'recall': results_lora['test_results']['recall'],\n","        'f1': results_lora['test_results']['f1']\n","    }\n","\n","    # Clear memory\n","    del pipeline_lora\n","    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    # Test Full Fine-tuning\n","    print(f\"\\n{'='*60}\")\n","    print(f\"TESTING FULL FINE-TUNING\")\n","    print(f\"{'='*60}\")\n","\n","    pipeline_full = ABSALLMPipeline(model_name='distilbert-base-uncased', use_lora=False)\n","    results_full = pipeline_full.run_pipeline(include_cv=False)\n","\n","    results_comparison['Full Fine-tuning'] = {\n","        'accuracy': results_full['test_results']['accuracy'],\n","        'precision': results_full['test_results']['precision'],\n","        'recall': results_full['test_results']['recall'],\n","        'f1': results_full['test_results']['f1']\n","    }\n","\n","    # Print comparison\n","    print(f\"\\n{'='*60}\")\n","    print(\"LoRA vs FULL FINE-TUNING COMPARISON\")\n","    print(f\"{'='*60}\")\n","\n","    df_comparison = pd.DataFrame(results_comparison).T\n","    print(df_comparison.round(4))\n","\n","    return results_comparison\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Option 1: Run LoRA fine-tuning (recommended for T4)\n","    print(\"Running ABSA Pipeline with LoRA Fine-tuning...\")\n","    pipeline = ABSALLMPipeline(model_name='distilbert-base-uncased', use_lora=True)\n","    results = pipeline.run_pipeline(include_cv=True)\n","\n","    print(f\"\\n{'='*50}\")\n","    print(\"LORA PIPELINE COMPLETED!\")\n","    print(f\"{'='*50}\")\n","    print(f\"Final Test Results:\")\n","    print(f\"Accuracy: {results['test_results']['accuracy']:.4f}\")\n","    print(f\"Precision: {results['test_results']['precision']:.4f}\")\n","    print(f\"Recall: {results['test_results']['recall']:.4f}\")\n","    print(f\"F1-Score: {results['test_results']['f1']:.4f}\")\n","\n","    # Option 2: Compare LoRA vs Full Fine-tuning\n","    # comparison_results = compare_lora_vs_full_finetuning()\n","\n","    # Option 3: Run full fine-tuning\n","    # pipeline_full = ABSALLMPipeline(model_name='distilbert-base-uncased', use_lora=False)\n","    # results_full = pipeline_full.run_pipeline(include_cv=False)"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"QjOKAB0WQWbG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from pandas_gbq import to_gbq\n","\n","PROJECT_ID = 'able-balm-454718-n8'  # your GCP project ID\n","DATASET = 'absa_outputs'  # or any dataset name you like (will be created if it doesn't exist)\n","\n","# Helper to upload DataFrame\n","def upload_to_bigquery(df, table, project_id=PROJECT_ID, dataset=DATASET):\n","    full_table = f\"{dataset}.{table}\"\n","    df.to_gbq(full_table, project_id=project_id, if_exists='replace')\n","    print(f\"Uploaded {table} to BigQuery dataset {dataset}.\")\n","\n","# Export train/test data\n","upload_to_bigquery(train_data, 'train_data')\n","upload_to_bigquery(test_data, 'test_data')\n","\n","# Export metrics\n","upload_to_bigquery(summary_df, 'test_metrics')\n","\n","# Export recommendations and mixed reviews\n","upload_to_bigquery(rec_df, 'business_recommendations')\n","upload_to_bigquery(mixed_df, 'mixed_reviews')\n"],"metadata":{"id":"dx2loSRwUz1V"},"execution_count":null,"outputs":[]}]}