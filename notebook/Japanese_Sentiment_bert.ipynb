{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/notebookd4cfb385ee.b94b9b4d-feb6-476f-836e-b6a74e98ef5c.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250612/auto/storage/goog4_request&X-Goog-Date=20250612T131831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=aeb880bf0c73e8f64c855b028a441c37077c446901b1d4906c1e568dbdb91ef896647d9587b0fc3d22628372a981ab8f7a9166cc13a3ac869d9b5609862fcf22e843f6675a66c500826d8f77c428721ccfcb8d2301dbeff8f5ee129021ec173dd252b30806d1faff927d87dfacba0558e3b287e8ae442ad1556aaa15cf6e692320ca96522b846b4e6ea20866c5bce56c44c2e5ddd6acbc82274a98ac4cf844ba57f0184b33f952f3cac4157a2e43e0f93600bd0207edbcd74b3224c6021d6b9fe8f4562ef63d8d45c9c3c2f86f4314907ac22553f0c647a51ab276959a5a959cf2753d23909bd1e29b935ee3fd1bde19f0ebed4dbcdb4f1336441bfa2e03e260","timestamp":1749748853251}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# ========================\n","# 0. Install libraries\n","# ========================\n","!pit install --quiet numpy spacy thinc\n","!pip install --quiet torch torchvision torchaudio\n","!pip install --quiet transformers fugashi ipadic accelerate peft sentencepiece matplotlib seaborn tqdm"],"metadata":{"id":"yAQcBFr3IMYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(\"CUDA available:\", torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfTjYNifrfFP","executionInfo":{"status":"ok","timestamp":1749745839913,"user_tz":-120,"elapsed":11956,"user":{"displayName":"","userId":""}},"outputId":"e7cb8ee5-9510-42b7-800f-2808e2fd5f23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: False\n"]}]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Nr0KW0ZiCea","executionInfo":{"status":"ok","timestamp":1749745845854,"user_tz":-120,"elapsed":5942,"user":{"displayName":"","userId":""}},"outputId":"59410d81-c86a-4a5b-d5a3-a9f0d0cc9a7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.52.4\n"]}]},{"cell_type":"code","source":["!pip install certifi\n","!mkdir -p /usr/local/share/ca-certificates/\n","!cp /etc/ssl/certs/ca-certificates.crt /usr/local/share/ca-certificates/\n","!update-ca-certificates\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TK8QWC8kvZa","executionInfo":{"status":"ok","timestamp":1749745864901,"user_tz":-120,"elapsed":19046,"user":{"displayName":"","userId":""}},"outputId":"ae8ac29b-aad2-4f64-d6cb-e0d0056a38c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.4.26)\n","Updating certificates in /etc/ssl/certs...\n","rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL\n","rehash: warning: skipping ca-certificates.pem,it does not contain exactly one certificate or CRL\n","1 added, 0 removed; done.\n","Running hooks in /etc/ca-certificates/update.d...\n","\n","Adding debian:ca-certificates.pem\n","done.\n","done.\n"]}]},{"cell_type":"code","source":["# ========================\n","# 1. Imports & Setup\n","# ========================\n","from tqdm import tqdm\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from peft import get_peft_model, LoraConfig, TaskType\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelEncoder\n","import optuna\n","import shap\n","import matplotlib.pyplot as plt\n","from ace_tools_open import display_dataframe_to_user\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ========================\n","# 2. Kansai-ben & Directness Detection\n","# ========================\n","kansaiben_keywords = [\"〜やん\", \"〜やで\", \"〜せなあかん\", \"ちゃう\", \"ほんま\", \"めっちゃ\", \"〜せんと\", \"なんでやねん\"]\n","def detect_kansaiben(text):\n","    return any(k in text for k in kansaiben_keywords)\n","\n","def detect_directness(text):\n","    direct_phrases = [\"最悪\", \"ありえない\", \"めっちゃ\", \"だめ\", \"良い\", \"良くない\", \"おすすめ\", \"絶対\", \"微妙\"]\n","    return any(word in text for word in direct_phrases)\n","\n","# ========================\n","# 3. Load & Prepare Data\n","# ========================\n","def load_jsts_json(url):\n","    df = pd.read_json(url, lines=True)\n","    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n","    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n","    return df[['text', 'sentiment']]\n","\n","df_train = load_jsts_json(\"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\").sample(500, random_state=42)\n","df_valid = load_jsts_json(\"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\").sample(100, random_state=42)\n","\n","model_name = \"cl-tohoku/bert-base-japanese-v2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_batch(texts):\n","    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n","\n","class SimpleDataset(torch.utils.data.Dataset):\n","    def __init__(self, df):\n","        self.encodings = tokenize_batch(df['text'])\n","        self.labels = torch.tensor(df['sentiment'].values)\n","    def __getitem__(self, idx):\n","        item = {k: v[idx] for k, v in self.encodings.items()}\n","        item[\"labels\"] = self.labels[idx]\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_ds = SimpleDataset(df_train)\n","eval_ds = SimpleDataset(df_valid)\n","\n","# ========================\n","# 4. LoRA Model Init & Quick Finetune (for demonstration)\n","# ========================\n","base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n","model = get_peft_model(base_model, peft_config).to(device)\n","\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","\n","train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","model.train()\n","for epoch in range(1):\n","    loop = tqdm(train_loader, desc=\"Training\")\n","    for batch in loop:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        loop.set_postfix(loss=loss.item())\n","\n","# ========================\n","# 5. Extract Transformer [CLS] Embeddings (Validation Set)\n","# ========================\n","def extract_cls_embeddings(model, texts):\n","    model.eval()\n","    embeddings = []\n","    with torch.no_grad():\n","        for text in tqdm(texts, desc=\"Extracting embeddings\"):\n","            inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n","            outputs = model.base_model(**{k: v for k, v in inputs.items()})\n","            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n","            embeddings.append(emb)\n","    return np.vstack(embeddings)\n","\n","valid_embeddings = extract_cls_embeddings(model, df_valid['text'])\n","\n","# ========================\n","# 6. Optuna Hyperparameter Tuning for XGBoost\n","# ========================\n","le = LabelEncoder()\n","y_valid = le.fit_transform(df_valid['sentiment'])\n","\n","def objective(trial):\n","    params = {\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n","        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n","        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n","        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n","        \"use_label_encoder\": False,\n","        \"eval_metric\": \"mlogloss\",\n","        \"verbosity\": 0,\n","    }\n","    clf = XGBClassifier(**params)\n","    clf.fit(valid_embeddings, y_valid)\n","    preds = clf.predict(valid_embeddings)\n","    accuracy = np.mean(preds == y_valid)\n","    return accuracy\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=20)\n","print(\"Best trial:\", study.best_trial.params)\n","\n","# Use best params\n","clf = XGBClassifier(**study.best_trial.params)\n","clf.fit(valid_embeddings, y_valid)\n","df_valid['xgb_pred'] = le.inverse_transform(clf.predict(valid_embeddings))\n","\n","print(\"\\nClassification Report (XGBoost + Optuna):\")\n","print(classification_report(df_valid['sentiment'], df_valid['xgb_pred']))\n","\n","# ========================\n","# 7. XGBoost Feature Importance\n","# ========================\n","plt.figure(figsize=(10,4))\n","plt.bar(range(valid_embeddings.shape[1]), clf.feature_importances_)\n","plt.title('XGBoost Feature Importance')\n","plt.xlabel('Embedding Dimension')\n","plt.ylabel('Importance')\n","plt.show()\n","\n","# ========================\n","# 8. SHAP Analysis for XGBoost\n","# ========================\n","explainer = shap.Explainer(clf, valid_embeddings)\n","shap_values = explainer(valid_embeddings)\n","\n","shap.summary_plot(shap_values, valid_embeddings, show=True, plot_type=\"bar\", max_display=20)\n","\n","# ========================\n","# 9. Kansai-ben & Direct Tone Columns\n","# ========================\n","df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben)\n","df_valid['direct_tone'] = df_valid['text'].apply(detect_directness)\n","\n","# ========================\n","# 10. Display Results\n","# ========================\n","display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis\", dataframe=df_valid)\n"],"metadata":{"id":"mMxTA1g2UHy6"},"execution_count":null,"outputs":[]}]}