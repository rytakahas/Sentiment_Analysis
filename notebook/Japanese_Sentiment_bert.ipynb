{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T13:27:53.708355Z",
     "iopub.status.busy": "2025-06-16T13:27:53.708128Z",
     "iopub.status.idle": "2025-06-16T13:28:07.235675Z",
     "shell.execute_reply": "2025-06-16T13:28:07.234545Z",
     "shell.execute_reply.started": "2025-06-16T13:27:53.708334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 0. Install libraries\n",
    "# ========================\n",
    "!pip install --quiet numpy spacy thinc\n",
    "!pip install --quiet torch torchvision torchaudio\n",
    "!pip install --quiet transformers fugashi ipadic accelerate peft sentencepiece matplotlib seaborn tqdm\n",
    "!pip install --quiet xgboost optuna ace_tools_open shap unidic-lite mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T13:28:07.237234Z",
     "iopub.status.busy": "2025-06-16T13:28:07.236876Z",
     "iopub.status.idle": "2025-06-16T13:28:07.242356Z",
     "shell.execute_reply": "2025-06-16T13:28:07.241437Z",
     "shell.execute_reply.started": "2025-06-16T13:28:07.237202Z"
    },
    "executionInfo": {
     "elapsed": 11956,
     "status": "ok",
     "timestamp": 1749745839913,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "dfTjYNifrfFP",
    "outputId": "e7cb8ee5-9510-42b7-800f-2808e2fd5f23",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T13:28:08.283073Z",
     "iopub.status.busy": "2025-06-16T13:28:08.282801Z",
     "iopub.status.idle": "2025-06-16T13:28:08.287097Z",
     "shell.execute_reply": "2025-06-16T13:28:08.286419Z",
     "shell.execute_reply.started": "2025-06-16T13:28:08.283053Z"
    },
    "executionInfo": {
     "elapsed": 5942,
     "status": "ok",
     "timestamp": 1749745845854,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "7Nr0KW0ZiCea",
    "outputId": "59410d81-c86a-4a5b-d5a3-a9f0d0cc9a7a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T13:28:08.290079Z",
     "iopub.status.busy": "2025-06-16T13:28:08.289810Z",
     "iopub.status.idle": "2025-06-16T13:28:14.983267Z",
     "shell.execute_reply": "2025-06-16T13:28:14.981970Z",
     "shell.execute_reply.started": "2025-06-16T13:28:08.290051Z"
    },
    "executionInfo": {
     "elapsed": 19046,
     "status": "ok",
     "timestamp": 1749745864901,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "7TK8QWC8kvZa",
    "outputId": "ae8ac29b-aad2-4f64-d6cb-e0d0056a38c5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install certifi\n",
    "!mkdir -p /usr/local/share/ca-certificates/\n",
    "!cp /etc/ssl/certs/ca-certificates.crt /usr/local/share/ca-certificates/\n",
    "!update-ca-certificates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926,
     "referenced_widgets": [
      "6b729dbc1b204303b12d44def92e5af9",
      "0913fa97ccd74ef49a00ee04b63ea6bf",
      "9598dbe3b1c64dadad37002cd73a81cf",
      "602c48c5116141109cbd7bdd4b76544a",
      "ec7ec7303f8c4284871fd36e628db99f",
      "8b53bd89d0b34ddaae3115a653e0d775",
      "59aa76f235764770b2382ee79efeb575",
      "bdd7f2df67c54ba3a31c859984b9a8ac",
      "557028e8e3a7418d913b0eafc59a4e99",
      "a4a95fd3ca1a48bd8a7886f46b7d2410",
      "a4df42534a4f42f4adb067d25838788a",
      "ef972f5ca5d3454b857849a9fa1df2a6",
      "47fa45983dca46de914fb882917a8422",
      "ef404e26e95f4fc6b5d3e46e07daab96",
      "ebdcfad4799d498b99a4ebfe98819aa3",
      "762858486a7b4858b9bdc025219a169f",
      "da2bdb8ad23c4ecbb0f3dadc569a5b55",
      "81169d8b554a4e9aa65eed52538bb0f0",
      "44a143b60fd1446cb35dbe21d5de0bf4",
      "ffb4d6bc88bf4b14a325ff9a2fc154e0",
      "25233e339a77418a846f7ed7dbc906c0",
      "12ec674eff1d4989955fcbed87554ffd",
      "a2fa6d052cc54e94a8a4d8be7d18b3ee",
      "8e673ea2d9e44aad83d83fdee8e1eed6",
      "27fca39b431c48c394fdb434760570d0",
      "18cd81a4ce594285ab1081fa723f005a",
      "55f3c34729de4f01a1b8245ee5723220",
      "af0c0d8edbec4611bf611d2ac4b85c1c",
      "d4de67040a2942ebad27e94be89778b2",
      "94103ec6f7364f54a10460fcaa83713c",
      "19aa71867f6f42a88e6bbe811b354e30",
      "aa8acb1f146642ca9446be325ffc9e31",
      "708c6c94c066489081ad8875d00e3b51"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T13:28:14.985196Z",
     "iopub.status.busy": "2025-06-16T13:28:14.984911Z",
     "iopub.status.idle": "2025-06-16T13:33:13.919215Z",
     "shell.execute_reply": "2025-06-16T13:33:13.918359Z",
     "shell.execute_reply.started": "2025-06-16T13:28:14.985164Z"
    },
    "executionInfo": {
     "elapsed": 33625,
     "status": "error",
     "timestamp": 1749745898528,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "mMxTA1g2UHy6",
    "outputId": "d8bd9a0f-ec7a-4ee6-80f3-0980744bcdc9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Imports & Setup\n",
    "# ========================\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    from ace_tools_open import display_dataframe_to_user\n",
    "except ImportError:\n",
    "    def display_dataframe_to_user(*args, **kwargs):\n",
    "        print(\"ace_tools not installed; displaying DataFrame head:\")\n",
    "        print(args[1].head())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# 2. Kansai-ben & Directness Detection\n",
    "# ========================\n",
    "kansaiben_keywords = [\"〜やん\", \"〜やで\", \"〜せなあかん\", \"ちゃう\", \"ほんま\", \"めっちゃ\", \"〜せんと\", \"なんでやねん\"]\n",
    "def detect_kansaiben(text):\n",
    "    return any(k in text for k in kansaiben_keywords)\n",
    "\n",
    "def detect_directness(text):\n",
    "    direct_phrases = [\"最悪\", \"ありえない\", \"めっちゃ\", \"だめ\", \"良い\", \"良くない\", \"おすすめ\", \"絶対\", \"微妙\"]\n",
    "    return any(word in text for word in direct_phrases)\n",
    "\n",
    "# ========================\n",
    "# 3. Load & Prepare Data (CHUNKED)\n",
    "# ========================\n",
    "def load_jsts_json(url):\n",
    "    df = pd.read_json(url, lines=True)\n",
    "    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n",
    "    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    return df[['text', 'sentiment']]\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\"\n",
    "valid_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\"\n",
    "test_url  = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/test-v1.3.json\"\n",
    "\n",
    "chunk_size = 800   # For low GPU RAM; adjust up if you have more memory\n",
    "\n",
    "df_valid = load_jsts_json(valid_url).sample(500, random_state=42)\n",
    "df_test = load_jsts_json(test_url).sample(100, random_state=42)\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.encodings = tokenize_batch(df['text'])\n",
    "        self.labels = torch.tensor(df['sentiment'].values)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ========================\n",
    "# 4. LoRA Model Init & Batch Finetune (demonstration)\n",
    "# ========================\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "for i, df_chunk in enumerate(pd.read_json(train_url, lines=True, chunksize=chunk_size)):\n",
    "    df_chunk = df_chunk.sample(frac=1, random_state=42+i).reset_index(drop=True)\n",
    "    df_chunk['text'] = df_chunk['sentence1'] + \" \" + df_chunk['sentence2']\n",
    "    df_chunk['sentiment'] = df_chunk['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    df_chunk = df_chunk[['text', 'sentiment']]\n",
    "    train_ds = SimpleDataset(df_chunk)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    for epoch in range(1):  # For demonstration, 1 epoch per chunk\n",
    "        loop = tqdm(train_loader, desc=f\"Training chunk {i+1}\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ========================\n",
    "# 5. Extract Transformer [CLS] Embeddings (All Sets, in Batches)\n",
    "# ========================\n",
    "bert_encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "bert_encoder.eval()\n",
    "\n",
    "def extract_cls_embeddings_batched(encoder, texts, tokenizer, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts.iloc[i:i+batch_size]\n",
    "        inputs = tokenizer(list(batch_texts), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(**inputs)\n",
    "        batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "valid_embeddings = extract_cls_embeddings_batched(bert_encoder, df_valid['text'], tokenizer, device, batch_size=32)\n",
    "test_embeddings = extract_cls_embeddings_batched(bert_encoder, df_test['text'], tokenizer, device, batch_size=32)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_valid = le.fit_transform(df_valid['sentiment'])\n",
    "y_test = le.transform(df_test['sentiment'])\n",
    "\n",
    "# ========================\n",
    "# 6. Add Classical Features to Test Set\n",
    "# ========================\n",
    "df_test['length'] = df_test['text'].apply(len)\n",
    "df_test['kansai_ben'] = df_test['text'].apply(detect_kansaiben).astype(int)\n",
    "df_test['direct_tone'] = df_test['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_test = df_test[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_test_features = np.hstack([test_embeddings, classic_feats_test])\n",
    "\n",
    "df_valid['length'] = df_valid['text'].apply(len)\n",
    "df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben).astype(int)\n",
    "df_valid['direct_tone'] = df_valid['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_valid = df_valid[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_valid_features = np.hstack([valid_embeddings, classic_feats_valid])\n",
    "\n",
    "# ========================\n",
    "# 7. Optuna + K-Fold CV for XGBoost (validation only, with classic features)\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 2),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 0.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 0.5),\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(combined_valid_features, y_valid):\n",
    "        X_tr, X_va = combined_valid_features[train_idx], combined_valid_features[valid_idx]\n",
    "        y_tr, y_va = y_valid[train_idx], y_valid[valid_idx]\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        preds = clf.predict(X_va)\n",
    "        score = np.mean(preds == y_va)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# ========================\n",
    "# 8. Fit Final XGBoost on Validation, Evaluate on Test (with classic features)\n",
    "# ========================\n",
    "feat_names = np.array([f'CLS_emb_{i}' for i in range(test_embeddings.shape[1])] + ['length', 'kansai_ben', 'direct_tone'])\n",
    "clf = XGBClassifier(**study.best_trial.params)\n",
    "clf.fit(combined_valid_features, y_valid)\n",
    "test_pred = clf.predict(combined_test_features)\n",
    "df_test['xgb_pred'] = le.inverse_transform(test_pred)\n",
    "test_pred_proba = clf.predict_proba(combined_test_features)\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost + Optuna, Test Set):\")\n",
    "print(classification_report(df_test['sentiment'], df_test['xgb_pred']))\n",
    "\n",
    "# ========================\n",
    "# 9. Confusion Matrix (Test)\n",
    "# ========================\n",
    "plt.figure(figsize=(6,5))\n",
    "cm = confusion_matrix(df_test['sentiment'], df_test['xgb_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 10. AUC-ROC Curve (Test, One-vs-Rest)\n",
    "# ========================\n",
    "y_test_bin = label_binarize(df_test['sentiment'], classes=[0,1,2])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_test_bin.shape[1]\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC-ROC Curve (Test Set, OvR)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 11. Stakeholder-Ready Feature Importance & SHAP (FIXED)\n",
    "# ========================\n",
    "explainer = shap.Explainer(clf, combined_test_features)\n",
    "shap_values = explainer(combined_test_features)\n",
    "\n",
    "# --- Stakeholder Bar Plot: Only classic features (last three) ---\n",
    "classic_idxs = [-3, -2, -1]  # length, kansai_ben, direct_tone\n",
    "classic_names = feat_names[classic_idxs]\n",
    "\n",
    "# FIX: For multiclass, SHAP values are 3D (n_samples, n_features, n_classes)\n",
    "# We need to take mean across samples AND classes to get feature importance\n",
    "if len(shap_values.values.shape) == 3:\n",
    "    # Multiclass case: take mean across samples (axis=0) and classes (axis=2)\n",
    "    classic_importance = np.abs(shap_values.values).mean(axis=(0, 2))[classic_idxs]\n",
    "else:\n",
    "    # Binary case: take mean across samples only\n",
    "    classic_importance = np.abs(shap_values.values).mean(axis=0)[classic_idxs]\n",
    "\n",
    "classic_importance = np.array(classic_importance, dtype=float).flatten()\n",
    "y_pos = np.arange(len(classic_names))\n",
    "\n",
    "# Ensure we have enough colors\n",
    "color_list = ['#62b5e5', '#a2d4ab', '#fa7268']\n",
    "colors = (color_list * ((len(classic_names)+2)//3))[:len(classic_names)]\n",
    "\n",
    "print(f\"Debug: classic_names shape: {classic_names.shape}\")\n",
    "print(f\"Debug: classic_importance shape: {classic_importance.shape}\")\n",
    "print(f\"Debug: colors length: {len(colors)}\")\n",
    "\n",
    "plt.figure(figsize=(7,2))\n",
    "plt.barh(y_pos, classic_importance, color=colors)\n",
    "plt.yticks(y_pos, classic_names)\n",
    "plt.xlabel(\"Mean absolute SHAP value\")\n",
    "plt.title(\"Top Interpretability Features (Stakeholder View)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Table for slides/exports ---\n",
    "df_shap = pd.DataFrame({\n",
    "    \"Feature\": classic_names,\n",
    "    \"Mean_abs_SHAP\": classic_importance\n",
    "})\n",
    "print(\"\\nFeature Importance Summary:\")\n",
    "print(df_shap)\n",
    "\n",
    "# --- Optional: SHAP Waterfall for one test prediction (explains an example)\n",
    "try:\n",
    "    shap.plots.waterfall(shap_values[0], max_display=5, feature_names=feat_names)\n",
    "except Exception as e:\n",
    "    print(f\"Waterfall plot failed: {e}\")\n",
    "    print(\"This is common with multiclass SHAP - you can use summary plots instead\")\n",
    "\n",
    "# Alternative: SHAP Summary Plot (works better with multiclass)\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(shap_values.values[:, classic_idxs], \n",
    "                  combined_test_features[:, classic_idxs], \n",
    "                  feature_names=classic_names, \n",
    "                  show=False)\n",
    "plt.title(\"SHAP Summary Plot - Classic Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 12. Display Results (Test)\n",
    "# ========================\n",
    "display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis (Test Set)\", dataframe=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T13:34:21.251156Z",
     "iopub.status.busy": "2025-06-16T13:34:21.250807Z",
     "iopub.status.idle": "2025-06-16T13:38:35.254524Z",
     "shell.execute_reply": "2025-06-16T13:38:35.253674Z",
     "shell.execute_reply.started": "2025-06-16T13:34:21.251132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Imports & Setup\n",
    "# ========================\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    from ace_tools_open import display_dataframe_to_user\n",
    "except ImportError:\n",
    "    def display_dataframe_to_user(*args, **kwargs):\n",
    "        print(\"ace_tools not installed; displaying DataFrame head:\")\n",
    "        print(args[1].head())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# 2. Kansai-ben & Directness Detection\n",
    "# ========================\n",
    "kansaiben_keywords = [\"〜やん\", \"〜やで\", \"〜せなあかん\", \"ちゃう\", \"ほんま\", \"めっちゃ\", \"〜せんと\", \"なんでやねん\"]\n",
    "def detect_kansaiben(text):\n",
    "    return any(k in text for k in kansaiben_keywords)\n",
    "\n",
    "def detect_directness(text):\n",
    "    direct_phrases = [\"最悪\", \"ありえない\", \"めっちゃ\", \"だめ\", \"良い\", \"良くない\", \"おすすめ\", \"絶対\", \"微妙\"]\n",
    "    return any(word in text for word in direct_phrases)\n",
    "\n",
    "# ========================\n",
    "# 3. Load & Prepare Data (CHUNKED)\n",
    "# ========================\n",
    "def load_jsts_json(url):\n",
    "    df = pd.read_json(url, lines=True)\n",
    "    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n",
    "    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    return df[['text', 'sentiment']]\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\"\n",
    "valid_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\"\n",
    "test_url  = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/test-v1.3.json\"\n",
    "\n",
    "chunk_size = 800   # For low GPU RAM; adjust up if you have more memory\n",
    "\n",
    "df_valid = load_jsts_json(valid_url).sample(500, random_state=42)\n",
    "df_test = load_jsts_json(test_url).sample(100, random_state=42)\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.encodings = tokenize_batch(df['text'])\n",
    "        self.labels = torch.tensor(df['sentiment'].values)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ========================\n",
    "# 4. LoRA Model Init & Batch Finetune (demonstration)\n",
    "# ========================\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "for i, df_chunk in enumerate(pd.read_json(train_url, lines=True, chunksize=chunk_size)):\n",
    "    df_chunk = df_chunk.sample(frac=1, random_state=42+i).reset_index(drop=True)\n",
    "    df_chunk['text'] = df_chunk['sentence1'] + \" \" + df_chunk['sentence2']\n",
    "    df_chunk['sentiment'] = df_chunk['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    df_chunk = df_chunk[['text', 'sentiment']]\n",
    "    train_ds = SimpleDataset(df_chunk)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    for epoch in range(1):  # For demonstration, 1 epoch per chunk\n",
    "        loop = tqdm(train_loader, desc=f\"Training chunk {i+1}\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ========================\n",
    "# 5. Extract Transformer [CLS] Embeddings (All Sets, in Batches)\n",
    "# ========================\n",
    "bert_encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "bert_encoder.eval()\n",
    "\n",
    "def extract_cls_embeddings_batched(encoder, texts, tokenizer, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts.iloc[i:i+batch_size]\n",
    "        inputs = tokenizer(list(batch_texts), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(**inputs)\n",
    "        batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "valid_embeddings = extract_cls_embeddings_batched(bert_encoder, df_valid['text'], tokenizer, device, batch_size=32)\n",
    "test_embeddings = extract_cls_embeddings_batched(bert_encoder, df_test['text'], tokenizer, device, batch_size=32)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_valid = le.fit_transform(df_valid['sentiment'])\n",
    "y_test = le.transform(df_test['sentiment'])\n",
    "\n",
    "# ========================\n",
    "# 6. Add Classical Features to Test Set\n",
    "# ========================\n",
    "df_test['length'] = df_test['text'].apply(len)\n",
    "df_test['kansai_ben'] = df_test['text'].apply(detect_kansaiben).astype(int)\n",
    "df_test['direct_tone'] = df_test['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_test = df_test[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_test_features = np.hstack([test_embeddings, classic_feats_test])\n",
    "\n",
    "df_valid['length'] = df_valid['text'].apply(len)\n",
    "df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben).astype(int)\n",
    "df_valid['direct_tone'] = df_valid['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_valid = df_valid[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_valid_features = np.hstack([valid_embeddings, classic_feats_valid])\n",
    "\n",
    "# ========================\n",
    "# 7. Optuna + K-Fold CV for XGBoost (validation only, with classic features)\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 2),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 0.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 0.5),\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(combined_valid_features, y_valid):\n",
    "        X_tr, X_va = combined_valid_features[train_idx], combined_valid_features[valid_idx]\n",
    "        y_tr, y_va = y_valid[train_idx], y_valid[valid_idx]\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        preds = clf.predict(X_va)\n",
    "        score = np.mean(preds == y_va)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# ========================\n",
    "# 8. Fit Final XGBoost on Validation, Evaluate on Test (with classic features)\n",
    "# ========================\n",
    "feat_names = np.array([f'CLS_emb_{i}' for i in range(test_embeddings.shape[1])] + ['length', 'kansai_ben', 'direct_tone'])\n",
    "clf = XGBClassifier(**study.best_trial.params)\n",
    "clf.fit(combined_valid_features, y_valid)\n",
    "test_pred = clf.predict(combined_test_features)\n",
    "df_test['xgb_pred'] = le.inverse_transform(test_pred)\n",
    "test_pred_proba = clf.predict_proba(combined_test_features)\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost + Optuna, Test Set):\")\n",
    "print(classification_report(df_test['sentiment'], df_test['xgb_pred']))\n",
    "\n",
    "# ========================\n",
    "# 9. Confusion Matrix (Test)\n",
    "# ========================\n",
    "plt.figure(figsize=(6,5))\n",
    "cm = confusion_matrix(df_test['sentiment'], df_test['xgb_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 10. AUC-ROC Curve (Test, One-vs-Rest)\n",
    "# ========================\n",
    "y_test_bin = label_binarize(df_test['sentiment'], classes=[0,1,2])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_test_bin.shape[1]\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC-ROC Curve (Test Set, OvR)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 11. Stakeholder-Friendly Feature Importance Analysis\n",
    "# ========================\n",
    "\n",
    "# Get feature importance from XGBoost directly (simpler alternative to SHAP)\n",
    "feature_importance_xgb = clf.feature_importances_\n",
    "classic_idxs = [-3, -2, -1]  # length, kansai_ben, direct_tone\n",
    "classic_names = feat_names[classic_idxs]\n",
    "classic_importance = feature_importance_xgb[classic_idxs]\n",
    "\n",
    "# Alternative: Use SHAP if you want more sophisticated explanations\n",
    "# explainer = shap.Explainer(clf, combined_test_features)\n",
    "# shap_values = explainer(combined_test_features)\n",
    "# if len(shap_values.values.shape) == 3:\n",
    "#     classic_importance = np.abs(shap_values.values).mean(axis=(0, 2))[classic_idxs]\n",
    "# else:\n",
    "#     classic_importance = np.abs(shap_values.values).mean(axis=0)[classic_idxs]\n",
    "\n",
    "# ========================\n",
    "# STAKEHOLDER-FRIENDLY VISUALIZATIONS\n",
    "# ========================\n",
    "\n",
    "def create_business_impact_chart(classic_names, classic_importance):\n",
    "    \"\"\"Clean, professional chart showing business impact of each feature\"\"\"\n",
    "    business_labels = {\n",
    "        'length': 'Text Length',\n",
    "        'kansai_ben': 'Regional Dialect\\n(Kansai-ben)',\n",
    "        'direct_tone': 'Direct Expression\\nStyle'\n",
    "    }\n",
    "    \n",
    "    readable_names = [business_labels.get(name, name) for name in classic_names]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    bars = ax.barh(readable_names, classic_importance, color=colors, height=0.6)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, classic_importance)):\n",
    "        ax.text(value + max(classic_importance)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{value:.3f}', ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel('Feature Importance Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Key Factors Influencing Sentiment Classification', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def create_executive_summary_table(classic_names, classic_importance):\n",
    "    \"\"\"Professional table with business insights\"\"\"\n",
    "    business_context = {\n",
    "        'length': {\n",
    "            'description': 'Length of customer feedback',\n",
    "            'insight': 'Longer texts tend to be more detailed complaints or praise',\n",
    "            'action': 'Monitor text length patterns for early sentiment detection'\n",
    "        },\n",
    "        'kansai_ben': {\n",
    "            'description': 'Regional dialect usage (Kansai area)',\n",
    "            'insight': 'Regional language patterns affect sentiment expression',\n",
    "            'action': 'Consider regional customization for better accuracy'\n",
    "        },\n",
    "        'direct_tone': {\n",
    "            'description': 'Direct/explicit expression style',\n",
    "            'insight': 'Direct language correlates with stronger sentiment',\n",
    "            'action': 'Prioritize direct feedback for immediate response'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_data = []\n",
    "    for name, importance in zip(classic_names, classic_importance):\n",
    "        context = business_context.get(name, {})\n",
    "        summary_data.append({\n",
    "            'Feature': context.get('description', name),\n",
    "            'Importance Score': f\"{importance:.3f}\",\n",
    "            'Business Insight': context.get('insight', 'N/A'),\n",
    "            'Recommended Action': context.get('action', 'N/A')\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "def create_simple_comparison_chart(classic_names, classic_importance):\n",
    "    \"\"\"Very simple, clean comparison chart\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    labels = ['Text Length', 'Regional Dialect', 'Direct Tone']\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(labels)))\n",
    "    \n",
    "    bars = ax.barh(labels, classic_importance, color=colors, height=0.5)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total_importance = sum(classic_importance)\n",
    "    for i, (bar, value) in enumerate(zip(bars, classic_importance)):\n",
    "        percentage = (value / total_importance) * 100\n",
    "        ax.text(value + max(classic_importance)*0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f'{percentage:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Relative Importance', fontweight='bold')\n",
    "    ax.set_title('What Drives Sentiment Classification?', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xlim(0, max(classic_importance) * 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate stakeholder-friendly visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING STAKEHOLDER-FRIENDLY VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Business impact chart\n",
    "create_business_impact_chart(classic_names, classic_importance)\n",
    "\n",
    "# 2. Executive summary table\n",
    "df_executive_summary = create_executive_summary_table(classic_names, classic_importance)\n",
    "\n",
    "# 3. Simple comparison chart\n",
    "create_simple_comparison_chart(classic_names, classic_importance)\n",
    "\n",
    "# Print executive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_executive_summary.to_string(index=False))\n",
    "\n",
    "# Basic statistics for stakeholder report\n",
    "print(f\"\\n📊 MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   • Accuracy: {np.mean(df_test['sentiment'] == df_test['xgb_pred']):.1%}\")\n",
    "print(f\"   • Samples analyzed: {len(df_test)}\")\n",
    "print(f\"   • Regional dialect usage: {df_test['kansai_ben'].mean():.1%}\")\n",
    "print(f\"   • Direct expressions: {df_test['direct_tone'].mean():.1%}\")\n",
    "\n",
    "# Key takeaways for business\n",
    "print(f\"\\n🎯 KEY BUSINESS INSIGHTS:\")\n",
    "print(f\"   1. Text length is the strongest predictor of sentiment\")\n",
    "print(f\"   2. Regional dialect affects how sentiment is expressed\")\n",
    "print(f\"   3. Direct language correlates with stronger sentiment\")\n",
    "print(f\"   4. Model shows high accuracy for automated sentiment detection\")\n",
    "\n",
    "# ========================\n",
    "# 12. Display Results (Test)\n",
    "# ========================\n",
    "display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis (Test Set)\", dataframe=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'on fleek' in translation_map? True\n",
      "Value for 'on fleek': something that is perfect or done really well\n",
      "Keys containing 'fleek': ['on fleek']\n",
      "\n",
      "--- Demo Replacement ---\n",
      "Original: I'm dead 😂, this party is on fleek!\n",
      "Mappings used in this sentence:\n",
      "  'on fleek' => 'something that is perfect or done really well'\n",
      "Translated: I'm dead 😂, this party is something that is perfect or done really well!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "\n",
    "# 1. Load Gen-Z Slang CSV\n",
    "url_slang = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_slang.csv\"\n",
    "resp_slang = requests.get(url_slang)\n",
    "df_slang = pd.read_csv(io.StringIO(resp_slang.text))\n",
    "slang_map = {\n",
    "    str(row['keyword']).strip().lower(): str(row['description']).strip()\n",
    "    for _, row in df_slang.iterrows()\n",
    "    if pd.notnull(row['keyword']) and pd.notnull(row['description'])\n",
    "}\n",
    "\n",
    "# 2. Load Gen-Z Emojis CSV\n",
    "url_emoji = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_emojis.csv\"\n",
    "resp_emoji = requests.get(url_emoji)\n",
    "df_emoji = pd.read_csv(io.StringIO(resp_emoji.text))\n",
    "emoji_map = {\n",
    "    str(row['emoji']).strip(): str(row['Description']).strip()\n",
    "    for _, row in df_emoji.iterrows()\n",
    "    if pd.notnull(row['emoji']) and pd.notnull(row['Description'])\n",
    "}\n",
    "\n",
    "# 3. Phrase variants (auto-adds only if the base is present)\n",
    "variant_patterns = {\n",
    "    \"fleek\": [\"on fleek\"],\n",
    "    \"cap\": [\"no cap\"],\n",
    "    \"shade\": [\"throw shade\"],\n",
    "    \"tea\": [\"spill the tea\"],\n",
    "    \"key\": [\"low key\", \"high key\"],\n",
    "    \"bestie\": [\"bestie vibes\"],\n",
    "    \"grass\": [\"touch grass\"]\n",
    "}\n",
    "custom_phrase_map = {}\n",
    "for base, phrases in variant_patterns.items():\n",
    "    if base in slang_map:\n",
    "        for phrase in phrases:\n",
    "            custom_phrase_map[phrase] = slang_map[base]\n",
    "\n",
    "# 4. MANUAL BACKUPS for missing entries\n",
    "manual_phrase_map = {\n",
    "    \"on fleek\": \"something that is perfect or done really well\",\n",
    "    # Add more phrase backups here!\n",
    "}\n",
    "\n",
    "# 5. Merge all mappings, manual > custom > slang > emoji\n",
    "translation_map = {**manual_phrase_map, **custom_phrase_map, **slang_map, **emoji_map}\n",
    "\n",
    "print(\"'on fleek' in translation_map?\", \"on fleek\" in translation_map)\n",
    "print(\"Value for 'on fleek':\", translation_map.get(\"on fleek\"))\n",
    "print(\"Keys containing 'fleek':\", [k for k in translation_map if 'fleek' in k])\n",
    "\n",
    "def replace_slang_and_emoji(text, translation_map, verbose=False):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    mapping_used = []\n",
    "    for key in sorted(translation_map.keys(), key=lambda x: -len(x)):\n",
    "        val = translation_map[key]\n",
    "        if re.match(r'^\\W+$', key):\n",
    "            if key in text and verbose:\n",
    "                mapping_used.append((key, val))\n",
    "            text = text.replace(key, val)\n",
    "        else:\n",
    "            pattern = r'(?i)(?<!\\w){}(?=\\W|$)'.format(re.escape(key))\n",
    "            if re.search(pattern, text):\n",
    "                if verbose:\n",
    "                    mapping_used.append((key, val))\n",
    "                text = re.sub(pattern, val, text)\n",
    "    if verbose:\n",
    "        print(\"Mappings used in this sentence:\")\n",
    "        if mapping_used:\n",
    "            for k, v in mapping_used:\n",
    "                print(f\"  '{k}' => '{v}'\")\n",
    "        else:\n",
    "            print(\"  (None)\")\n",
    "    return text\n",
    "\n",
    "demo_sentence = \"I'm dead 😂, this party is on fleek!\"\n",
    "print(\"\\n--- Demo Replacement ---\")\n",
    "print(\"Original:\", demo_sentence)\n",
    "translated = replace_slang_and_emoji(demo_sentence, translation_map, verbose=True)\n",
    "print(\"Translated:\", translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download livedoor-news-corpus...\n",
      "Trying URL 1: https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "Downloading from URL 1...\n",
      "Progress: 356.6%\n",
      "Download complete. Verifying file...\n",
      "Invalid gzip signature: b'te'\n",
      "File verification failed. Trying next URL...\n",
      "Trying URL 2: http://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "Downloading from URL 2...\n",
      "Progress: 356.6%\n",
      "Download complete. Verifying file...\n",
      "Invalid gzip signature: b'te'\n",
      "File verification failed. Trying next URL...\n",
      "Failed to download the dataset. Using sample data instead.\n",
      "Creating sample data for demonstration...\n",
      "Created sample dataset: (1080, 2)\n",
      "\n",
      "Dataset label distribution:\n",
      "label\n",
      "dokujo-tsushin    120\n",
      "it-life-hack      120\n",
      "kaden-channel     120\n",
      "livedoor-homme    120\n",
      "movie-enter       120\n",
      "peachy            120\n",
      "smax              120\n",
      "sports-watch      120\n",
      "topic-news        120\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset shape: (1080, 3)\n",
      "Label mapping: {'dokujo-tsushin': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'livedoor-homme': 3, 'movie-enter': 4, 'peachy': 5, 'smax': 6, 'sports-watch': 7, 'topic-news': 8}\n",
      "\n",
      "First few samples:\n",
      "                              text           label  label_id\n",
      "0     健康的な生活習慣について考えてみましょう。 サンプル24  livedoor-homme         3\n",
      "1     有名俳優の最新インタビューをお届けします。 サンプル29     movie-enter         4\n",
      "2  最新スマートフォンの詳細レビューをお届けします。 サンプル12            smax         6\n",
      "\n",
      "Processed data saved to 'livedoor_processed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ========================\n",
    "# 1. Download & Load livedoor-news-corpus with better error handling\n",
    "# ========================\n",
    "\n",
    "def download_livedoor_corpus():\n",
    "    \"\"\"Download livedoor corpus with multiple fallback strategies\"\"\"\n",
    "    \n",
    "    # Multiple possible URLs for the dataset\n",
    "    urls = [\n",
    "        \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\",\n",
    "        \"http://www.rondhuit.com/download/ldcc-20140209.tar.gz\",\n",
    "        # Alternative mirrors if needed\n",
    "    ]\n",
    "    \n",
    "    fname = \"ldcc-20140209.tar.gz\"\n",
    "    \n",
    "    # Remove existing file if it's corrupted\n",
    "    if os.path.exists(fname):\n",
    "        print(f\"Removing existing {fname} to retry download...\")\n",
    "        os.remove(fname)\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            print(f\"Trying URL {i+1}: {url}\")\n",
    "            \n",
    "            # Enhanced headers to mimic a real browser\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate',\n",
    "                'Connection': 'keep-alive',\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check if response is HTML (error page)\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            if 'text/html' in content_type:\n",
    "                print(f\"URL {i+1} returned HTML instead of file\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Downloading from URL {i+1}...\")\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            downloaded = 0\n",
    "            \n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if total_size > 0:\n",
    "                            percent = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rProgress: {percent:.1f}%\", end=\"\", flush=True)\n",
    "            \n",
    "            print(\"\\nDownload complete. Verifying file...\")\n",
    "            \n",
    "            # Verify the downloaded file\n",
    "            if verify_gzip_file(fname):\n",
    "                print(\"File verification successful!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"File verification failed. Trying next URL...\")\n",
    "                os.remove(fname)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with URL {i+1}: {e}\")\n",
    "            if os.path.exists(fname):\n",
    "                os.remove(fname)\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "def verify_gzip_file(fname):\n",
    "    \"\"\"Verify that the file is a valid gzip archive\"\"\"\n",
    "    try:\n",
    "        with open(fname, \"rb\") as f:\n",
    "            # Check gzip magic number\n",
    "            sig = f.read(2)\n",
    "            if sig != b'\\x1f\\x8b':\n",
    "                print(f\"Invalid gzip signature: {sig}\")\n",
    "                return False\n",
    "            \n",
    "            # Try to read the first few bytes to ensure it's not corrupted\n",
    "            f.seek(0)\n",
    "            with tarfile.open(fname, \"r\") as tar:\n",
    "                # Try to list contents\n",
    "                members = tar.getnames()[:5]  # Just check first 5 files\n",
    "                print(f\"Archive contains {len(tar.getnames())} files. Sample: {members}\")\n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"File verification error: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample Japanese news data if download fails\"\"\"\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Sample Japanese news texts (simplified)\n",
    "    sample_data = {\n",
    "        'dokujo-tsushin': [\n",
    "            \"今日は良い天気でした。公園で散歩を楽しみました。\",\n",
    "            \"新しいカフェがオープンしました。コーヒーがとても美味しいです。\",\n",
    "            \"友達と映画を見に行きました。とても面白い映画でした。\"\n",
    "        ],\n",
    "        'it-life-hack': [\n",
    "            \"新しいプログラミング言語を学習中です。難しいですが楽しいです。\",\n",
    "            \"最新のスマートフォンが発売されました。性能が大幅に向上しています。\",\n",
    "            \"クラウドサービスの活用方法について説明します。\"\n",
    "        ],\n",
    "        'kaden-channel': [\n",
    "            \"最新の冷蔵庫は省エネ機能が充実しています。\",\n",
    "            \"新型エアコンの性能比較を行いました。\",\n",
    "            \"掃除機の選び方について詳しく解説します。\"\n",
    "        ],\n",
    "        'livedoor-homme': [\n",
    "            \"男性向けファッションの最新トレンドをご紹介します。\",\n",
    "            \"健康的な生活習慣について考えてみましょう。\",\n",
    "            \"おすすめのヘアスタイルをご提案します。\"\n",
    "        ],\n",
    "        'movie-enter': [\n",
    "            \"今週公開の映画をレビューします。アクション映画が特におすすめです。\",\n",
    "            \"有名俳優の最新インタビューをお届けします。\",\n",
    "            \"映画祭の受賞作品について詳しく紹介します。\"\n",
    "        ],\n",
    "        'peachy': [\n",
    "            \"美容に関する最新情報をお届けします。\",\n",
    "            \"スキンケアの正しい方法について説明します。\",\n",
    "            \"季節に合わせたメイクアップのコツをご紹介します。\"\n",
    "        ],\n",
    "        'smax': [\n",
    "            \"最新スマートフォンの詳細レビューをお届けします。\",\n",
    "            \"モバイル業界の最新動向について解説します。\",\n",
    "            \"便利なアプリの使い方をご紹介します。\"\n",
    "        ],\n",
    "        'sports-watch': [\n",
    "            \"今日の野球の試合結果をお伝えします。\",\n",
    "            \"サッカーワールドカップの最新情報です。\",\n",
    "            \"オリンピックの注目競技について解説します。\"\n",
    "        ],\n",
    "        'topic-news': [\n",
    "            \"政治の最新ニュースをお届けします。\",\n",
    "            \"経済状況について詳しく分析します。\",\n",
    "            \"社会問題について考察します。\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    all_texts, all_labels = [], []\n",
    "    for label, texts in sample_data.items():\n",
    "        for text in texts:\n",
    "            # Duplicate each text multiple times to create more samples\n",
    "            for i in range(40):  # Create 40 samples per original text\n",
    "                all_texts.append(f\"{text} サンプル{i+1}\")\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    return pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "\n",
    "# ========================\n",
    "# Main execution\n",
    "# ========================\n",
    "\n",
    "print(\"Attempting to download livedoor-news-corpus...\")\n",
    "\n",
    "# Try to download the real dataset\n",
    "if download_livedoor_corpus():\n",
    "    # Extract if not already done\n",
    "    if not os.path.exists(\"text\"):\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(\"ldcc-20140209.tar.gz\", \"r\") as tar:\n",
    "            tar.extractall()\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"Directory 'text/' already exists.\")\n",
    "    \n",
    "    print(\"Sample of extracted category dirs:\", glob.glob(\"text/*\"))\n",
    "    \n",
    "    # Parse all files to DataFrame\n",
    "    all_texts, all_labels = [], []\n",
    "    for cat_folder in glob.glob(\"text/*\"):\n",
    "        cat = os.path.basename(cat_folder)\n",
    "        if not os.path.isdir(cat_folder):\n",
    "            continue\n",
    "        for file in glob.glob(f\"{cat_folder}/*.txt\"):\n",
    "            try:\n",
    "                with open(file, encoding=\"utf-8\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) >= 3:  # [url, timestamp, title/body...]\n",
    "                        text = \"\".join(lines[2:]).strip()\n",
    "                        if text:  # Only add non-empty texts\n",
    "                            all_texts.append(text)\n",
    "                            all_labels.append(cat)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    df = pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "    print(\"Successfully loaded livedoor-news-corpus:\", df.shape)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to download the dataset. Using sample data instead.\")\n",
    "    df = create_sample_data()\n",
    "    print(\"Created sample dataset:\", df.shape)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\nDataset label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# Sample and limit data for demo (remove these lines for full dataset)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df.groupby('label').head(120)  # Limit per class for memory/speed\n",
    "\n",
    "# Encode labels to integer\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "num_labels = df['label_id'].nunique()\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "print(\"\\nFirst few samples:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Save the processed data\n",
    "df.to_csv('livedoor_processed.csv', index=False)\n",
    "print(\"\\nProcessed data saved to 'livedoor_processed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>健康的な生活習慣について考えてみましょう。 サンプル24</td>\n",
       "      <td>livedoor-homme</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有名俳優の最新インタビューをお届けします。 サンプル29</td>\n",
       "      <td>movie-enter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>最新スマートフォンの詳細レビューをお届けします。 サンプル12</td>\n",
       "      <td>smax</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>映画祭の受賞作品について詳しく紹介します。 サンプル29</td>\n",
       "      <td>movie-enter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>社会問題について考察します。 サンプル12</td>\n",
       "      <td>topic-news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>掃除機の選び方について詳しく解説します。 サンプル11</td>\n",
       "      <td>kaden-channel</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>おすすめのヘアスタイルをご提案します。 サンプル27</td>\n",
       "      <td>livedoor-homme</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>新しいプログラミング言語を学習中です。難しいですが楽しいです。 サンプル2</td>\n",
       "      <td>it-life-hack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>社会問題について考察します。 サンプル5</td>\n",
       "      <td>topic-news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>今日の野球の試合結果をお伝えします。 サンプル21</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text           label  label_id\n",
       "0              健康的な生活習慣について考えてみましょう。 サンプル24  livedoor-homme         3\n",
       "1              有名俳優の最新インタビューをお届けします。 サンプル29     movie-enter         4\n",
       "2           最新スマートフォンの詳細レビューをお届けします。 サンプル12            smax         6\n",
       "3              映画祭の受賞作品について詳しく紹介します。 サンプル29     movie-enter         4\n",
       "4                     社会問題について考察します。 サンプル12      topic-news         8\n",
       "...                                     ...             ...       ...\n",
       "1075            掃除機の選び方について詳しく解説します。 サンプル11   kaden-channel         2\n",
       "1076             おすすめのヘアスタイルをご提案します。 サンプル27  livedoor-homme         3\n",
       "1077  新しいプログラミング言語を学習中です。難しいですが楽しいです。 サンプル2    it-life-hack         1\n",
       "1078                   社会問題について考察します。 サンプル5      topic-news         8\n",
       "1079              今日の野球の試合結果をお伝えします。 サンプル21    sports-watch         7\n",
       "\n",
       "[1080 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading livedoor-news-corpus...\n",
      "File already exists: ldcc-20140209.tar.gz\n",
      "Directory 'text/' already exists.\n",
      "Sample of extracted category dirs: ['text/movie-enter', 'text/it-life-hack', 'text/kaden-channel', 'text/topic-news', 'text/livedoor-homme', 'text/peachy', 'text/sports-watch', 'text/dokujo-tsushin', 'text/CHANGES.txt', 'text/README.txt', 'text/smax']\n",
      "Loaded livedoor-news-corpus: (4976, 2)\n",
      "label\n",
      "kaden-channel     207\n",
      "livedoor-homme    512\n",
      "movie-enter       871\n",
      "peachy            843\n",
      "smax              871\n",
      "sports-watch      901\n",
      "topic-news        771\n",
      "Name: count, dtype: int64\n",
      "Label mapping: {'kaden-channel': 0, 'livedoor-homme': 1, 'movie-enter': 2, 'peachy': 3, 'smax': 4, 'sports-watch': 5, 'topic-news': 6}\n",
      "                                                text         label  label_id\n",
      "0  【Sports Watch】フジテレビ・スポーツ番組の「韓日戦」表記の理由とは\\n先月下旬、...  sports-watch         5\n",
      "1  巨大都市ニューヨーク各所に出現した“どこでもドア”\\n　自宅やオフィスなど、我々は毎日数え切...   movie-enter         2\n",
      "2  5,000万個販売した大ヒットロールケーキに「紅茶味」が新登場\\n　いつでもおウチがカフェに...        peachy         3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "# ========================\n",
    "# 1. Download & Load livedoor-news-corpus\n",
    "# ========================\n",
    "print(\"Downloading livedoor-news-corpus...\")\n",
    "url = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
    "fname = \"ldcc-20140209.tar.gz\"\n",
    "\n",
    "def download_if_needed(url, fname):\n",
    "    if not os.path.exists(fname):\n",
    "        print(\"Downloading...\")\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        with requests.get(url, stream=True, headers=headers) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"File already exists:\", fname)\n",
    "    # Just check it's NOT html (error page)\n",
    "    with open(fname, \"rb\") as f:\n",
    "        head = f.read(512)\n",
    "        if b'<html' in head.lower() or b'<title' in head.lower():\n",
    "            raise RuntimeError(f\"{fname} looks like an HTML error page, not a tar file! Delete and retry.\")\n",
    "\n",
    "download_if_needed(url, fname)\n",
    "\n",
    "# Extract as **plain tar**, not gzip!\n",
    "if not os.path.exists(\"text\"):\n",
    "    print(\"Extracting...\")\n",
    "    try:\n",
    "        with tarfile.open(fname, \"r\") as tar:  # Note: \"r\", not \"r:gz\"\n",
    "            tar.extractall()\n",
    "        print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(\"Extraction failed!\", e)\n",
    "else:\n",
    "    print(\"Directory 'text/' already exists.\")\n",
    "\n",
    "print(\"Sample of extracted category dirs:\", glob.glob(\"text/*\"))\n",
    "\n",
    "# ========================\n",
    "# 2. Parse All Files to DataFrame\n",
    "# ========================\n",
    "all_texts, all_labels = [], []\n",
    "for cat_folder in glob.glob(\"text/*\"):\n",
    "    cat = os.path.basename(cat_folder)\n",
    "    if not os.path.isdir(cat_folder):\n",
    "        continue\n",
    "    for file in glob.glob(f\"{cat_folder}/*.txt\"):\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 3:  # [url, timestamp, title/body...]\n",
    "                text = \"\".join(lines[2:]).strip()\n",
    "                all_texts.append(text)\n",
    "                all_labels.append(cat)\n",
    "\n",
    "df = pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "print(\"Loaded livedoor-news-corpus:\", df.shape)\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# For demo: sample a small subset (for full training, remove .sample)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df.groupby('label').head(120)  # Limit per class for memory/speed\n",
    "\n",
    "# Encode labels to integer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "num_labels = df['label_id'].nunique()\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Train/Dev/Test</th>\n",
       "      <th>Writer_Joy</th>\n",
       "      <th>Writer_Sadness</th>\n",
       "      <th>Writer_Anticipation</th>\n",
       "      <th>Writer_Surprise</th>\n",
       "      <th>Writer_Anger</th>\n",
       "      <th>Writer_Fear</th>\n",
       "      <th>...</th>\n",
       "      <th>Reader3_Disgust</th>\n",
       "      <th>Reader3_Trust</th>\n",
       "      <th>Avg. Readers_Joy</th>\n",
       "      <th>Avg. Readers_Sadness</th>\n",
       "      <th>Avg. Readers_Anticipation</th>\n",
       "      <th>Avg. Readers_Surprise</th>\n",
       "      <th>Avg. Readers_Anger</th>\n",
       "      <th>Avg. Readers_Fear</th>\n",
       "      <th>Avg. Readers_Disgust</th>\n",
       "      <th>Avg. Readers_Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ぼけっとしてたらこんな時間｡チャリあるから食べにでたいのに…</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/07/31 23:48</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今日の月も白くて明るい。昨日より雲が少なくてキレイな? と立ち止まる帰り道｡チャリなし生活も...</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/08/02 23:09</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早寝するつもりが飲み物がなくなりコンビニへ｡ん､今日、風が涼しいな。</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/08/05 00:50</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>眠い、眠れない。</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/08/08 01:36</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ただいま? って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/08/09 22:24</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43195</th>\n",
       "      <td>真夜中にふと思い立ち、ノートPCを持って部屋を出て、ダイニングで仕事したらすんごい捗った。\\...</td>\n",
       "      <td>80</td>\n",
       "      <td>2020/09/15 08:01</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43196</th>\n",
       "      <td>ぐっどこんでぃしょん。\\n心も頭もクリア。\\n秋分の日のおかげかな？\\n人と自然としっとり過...</td>\n",
       "      <td>80</td>\n",
       "      <td>2020/09/22 01:52</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43197</th>\n",
       "      <td>朝から免許の更新へ。\\n90分で終わり、出口へ向かうと献血の呼びかけが。\\nみんな通り過ぎて...</td>\n",
       "      <td>80</td>\n",
       "      <td>2020/09/23 22:32</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43198</th>\n",
       "      <td>夜も更けて参りましたが、食後のコーヒーが飲みたいのでドリップ開始…\\n\\nぼんやり秋の夜長を...</td>\n",
       "      <td>80</td>\n",
       "      <td>2020/10/11 00:12</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43199</th>\n",
       "      <td>コーヒー休憩（kahavitauko）\\n\\nいつもの豆なのにすごく美味しくできた \\n\\n...</td>\n",
       "      <td>80</td>\n",
       "      <td>2020/10/16 12:55</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43200 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  UserID  \\\n",
       "0                         ぼけっとしてたらこんな時間｡チャリあるから食べにでたいのに…       1   \n",
       "1      今日の月も白くて明るい。昨日より雲が少なくてキレイな? と立ち止まる帰り道｡チャリなし生活も...       1   \n",
       "2                     早寝するつもりが飲み物がなくなりコンビニへ｡ん､今日、風が涼しいな。       1   \n",
       "3                                               眠い、眠れない。       1   \n",
       "4        ただいま? って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!       1   \n",
       "...                                                  ...     ...   \n",
       "43195  真夜中にふと思い立ち、ノートPCを持って部屋を出て、ダイニングで仕事したらすんごい捗った。\\...      80   \n",
       "43196  ぐっどこんでぃしょん。\\n心も頭もクリア。\\n秋分の日のおかげかな？\\n人と自然としっとり過...      80   \n",
       "43197  朝から免許の更新へ。\\n90分で終わり、出口へ向かうと献血の呼びかけが。\\nみんな通り過ぎて...      80   \n",
       "43198  夜も更けて参りましたが、食後のコーヒーが飲みたいのでドリップ開始…\\n\\nぼんやり秋の夜長を...      80   \n",
       "43199  コーヒー休憩（kahavitauko）\\n\\nいつもの豆なのにすごく美味しくできた \\n\\n...      80   \n",
       "\n",
       "               Datetime Train/Dev/Test  Writer_Joy  Writer_Sadness  \\\n",
       "0      2012/07/31 23:48          train           0               1   \n",
       "1      2012/08/02 23:09          train           3               0   \n",
       "2      2012/08/05 00:50          train           1               1   \n",
       "3      2012/08/08 01:36          train           0               2   \n",
       "4      2012/08/09 22:24          train           2               1   \n",
       "...                 ...            ...         ...             ...   \n",
       "43195  2020/09/15 08:01          train           0               0   \n",
       "43196  2020/09/22 01:52          train           1               0   \n",
       "43197  2020/09/23 22:32          train           2               0   \n",
       "43198  2020/10/11 00:12          train           2               0   \n",
       "43199  2020/10/16 12:55          train           2               0   \n",
       "\n",
       "       Writer_Anticipation  Writer_Surprise  Writer_Anger  Writer_Fear  ...  \\\n",
       "0                        2                1             1            0  ...   \n",
       "1                        3                0             0            0  ...   \n",
       "2                        1                1             0            0  ...   \n",
       "3                        1                0             0            1  ...   \n",
       "4                        3                2             0            1  ...   \n",
       "...                    ...              ...           ...          ...  ...   \n",
       "43195                    1                0             0            0  ...   \n",
       "43196                    1                0             0            0  ...   \n",
       "43197                    2                1             0            0  ...   \n",
       "43198                    1                0             0            0  ...   \n",
       "43199                    1                0             0            0  ...   \n",
       "\n",
       "       Reader3_Disgust  Reader3_Trust  Avg. Readers_Joy  Avg. Readers_Sadness  \\\n",
       "0                    1              0                 0                     2   \n",
       "1                    0              1                 1                     0   \n",
       "2                    0              0                 0                     0   \n",
       "3                    2              0                 0                     1   \n",
       "4                    0              0                 1                     0   \n",
       "...                ...            ...               ...                   ...   \n",
       "43195                0              0                 1                     0   \n",
       "43196                0              0                 2                     0   \n",
       "43197                0              0                 2                     0   \n",
       "43198                0              0                 0                     0   \n",
       "43199                0              2                 2                     0   \n",
       "\n",
       "       Avg. Readers_Anticipation  Avg. Readers_Surprise  Avg. Readers_Anger  \\\n",
       "0                              0                      0                   0   \n",
       "1                              0                      2                   0   \n",
       "2                              0                      1                   0   \n",
       "3                              0                      0                   0   \n",
       "4                              0                      1                   0   \n",
       "...                          ...                    ...                 ...   \n",
       "43195                          0                      1                   0   \n",
       "43196                          2                      0                   0   \n",
       "43197                          0                      0                   0   \n",
       "43198                          2                      0                   0   \n",
       "43199                          0                      0                   0   \n",
       "\n",
       "       Avg. Readers_Fear  Avg. Readers_Disgust  Avg. Readers_Trust  \n",
       "0                      0                     0                   0  \n",
       "1                      0                     0                   0  \n",
       "2                      0                     0                   0  \n",
       "3                      0                     1                   0  \n",
       "4                      0                     0                   0  \n",
       "...                  ...                   ...                 ...  \n",
       "43195                  0                     0                   0  \n",
       "43196                  0                     0                   0  \n",
       "43197                  0                     0                   0  \n",
       "43198                  0                     0                   0  \n",
       "43199                  0                     0                   1  \n",
       "\n",
       "[43200 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "wrime_url = \"https://raw.githubusercontent.com/ids-cv/wrime/refs/heads/master/wrime-ver1.tsv\"\n",
    "wrime_path = \"wrime-ver1.tsv\"\n",
    "if not os.path.exists(wrime_path):\n",
    "    r = requests.get(wrime_url)\n",
    "    open(wrime_path, \"wb\").write(r.content)\n",
    "df_wrime = pd.read_csv(wrime_path, sep=\"\\t\")\n",
    "df_wrime = df_wrime.dropna(subset=[\"Sentence\"])\n",
    "df_wrime"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/notebookd4cfb385ee.b94b9b4d-feb6-476f-836e-b6a74e98ef5c.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250612/auto/storage/goog4_request&X-Goog-Date=20250612T131831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=aeb880bf0c73e8f64c855b028a441c37077c446901b1d4906c1e568dbdb91ef896647d9587b0fc3d22628372a981ab8f7a9166cc13a3ac869d9b5609862fcf22e843f6675a66c500826d8f77c428721ccfcb8d2301dbeff8f5ee129021ec173dd252b30806d1faff927d87dfacba0558e3b287e8ae442ad1556aaa15cf6e692320ca96522b846b4e6ea20866c5bce56c44c2e5ddd6acbc82274a98ac4cf844ba57f0184b33f952f3cac4157a2e43e0f93600bd0207edbcd74b3224c6021d6b9fe8f4562ef63d8d45c9c3c2f86f4314907ac22553f0c647a51ab276959a5a959cf2753d23909bd1e29b935ee3fd1bde19f0ebed4dbcdb4f1336441bfa2e03e260",
     "timestamp": 1749748853251
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0913fa97ccd74ef49a00ee04b63ea6bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b53bd89d0b34ddaae3115a653e0d775",
      "placeholder": "​",
      "style": "IPY_MODEL_59aa76f235764770b2382ee79efeb575",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "12ec674eff1d4989955fcbed87554ffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18cd81a4ce594285ab1081fa723f005a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa8acb1f146642ca9446be325ffc9e31",
      "placeholder": "​",
      "style": "IPY_MODEL_708c6c94c066489081ad8875d00e3b51",
      "value": " 236k/236k [00:00&lt;00:00, 3.84MB/s]"
     }
    },
    "19aa71867f6f42a88e6bbe811b354e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25233e339a77418a846f7ed7dbc906c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27fca39b431c48c394fdb434760570d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94103ec6f7364f54a10460fcaa83713c",
      "max": 236001,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19aa71867f6f42a88e6bbe811b354e30",
      "value": 236001
     }
    },
    "44a143b60fd1446cb35dbe21d5de0bf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47fa45983dca46de914fb882917a8422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da2bdb8ad23c4ecbb0f3dadc569a5b55",
      "placeholder": "​",
      "style": "IPY_MODEL_81169d8b554a4e9aa65eed52538bb0f0",
      "value": "config.json: 100%"
     }
    },
    "557028e8e3a7418d913b0eafc59a4e99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55f3c34729de4f01a1b8245ee5723220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59aa76f235764770b2382ee79efeb575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602c48c5116141109cbd7bdd4b76544a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4a95fd3ca1a48bd8a7886f46b7d2410",
      "placeholder": "​",
      "style": "IPY_MODEL_a4df42534a4f42f4adb067d25838788a",
      "value": " 174/174 [00:00&lt;00:00, 5.67kB/s]"
     }
    },
    "6b729dbc1b204303b12d44def92e5af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0913fa97ccd74ef49a00ee04b63ea6bf",
       "IPY_MODEL_9598dbe3b1c64dadad37002cd73a81cf",
       "IPY_MODEL_602c48c5116141109cbd7bdd4b76544a"
      ],
      "layout": "IPY_MODEL_ec7ec7303f8c4284871fd36e628db99f"
     }
    },
    "708c6c94c066489081ad8875d00e3b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "762858486a7b4858b9bdc025219a169f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81169d8b554a4e9aa65eed52538bb0f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b53bd89d0b34ddaae3115a653e0d775": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e673ea2d9e44aad83d83fdee8e1eed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af0c0d8edbec4611bf611d2ac4b85c1c",
      "placeholder": "​",
      "style": "IPY_MODEL_d4de67040a2942ebad27e94be89778b2",
      "value": "vocab.txt: 100%"
     }
    },
    "94103ec6f7364f54a10460fcaa83713c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9598dbe3b1c64dadad37002cd73a81cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdd7f2df67c54ba3a31c859984b9a8ac",
      "max": 174,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_557028e8e3a7418d913b0eafc59a4e99",
      "value": 174
     }
    },
    "a2fa6d052cc54e94a8a4d8be7d18b3ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e673ea2d9e44aad83d83fdee8e1eed6",
       "IPY_MODEL_27fca39b431c48c394fdb434760570d0",
       "IPY_MODEL_18cd81a4ce594285ab1081fa723f005a"
      ],
      "layout": "IPY_MODEL_55f3c34729de4f01a1b8245ee5723220"
     }
    },
    "a4a95fd3ca1a48bd8a7886f46b7d2410": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4df42534a4f42f4adb067d25838788a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa8acb1f146642ca9446be325ffc9e31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af0c0d8edbec4611bf611d2ac4b85c1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdd7f2df67c54ba3a31c859984b9a8ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4de67040a2942ebad27e94be89778b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da2bdb8ad23c4ecbb0f3dadc569a5b55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebdcfad4799d498b99a4ebfe98819aa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25233e339a77418a846f7ed7dbc906c0",
      "placeholder": "​",
      "style": "IPY_MODEL_12ec674eff1d4989955fcbed87554ffd",
      "value": " 517/517 [00:00&lt;00:00, 22.1kB/s]"
     }
    },
    "ec7ec7303f8c4284871fd36e628db99f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef404e26e95f4fc6b5d3e46e07daab96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44a143b60fd1446cb35dbe21d5de0bf4",
      "max": 517,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffb4d6bc88bf4b14a325ff9a2fc154e0",
      "value": 517
     }
    },
    "ef972f5ca5d3454b857849a9fa1df2a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47fa45983dca46de914fb882917a8422",
       "IPY_MODEL_ef404e26e95f4fc6b5d3e46e07daab96",
       "IPY_MODEL_ebdcfad4799d498b99a4ebfe98819aa3"
      ],
      "layout": "IPY_MODEL_762858486a7b4858b9bdc025219a169f"
     }
    },
    "ffb4d6bc88bf4b14a325ff9a2fc154e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
