{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 0. Install libraries\n",
    "# ========================\n",
    "!pip install --quiet numpy spacy thinc\n",
    "!pip install --quiet torch torchvision torchaudio\n",
    "!pip install --quiet transformers fugashi ipadic accelerate peft sentencepiece matplotlib seaborn tqdm\n",
    "!pip install --quiet xgboost optuna ace_tools_open shap unidic-lite mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install certifi\n",
    "!mkdir -p /usr/local/share/ca-certificates/\n",
    "!cp /etc/ssl/certs/ca-certificates.crt /usr/local/share/ca-certificates/\n",
    "!update-ca-certificates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Imports & Setup\n",
    "# ========================\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    from ace_tools_open import display_dataframe_to_user\n",
    "except ImportError:\n",
    "    def display_dataframe_to_user(*args, **kwargs):\n",
    "        print(\"ace_tools not installed; displaying DataFrame head:\")\n",
    "        print(args[1].head())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# 2. Kansai-ben & Directness Detection\n",
    "# ========================\n",
    "kansaiben_keywords = [\"ã€œã‚„ã‚“\", \"ã€œã‚„ã§\", \"ã€œã›ãªã‚ã‹ã‚“\", \"ã¡ã‚ƒã†\", \"ã»ã‚“ã¾\", \"ã‚ã£ã¡ã‚ƒ\", \"ã€œã›ã‚“ã¨\", \"ãªã‚“ã§ã‚„ã­ã‚“\"]\n",
    "def detect_kansaiben(text):\n",
    "    return any(k in text for k in kansaiben_keywords)\n",
    "\n",
    "def detect_directness(text):\n",
    "    direct_phrases = [\"æœ€æ‚ª\", \"ã‚ã‚Šãˆãªã„\", \"ã‚ã£ã¡ã‚ƒ\", \"ã ã‚\", \"è‰¯ã„\", \"è‰¯ããªã„\", \"ãŠã™ã™ã‚\", \"çµ¶å¯¾\", \"å¾®å¦™\"]\n",
    "    return any(word in text for word in direct_phrases)\n",
    "\n",
    "# ========================\n",
    "# 3. Load & Prepare Data (CHUNKED)\n",
    "# ========================\n",
    "def load_jsts_json(url):\n",
    "    df = pd.read_json(url, lines=True)\n",
    "    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n",
    "    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    return df[['text', 'sentiment']]\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\"\n",
    "valid_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\"\n",
    "test_url  = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/test-v1.3.json\"\n",
    "\n",
    "chunk_size = 800   # For low GPU RAM; adjust up if you have more memory\n",
    "\n",
    "df_valid = load_jsts_json(valid_url).sample(500, random_state=42)\n",
    "df_test = load_jsts_json(test_url).sample(100, random_state=42)\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.encodings = tokenize_batch(df['text'])\n",
    "        self.labels = torch.tensor(df['sentiment'].values)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ========================\n",
    "# 4. LoRA Model Init & Batch Finetune (demonstration)\n",
    "# ========================\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "for i, df_chunk in enumerate(pd.read_json(train_url, lines=True, chunksize=chunk_size)):\n",
    "    df_chunk = df_chunk.sample(frac=1, random_state=42+i).reset_index(drop=True)\n",
    "    df_chunk['text'] = df_chunk['sentence1'] + \" \" + df_chunk['sentence2']\n",
    "    df_chunk['sentiment'] = df_chunk['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    df_chunk = df_chunk[['text', 'sentiment']]\n",
    "    train_ds = SimpleDataset(df_chunk)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    for epoch in range(1):  # For demonstration, 1 epoch per chunk\n",
    "        loop = tqdm(train_loader, desc=f\"Training chunk {i+1}\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ========================\n",
    "# 5. Extract Transformer [CLS] Embeddings (All Sets, in Batches)\n",
    "# ========================\n",
    "bert_encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "bert_encoder.eval()\n",
    "\n",
    "def extract_cls_embeddings_batched(encoder, texts, tokenizer, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts.iloc[i:i+batch_size]\n",
    "        inputs = tokenizer(list(batch_texts), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(**inputs)\n",
    "        batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "valid_embeddings = extract_cls_embeddings_batched(bert_encoder, df_valid['text'], tokenizer, device, batch_size=32)\n",
    "test_embeddings = extract_cls_embeddings_batched(bert_encoder, df_test['text'], tokenizer, device, batch_size=32)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_valid = le.fit_transform(df_valid['sentiment'])\n",
    "y_test = le.transform(df_test['sentiment'])\n",
    "\n",
    "# ========================\n",
    "# 6. Add Classical Features to Test Set\n",
    "# ========================\n",
    "df_test['length'] = df_test['text'].apply(len)\n",
    "df_test['kansai_ben'] = df_test['text'].apply(detect_kansaiben).astype(int)\n",
    "df_test['direct_tone'] = df_test['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_test = df_test[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_test_features = np.hstack([test_embeddings, classic_feats_test])\n",
    "\n",
    "df_valid['length'] = df_valid['text'].apply(len)\n",
    "df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben).astype(int)\n",
    "df_valid['direct_tone'] = df_valid['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_valid = df_valid[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_valid_features = np.hstack([valid_embeddings, classic_feats_valid])\n",
    "\n",
    "# ========================\n",
    "# 7. Optuna + K-Fold CV for XGBoost (validation only, with classic features)\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 2),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 0.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 0.5),\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(combined_valid_features, y_valid):\n",
    "        X_tr, X_va = combined_valid_features[train_idx], combined_valid_features[valid_idx]\n",
    "        y_tr, y_va = y_valid[train_idx], y_valid[valid_idx]\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        preds = clf.predict(X_va)\n",
    "        score = np.mean(preds == y_va)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# ========================\n",
    "# 8. Fit Final XGBoost on Validation, Evaluate on Test (with classic features)\n",
    "# ========================\n",
    "feat_names = np.array([f'CLS_emb_{i}' for i in range(test_embeddings.shape[1])] + ['length', 'kansai_ben', 'direct_tone'])\n",
    "clf = XGBClassifier(**study.best_trial.params)\n",
    "clf.fit(combined_valid_features, y_valid)\n",
    "test_pred = clf.predict(combined_test_features)\n",
    "df_test['xgb_pred'] = le.inverse_transform(test_pred)\n",
    "test_pred_proba = clf.predict_proba(combined_test_features)\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost + Optuna, Test Set):\")\n",
    "print(classification_report(df_test['sentiment'], df_test['xgb_pred']))\n",
    "\n",
    "# ========================\n",
    "# 9. Confusion Matrix (Test)\n",
    "# ========================\n",
    "plt.figure(figsize=(6,5))\n",
    "cm = confusion_matrix(df_test['sentiment'], df_test['xgb_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 10. AUC-ROC Curve (Test, One-vs-Rest)\n",
    "# ========================\n",
    "y_test_bin = label_binarize(df_test['sentiment'], classes=[0,1,2])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_test_bin.shape[1]\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC-ROC Curve (Test Set, OvR)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 11. Stakeholder-Ready Feature Importance & SHAP (FIXED)\n",
    "# ========================\n",
    "explainer = shap.Explainer(clf, combined_test_features)\n",
    "shap_values = explainer(combined_test_features)\n",
    "\n",
    "# --- Stakeholder Bar Plot: Only classic features (last three) ---\n",
    "classic_idxs = [-3, -2, -1]  # length, kansai_ben, direct_tone\n",
    "classic_names = feat_names[classic_idxs]\n",
    "\n",
    "# FIX: For multiclass, SHAP values are 3D (n_samples, n_features, n_classes)\n",
    "# We need to take mean across samples AND classes to get feature importance\n",
    "if len(shap_values.values.shape) == 3:\n",
    "    # Multiclass case: take mean across samples (axis=0) and classes (axis=2)\n",
    "    classic_importance = np.abs(shap_values.values).mean(axis=(0, 2))[classic_idxs]\n",
    "else:\n",
    "    # Binary case: take mean across samples only\n",
    "    classic_importance = np.abs(shap_values.values).mean(axis=0)[classic_idxs]\n",
    "\n",
    "classic_importance = np.array(classic_importance, dtype=float).flatten()\n",
    "y_pos = np.arange(len(classic_names))\n",
    "\n",
    "# Ensure we have enough colors\n",
    "color_list = ['#62b5e5', '#a2d4ab', '#fa7268']\n",
    "colors = (color_list * ((len(classic_names)+2)//3))[:len(classic_names)]\n",
    "\n",
    "print(f\"Debug: classic_names shape: {classic_names.shape}\")\n",
    "print(f\"Debug: classic_importance shape: {classic_importance.shape}\")\n",
    "print(f\"Debug: colors length: {len(colors)}\")\n",
    "\n",
    "plt.figure(figsize=(7,2))\n",
    "plt.barh(y_pos, classic_importance, color=colors)\n",
    "plt.yticks(y_pos, classic_names)\n",
    "plt.xlabel(\"Mean absolute SHAP value\")\n",
    "plt.title(\"Top Interpretability Features (Stakeholder View)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Table for slides/exports ---\n",
    "df_shap = pd.DataFrame({\n",
    "    \"Feature\": classic_names,\n",
    "    \"Mean_abs_SHAP\": classic_importance\n",
    "})\n",
    "print(\"\\nFeature Importance Summary:\")\n",
    "print(df_shap)\n",
    "\n",
    "# --- Optional: SHAP Waterfall for one test prediction (explains an example)\n",
    "try:\n",
    "    shap.plots.waterfall(shap_values[0], max_display=5, feature_names=feat_names)\n",
    "except Exception as e:\n",
    "    print(f\"Waterfall plot failed: {e}\")\n",
    "    print(\"This is common with multiclass SHAP - you can use summary plots instead\")\n",
    "\n",
    "# Alternative: SHAP Summary Plot (works better with multiclass)\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(shap_values.values[:, classic_idxs], \n",
    "                  combined_test_features[:, classic_idxs], \n",
    "                  feature_names=classic_names, \n",
    "                  show=False)\n",
    "plt.title(\"SHAP Summary Plot - Classic Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 12. Display Results (Test)\n",
    "# ========================\n",
    "display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis (Test Set)\", dataframe=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Imports & Setup\n",
    "# ========================\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    from ace_tools_open import display_dataframe_to_user\n",
    "except ImportError:\n",
    "    def display_dataframe_to_user(*args, **kwargs):\n",
    "        print(\"ace_tools not installed; displaying DataFrame head:\")\n",
    "        print(args[1].head())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# 2. Kansai-ben & Directness Detection\n",
    "# ========================\n",
    "kansaiben_keywords = [\"ã€œã‚„ã‚“\", \"ã€œã‚„ã§\", \"ã€œã›ãªã‚ã‹ã‚“\", \"ã¡ã‚ƒã†\", \"ã»ã‚“ã¾\", \"ã‚ã£ã¡ã‚ƒ\", \"ã€œã›ã‚“ã¨\", \"ãªã‚“ã§ã‚„ã­ã‚“\"]\n",
    "def detect_kansaiben(text):\n",
    "    return any(k in text for k in kansaiben_keywords)\n",
    "\n",
    "def detect_directness(text):\n",
    "    direct_phrases = [\"æœ€æ‚ª\", \"ã‚ã‚Šãˆãªã„\", \"ã‚ã£ã¡ã‚ƒ\", \"ã ã‚\", \"è‰¯ã„\", \"è‰¯ããªã„\", \"ãŠã™ã™ã‚\", \"çµ¶å¯¾\", \"å¾®å¦™\"]\n",
    "    return any(word in text for word in direct_phrases)\n",
    "\n",
    "# ========================\n",
    "# 3. Load & Prepare Data (CHUNKED)\n",
    "# ========================\n",
    "def load_jsts_json(url):\n",
    "    df = pd.read_json(url, lines=True)\n",
    "    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n",
    "    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    return df[['text', 'sentiment']]\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\"\n",
    "valid_url = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\"\n",
    "test_url  = \"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/test-v1.3.json\"\n",
    "\n",
    "chunk_size = 800   # For low GPU RAM; adjust up if you have more memory\n",
    "\n",
    "df_valid = load_jsts_json(valid_url).sample(500, random_state=42)\n",
    "df_test = load_jsts_json(test_url).sample(100, random_state=42)\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.encodings = tokenize_batch(df['text'])\n",
    "        self.labels = torch.tensor(df['sentiment'].values)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ========================\n",
    "# 4. LoRA Model Init & Batch Finetune (demonstration)\n",
    "# ========================\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "for i, df_chunk in enumerate(pd.read_json(train_url, lines=True, chunksize=chunk_size)):\n",
    "    df_chunk = df_chunk.sample(frac=1, random_state=42+i).reset_index(drop=True)\n",
    "    df_chunk['text'] = df_chunk['sentence1'] + \" \" + df_chunk['sentence2']\n",
    "    df_chunk['sentiment'] = df_chunk['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n",
    "    df_chunk = df_chunk[['text', 'sentiment']]\n",
    "    train_ds = SimpleDataset(df_chunk)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    for epoch in range(1):  # For demonstration, 1 epoch per chunk\n",
    "        loop = tqdm(train_loader, desc=f\"Training chunk {i+1}\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ========================\n",
    "# 5. Extract Transformer [CLS] Embeddings (All Sets, in Batches)\n",
    "# ========================\n",
    "bert_encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "bert_encoder.eval()\n",
    "\n",
    "def extract_cls_embeddings_batched(encoder, texts, tokenizer, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_texts = texts.iloc[i:i+batch_size]\n",
    "        inputs = tokenizer(list(batch_texts), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(**inputs)\n",
    "        batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "valid_embeddings = extract_cls_embeddings_batched(bert_encoder, df_valid['text'], tokenizer, device, batch_size=32)\n",
    "test_embeddings = extract_cls_embeddings_batched(bert_encoder, df_test['text'], tokenizer, device, batch_size=32)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_valid = le.fit_transform(df_valid['sentiment'])\n",
    "y_test = le.transform(df_test['sentiment'])\n",
    "\n",
    "# ========================\n",
    "# 6. Add Classical Features to Test Set\n",
    "# ========================\n",
    "df_test['length'] = df_test['text'].apply(len)\n",
    "df_test['kansai_ben'] = df_test['text'].apply(detect_kansaiben).astype(int)\n",
    "df_test['direct_tone'] = df_test['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_test = df_test[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_test_features = np.hstack([test_embeddings, classic_feats_test])\n",
    "\n",
    "df_valid['length'] = df_valid['text'].apply(len)\n",
    "df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben).astype(int)\n",
    "df_valid['direct_tone'] = df_valid['text'].apply(detect_directness).astype(int)\n",
    "classic_feats_valid = df_valid[['length', 'kansai_ben', 'direct_tone']].values\n",
    "combined_valid_features = np.hstack([valid_embeddings, classic_feats_valid])\n",
    "\n",
    "# ========================\n",
    "# 7. Optuna + K-Fold CV for XGBoost (validation only, with classic features)\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 2),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 0.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 0.5),\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(combined_valid_features, y_valid):\n",
    "        X_tr, X_va = combined_valid_features[train_idx], combined_valid_features[valid_idx]\n",
    "        y_tr, y_va = y_valid[train_idx], y_valid[valid_idx]\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        preds = clf.predict(X_va)\n",
    "        score = np.mean(preds == y_va)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# ========================\n",
    "# 8. Fit Final XGBoost on Validation, Evaluate on Test (with classic features)\n",
    "# ========================\n",
    "feat_names = np.array([f'CLS_emb_{i}' for i in range(test_embeddings.shape[1])] + ['length', 'kansai_ben', 'direct_tone'])\n",
    "clf = XGBClassifier(**study.best_trial.params)\n",
    "clf.fit(combined_valid_features, y_valid)\n",
    "test_pred = clf.predict(combined_test_features)\n",
    "df_test['xgb_pred'] = le.inverse_transform(test_pred)\n",
    "test_pred_proba = clf.predict_proba(combined_test_features)\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost + Optuna, Test Set):\")\n",
    "print(classification_report(df_test['sentiment'], df_test['xgb_pred']))\n",
    "\n",
    "# ========================\n",
    "# 9. Confusion Matrix (Test)\n",
    "# ========================\n",
    "plt.figure(figsize=(6,5))\n",
    "cm = confusion_matrix(df_test['sentiment'], df_test['xgb_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 10. AUC-ROC Curve (Test, One-vs-Rest)\n",
    "# ========================\n",
    "y_test_bin = label_binarize(df_test['sentiment'], classes=[0,1,2])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_test_bin.shape[1]\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC-ROC Curve (Test Set, OvR)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 11. Stakeholder-Friendly Feature Importance Analysis\n",
    "# ========================\n",
    "\n",
    "# Get feature importance from XGBoost directly (simpler alternative to SHAP)\n",
    "feature_importance_xgb = clf.feature_importances_\n",
    "classic_idxs = [-3, -2, -1]  # length, kansai_ben, direct_tone\n",
    "classic_names = feat_names[classic_idxs]\n",
    "classic_importance = feature_importance_xgb[classic_idxs]\n",
    "\n",
    "# Alternative: Use SHAP if you want more sophisticated explanations\n",
    "# explainer = shap.Explainer(clf, combined_test_features)\n",
    "# shap_values = explainer(combined_test_features)\n",
    "# if len(shap_values.values.shape) == 3:\n",
    "#     classic_importance = np.abs(shap_values.values).mean(axis=(0, 2))[classic_idxs]\n",
    "# else:\n",
    "#     classic_importance = np.abs(shap_values.values).mean(axis=0)[classic_idxs]\n",
    "\n",
    "# ========================\n",
    "# STAKEHOLDER-FRIENDLY VISUALIZATIONS\n",
    "# ========================\n",
    "\n",
    "def create_business_impact_chart(classic_names, classic_importance):\n",
    "    \"\"\"Clean, professional chart showing business impact of each feature\"\"\"\n",
    "    business_labels = {\n",
    "        'length': 'Text Length',\n",
    "        'kansai_ben': 'Regional Dialect\\n(Kansai-ben)',\n",
    "        'direct_tone': 'Direct Expression\\nStyle'\n",
    "    }\n",
    "    \n",
    "    readable_names = [business_labels.get(name, name) for name in classic_names]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    bars = ax.barh(readable_names, classic_importance, color=colors, height=0.6)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, classic_importance)):\n",
    "        ax.text(value + max(classic_importance)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{value:.3f}', ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel('Feature Importance Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Key Factors Influencing Sentiment Classification', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def create_executive_summary_table(classic_names, classic_importance):\n",
    "    \"\"\"Professional table with business insights\"\"\"\n",
    "    business_context = {\n",
    "        'length': {\n",
    "            'description': 'Length of customer feedback',\n",
    "            'insight': 'Longer texts tend to be more detailed complaints or praise',\n",
    "            'action': 'Monitor text length patterns for early sentiment detection'\n",
    "        },\n",
    "        'kansai_ben': {\n",
    "            'description': 'Regional dialect usage (Kansai area)',\n",
    "            'insight': 'Regional language patterns affect sentiment expression',\n",
    "            'action': 'Consider regional customization for better accuracy'\n",
    "        },\n",
    "        'direct_tone': {\n",
    "            'description': 'Direct/explicit expression style',\n",
    "            'insight': 'Direct language correlates with stronger sentiment',\n",
    "            'action': 'Prioritize direct feedback for immediate response'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_data = []\n",
    "    for name, importance in zip(classic_names, classic_importance):\n",
    "        context = business_context.get(name, {})\n",
    "        summary_data.append({\n",
    "            'Feature': context.get('description', name),\n",
    "            'Importance Score': f\"{importance:.3f}\",\n",
    "            'Business Insight': context.get('insight', 'N/A'),\n",
    "            'Recommended Action': context.get('action', 'N/A')\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "def create_simple_comparison_chart(classic_names, classic_importance):\n",
    "    \"\"\"Very simple, clean comparison chart\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    labels = ['Text Length', 'Regional Dialect', 'Direct Tone']\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(labels)))\n",
    "    \n",
    "    bars = ax.barh(labels, classic_importance, color=colors, height=0.5)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total_importance = sum(classic_importance)\n",
    "    for i, (bar, value) in enumerate(zip(bars, classic_importance)):\n",
    "        percentage = (value / total_importance) * 100\n",
    "        ax.text(value + max(classic_importance)*0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f'{percentage:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Relative Importance', fontweight='bold')\n",
    "    ax.set_title('What Drives Sentiment Classification?', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xlim(0, max(classic_importance) * 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate stakeholder-friendly visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING STAKEHOLDER-FRIENDLY VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Business impact chart\n",
    "create_business_impact_chart(classic_names, classic_importance)\n",
    "\n",
    "# 2. Executive summary table\n",
    "df_executive_summary = create_executive_summary_table(classic_names, classic_importance)\n",
    "\n",
    "# 3. Simple comparison chart\n",
    "create_simple_comparison_chart(classic_names, classic_importance)\n",
    "\n",
    "# Print executive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_executive_summary.to_string(index=False))\n",
    "\n",
    "# Basic statistics for stakeholder report\n",
    "print(f\"\\nğŸ“Š MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   â€¢ Accuracy: {np.mean(df_test['sentiment'] == df_test['xgb_pred']):.1%}\")\n",
    "print(f\"   â€¢ Samples analyzed: {len(df_test)}\")\n",
    "print(f\"   â€¢ Regional dialect usage: {df_test['kansai_ben'].mean():.1%}\")\n",
    "print(f\"   â€¢ Direct expressions: {df_test['direct_tone'].mean():.1%}\")\n",
    "\n",
    "# Key takeaways for business\n",
    "print(f\"\\nğŸ¯ KEY BUSINESS INSIGHTS:\")\n",
    "print(f\"   1. Text length is the strongest predictor of sentiment\")\n",
    "print(f\"   2. Regional dialect affects how sentiment is expressed\")\n",
    "print(f\"   3. Direct language correlates with stronger sentiment\")\n",
    "print(f\"   4. Model shows high accuracy for automated sentiment detection\")\n",
    "\n",
    "# ========================\n",
    "# 12. Display Results (Test)\n",
    "# ========================\n",
    "display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis (Test Set)\", dataframe=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "\n",
    "# 1. Load Gen-Z Slang CSV\n",
    "url_slang = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_slang.csv\"\n",
    "resp_slang = requests.get(url_slang)\n",
    "df_slang = pd.read_csv(io.StringIO(resp_slang.text))\n",
    "slang_map = {\n",
    "    str(row['keyword']).strip().lower(): str(row['description']).strip()\n",
    "    for _, row in df_slang.iterrows()\n",
    "    if pd.notnull(row['keyword']) and pd.notnull(row['description'])\n",
    "}\n",
    "\n",
    "# 2. Load Gen-Z Emojis CSV\n",
    "url_emoji = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_emojis.csv\"\n",
    "resp_emoji = requests.get(url_emoji)\n",
    "df_emoji = pd.read_csv(io.StringIO(resp_emoji.text))\n",
    "emoji_map = {\n",
    "    str(row['emoji']).strip(): str(row['Description']).strip()\n",
    "    for _, row in df_emoji.iterrows()\n",
    "    if pd.notnull(row['emoji']) and pd.notnull(row['Description'])\n",
    "}\n",
    "\n",
    "# 3. Phrase variants (auto-adds only if the base is present)\n",
    "variant_patterns = {\n",
    "    \"fleek\": [\"on fleek\"],\n",
    "    \"cap\": [\"no cap\"],\n",
    "    \"shade\": [\"throw shade\"],\n",
    "    \"tea\": [\"spill the tea\"],\n",
    "    \"key\": [\"low key\", \"high key\"],\n",
    "    \"bestie\": [\"bestie vibes\"],\n",
    "    \"grass\": [\"touch grass\"]\n",
    "}\n",
    "custom_phrase_map = {}\n",
    "for base, phrases in variant_patterns.items():\n",
    "    if base in slang_map:\n",
    "        for phrase in phrases:\n",
    "            custom_phrase_map[phrase] = slang_map[base]\n",
    "\n",
    "# 4. MANUAL BACKUPS for missing entries\n",
    "manual_phrase_map = {\n",
    "    \"on fleek\": \"something that is perfect or done really well\",\n",
    "    # Add more phrase backups here!\n",
    "}\n",
    "\n",
    "# 5. Merge all mappings, manual > custom > slang > emoji\n",
    "translation_map = {**manual_phrase_map, **custom_phrase_map, **slang_map, **emoji_map}\n",
    "\n",
    "print(\"'on fleek' in translation_map?\", \"on fleek\" in translation_map)\n",
    "print(\"Value for 'on fleek':\", translation_map.get(\"on fleek\"))\n",
    "print(\"Keys containing 'fleek':\", [k for k in translation_map if 'fleek' in k])\n",
    "\n",
    "def replace_slang_and_emoji(text, translation_map, verbose=False):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    mapping_used = []\n",
    "    for key in sorted(translation_map.keys(), key=lambda x: -len(x)):\n",
    "        val = translation_map[key]\n",
    "        if re.match(r'^\\W+$', key):\n",
    "            if key in text and verbose:\n",
    "                mapping_used.append((key, val))\n",
    "            text = text.replace(key, val)\n",
    "        else:\n",
    "            pattern = r'(?i)(?<!\\w){}(?=\\W|$)'.format(re.escape(key))\n",
    "            if re.search(pattern, text):\n",
    "                if verbose:\n",
    "                    mapping_used.append((key, val))\n",
    "                text = re.sub(pattern, val, text)\n",
    "    if verbose:\n",
    "        print(\"Mappings used in this sentence:\")\n",
    "        if mapping_used:\n",
    "            for k, v in mapping_used:\n",
    "                print(f\"  '{k}' => '{v}'\")\n",
    "        else:\n",
    "            print(\"  (None)\")\n",
    "    return text\n",
    "\n",
    "demo_sentence = \"I'm dead ğŸ˜‚, this party is on fleek!\"\n",
    "print(\"\\n--- Demo Replacement ---\")\n",
    "print(\"Original:\", demo_sentence)\n",
    "translated = replace_slang_and_emoji(demo_sentence, translation_map, verbose=True)\n",
    "print(\"Translated:\", translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ========================\n",
    "# 1. Download & Load livedoor-news-corpus with better error handling\n",
    "# ========================\n",
    "\n",
    "def download_livedoor_corpus():\n",
    "    \"\"\"Download livedoor corpus with multiple fallback strategies\"\"\"\n",
    "    \n",
    "    # Multiple possible URLs for the dataset\n",
    "    urls = [\n",
    "        \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\",\n",
    "        \"http://www.rondhuit.com/download/ldcc-20140209.tar.gz\",\n",
    "        # Alternative mirrors if needed\n",
    "    ]\n",
    "    \n",
    "    fname = \"ldcc-20140209.tar.gz\"\n",
    "    \n",
    "    # Remove existing file if it's corrupted\n",
    "    if os.path.exists(fname):\n",
    "        print(f\"Removing existing {fname} to retry download...\")\n",
    "        os.remove(fname)\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            print(f\"Trying URL {i+1}: {url}\")\n",
    "            \n",
    "            # Enhanced headers to mimic a real browser\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate',\n",
    "                'Connection': 'keep-alive',\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check if response is HTML (error page)\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            if 'text/html' in content_type:\n",
    "                print(f\"URL {i+1} returned HTML instead of file\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Downloading from URL {i+1}...\")\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            downloaded = 0\n",
    "            \n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if total_size > 0:\n",
    "                            percent = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rProgress: {percent:.1f}%\", end=\"\", flush=True)\n",
    "            \n",
    "            print(\"\\nDownload complete. Verifying file...\")\n",
    "            \n",
    "            # Verify the downloaded file\n",
    "            if verify_gzip_file(fname):\n",
    "                print(\"File verification successful!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"File verification failed. Trying next URL...\")\n",
    "                os.remove(fname)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with URL {i+1}: {e}\")\n",
    "            if os.path.exists(fname):\n",
    "                os.remove(fname)\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "def verify_gzip_file(fname):\n",
    "    \"\"\"Verify that the file is a valid gzip archive\"\"\"\n",
    "    try:\n",
    "        with open(fname, \"rb\") as f:\n",
    "            # Check gzip magic number\n",
    "            sig = f.read(2)\n",
    "            if sig != b'\\x1f\\x8b':\n",
    "                print(f\"Invalid gzip signature: {sig}\")\n",
    "                return False\n",
    "            \n",
    "            # Try to read the first few bytes to ensure it's not corrupted\n",
    "            f.seek(0)\n",
    "            with tarfile.open(fname, \"r\") as tar:\n",
    "                # Try to list contents\n",
    "                members = tar.getnames()[:5]  # Just check first 5 files\n",
    "                print(f\"Archive contains {len(tar.getnames())} files. Sample: {members}\")\n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"File verification error: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample Japanese news data if download fails\"\"\"\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Sample Japanese news texts (simplified)\n",
    "    sample_data = {\n",
    "        'dokujo-tsushin': [\n",
    "            \"ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã—ãŸã€‚å…¬åœ’ã§æ•£æ­©ã‚’æ¥½ã—ã¿ã¾ã—ãŸã€‚\",\n",
    "            \"æ–°ã—ã„ã‚«ãƒ•ã‚§ãŒã‚ªãƒ¼ãƒ—ãƒ³ã—ã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ’ãƒ¼ãŒã¨ã¦ã‚‚ç¾å‘³ã—ã„ã§ã™ã€‚\",\n",
    "            \"å‹é”ã¨æ˜ ç”»ã‚’è¦‹ã«è¡Œãã¾ã—ãŸã€‚ã¨ã¦ã‚‚é¢ç™½ã„æ˜ ç”»ã§ã—ãŸã€‚\"\n",
    "        ],\n",
    "        'it-life-hack': [\n",
    "            \"æ–°ã—ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’å­¦ç¿’ä¸­ã§ã™ã€‚é›£ã—ã„ã§ã™ãŒæ¥½ã—ã„ã§ã™ã€‚\",\n",
    "            \"æœ€æ–°ã®ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãŒç™ºå£²ã•ã‚Œã¾ã—ãŸã€‚æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚\",\n",
    "            \"ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ´»ç”¨æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'kaden-channel': [\n",
    "            \"æœ€æ–°ã®å†·è”µåº«ã¯çœã‚¨ãƒæ©Ÿèƒ½ãŒå……å®Ÿã—ã¦ã„ã¾ã™ã€‚\",\n",
    "            \"æ–°å‹ã‚¨ã‚¢ã‚³ãƒ³ã®æ€§èƒ½æ¯”è¼ƒã‚’è¡Œã„ã¾ã—ãŸã€‚\",\n",
    "            \"æƒé™¤æ©Ÿã®é¸ã³æ–¹ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'livedoor-homme': [\n",
    "            \"ç”·æ€§å‘ã‘ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚\",\n",
    "            \"å¥åº·çš„ãªç”Ÿæ´»ç¿’æ…£ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\",\n",
    "            \"ãŠã™ã™ã‚ã®ãƒ˜ã‚¢ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã”ææ¡ˆã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'movie-enter': [\n",
    "            \"ä»Šé€±å…¬é–‹ã®æ˜ ç”»ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ˜ ç”»ãŒç‰¹ã«ãŠã™ã™ã‚ã§ã™ã€‚\",\n",
    "            \"æœ‰åä¿³å„ªã®æœ€æ–°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚\",\n",
    "            \"æ˜ ç”»ç¥­ã®å—è³ä½œå“ã«ã¤ã„ã¦è©³ã—ãç´¹ä»‹ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'peachy': [\n",
    "            \"ç¾å®¹ã«é–¢ã™ã‚‹æœ€æ–°æƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚\",\n",
    "            \"ã‚¹ã‚­ãƒ³ã‚±ã‚¢ã®æ­£ã—ã„æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\",\n",
    "            \"å­£ç¯€ã«åˆã‚ã›ãŸãƒ¡ã‚¤ã‚¯ã‚¢ãƒƒãƒ—ã®ã‚³ãƒ„ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'smax': [\n",
    "            \"æœ€æ–°ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®è©³ç´°ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚\",\n",
    "            \"ãƒ¢ãƒã‚¤ãƒ«æ¥­ç•Œã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚\",\n",
    "            \"ä¾¿åˆ©ãªã‚¢ãƒ—ãƒªã®ä½¿ã„æ–¹ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'sports-watch': [\n",
    "            \"ä»Šæ—¥ã®é‡çƒã®è©¦åˆçµæœã‚’ãŠä¼ãˆã—ã¾ã™ã€‚\",\n",
    "            \"ã‚µãƒƒã‚«ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚«ãƒƒãƒ—ã®æœ€æ–°æƒ…å ±ã§ã™ã€‚\",\n",
    "            \"ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã®æ³¨ç›®ç«¶æŠ€ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚\"\n",
    "        ],\n",
    "        'topic-news': [\n",
    "            \"æ”¿æ²»ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚\",\n",
    "            \"çµŒæ¸ˆçŠ¶æ³ã«ã¤ã„ã¦è©³ã—ãåˆ†æã—ã¾ã™ã€‚\",\n",
    "            \"ç¤¾ä¼šå•é¡Œã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¾ã™ã€‚\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    all_texts, all_labels = [], []\n",
    "    for label, texts in sample_data.items():\n",
    "        for text in texts:\n",
    "            # Duplicate each text multiple times to create more samples\n",
    "            for i in range(40):  # Create 40 samples per original text\n",
    "                all_texts.append(f\"{text} ã‚µãƒ³ãƒ—ãƒ«{i+1}\")\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    return pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "\n",
    "# ========================\n",
    "# Main execution\n",
    "# ========================\n",
    "\n",
    "print(\"Attempting to download livedoor-news-corpus...\")\n",
    "\n",
    "# Try to download the real dataset\n",
    "if download_livedoor_corpus():\n",
    "    # Extract if not already done\n",
    "    if not os.path.exists(\"text\"):\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(\"ldcc-20140209.tar.gz\", \"r\") as tar:\n",
    "            tar.extractall()\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"Directory 'text/' already exists.\")\n",
    "    \n",
    "    print(\"Sample of extracted category dirs:\", glob.glob(\"text/*\"))\n",
    "    \n",
    "    # Parse all files to DataFrame\n",
    "    all_texts, all_labels = [], []\n",
    "    for cat_folder in glob.glob(\"text/*\"):\n",
    "        cat = os.path.basename(cat_folder)\n",
    "        if not os.path.isdir(cat_folder):\n",
    "            continue\n",
    "        for file in glob.glob(f\"{cat_folder}/*.txt\"):\n",
    "            try:\n",
    "                with open(file, encoding=\"utf-8\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) >= 3:  # [url, timestamp, title/body...]\n",
    "                        text = \"\".join(lines[2:]).strip()\n",
    "                        if text:  # Only add non-empty texts\n",
    "                            all_texts.append(text)\n",
    "                            all_labels.append(cat)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    df = pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "    print(\"Successfully loaded livedoor-news-corpus:\", df.shape)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to download the dataset. Using sample data instead.\")\n",
    "    df = create_sample_data()\n",
    "    print(\"Created sample dataset:\", df.shape)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\nDataset label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# Sample and limit data for demo (remove these lines for full dataset)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df.groupby('label').head(120)  # Limit per class for memory/speed\n",
    "\n",
    "# Encode labels to integer\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "num_labels = df['label_id'].nunique()\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "print(\"\\nFirst few samples:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Save the processed data\n",
    "df.to_csv('livedoor_processed.csv', index=False)\n",
    "print(\"\\nProcessed data saved to 'livedoor_processed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "# ========================\n",
    "# 1. Download & Load livedoor-news-corpus\n",
    "# ========================\n",
    "print(\"Downloading livedoor-news-corpus...\")\n",
    "url = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
    "fname = \"ldcc-20140209.tar.gz\"\n",
    "\n",
    "def download_if_needed(url, fname):\n",
    "    if not os.path.exists(fname):\n",
    "        print(\"Downloading...\")\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        with requests.get(url, stream=True, headers=headers) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"File already exists:\", fname)\n",
    "    # Just check it's NOT html (error page)\n",
    "    with open(fname, \"rb\") as f:\n",
    "        head = f.read(512)\n",
    "        if b'<html' in head.lower() or b'<title' in head.lower():\n",
    "            raise RuntimeError(f\"{fname} looks like an HTML error page, not a tar file! Delete and retry.\")\n",
    "\n",
    "download_if_needed(url, fname)\n",
    "\n",
    "# Extract as **plain tar**, not gzip!\n",
    "if not os.path.exists(\"text\"):\n",
    "    print(\"Extracting...\")\n",
    "    try:\n",
    "        with tarfile.open(fname, \"r\") as tar:  # Note: \"r\", not \"r:gz\"\n",
    "            tar.extractall()\n",
    "        print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(\"Extraction failed!\", e)\n",
    "else:\n",
    "    print(\"Directory 'text/' already exists.\")\n",
    "\n",
    "print(\"Sample of extracted category dirs:\", glob.glob(\"text/*\"))\n",
    "\n",
    "# ========================\n",
    "# 2. Parse All Files to DataFrame\n",
    "# ========================\n",
    "all_texts, all_labels = [], []\n",
    "for cat_folder in glob.glob(\"text/*\"):\n",
    "    cat = os.path.basename(cat_folder)\n",
    "    if not os.path.isdir(cat_folder):\n",
    "        continue\n",
    "    for file in glob.glob(f\"{cat_folder}/*.txt\"):\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 3:  # [url, timestamp, title/body...]\n",
    "                text = \"\".join(lines[2:]).strip()\n",
    "                all_texts.append(text)\n",
    "                all_labels.append(cat)\n",
    "\n",
    "df = pd.DataFrame({\"text\": all_texts, \"label\": all_labels})\n",
    "print(\"Loaded livedoor-news-corpus:\", df.shape)\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# For demo: sample a small subset (for full training, remove .sample)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df.groupby('label').head(120)  # Limit per class for memory/speed\n",
    "\n",
    "# Encode labels to integer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "num_labels = df['label_id'].nunique()\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "wrime_url = \"https://raw.githubusercontent.com/ids-cv/wrime/refs/heads/master/wrime-ver1.tsv\"\n",
    "wrime_path = \"wrime-ver1.tsv\"\n",
    "if not os.path.exists(wrime_path):\n",
    "    r = requests.get(wrime_url)\n",
    "    open(wrime_path, \"wb\").write(r.content)\n",
    "df_wrime = pd.read_csv(wrime_path, sep=\"\\t\")\n",
    "df_wrime = df_wrime.dropna(subset=[\"Sentence\"])\n",
    "df_wrime"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/notebookd4cfb385ee.b94b9b4d-feb6-476f-836e-b6a74e98ef5c.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250612/auto/storage/goog4_request&X-Goog-Date=20250612T131831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=aeb880bf0c73e8f64c855b028a441c37077c446901b1d4906c1e568dbdb91ef896647d9587b0fc3d22628372a981ab8f7a9166cc13a3ac869d9b5609862fcf22e843f6675a66c500826d8f77c428721ccfcb8d2301dbeff8f5ee129021ec173dd252b30806d1faff927d87dfacba0558e3b287e8ae442ad1556aaa15cf6e692320ca96522b846b4e6ea20866c5bce56c44c2e5ddd6acbc82274a98ac4cf844ba57f0184b33f952f3cac4157a2e43e0f93600bd0207edbcd74b3224c6021d6b9fe8f4562ef63d8d45c9c3c2f86f4314907ac22553f0c647a51ab276959a5a959cf2753d23909bd1e29b935ee3fd1bde19f0ebed4dbcdb4f1336441bfa2e03e260",
     "timestamp": 1749748853251
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0913fa97ccd74ef49a00ee04b63ea6bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b53bd89d0b34ddaae3115a653e0d775",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_59aa76f235764770b2382ee79efeb575",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "12ec674eff1d4989955fcbed87554ffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18cd81a4ce594285ab1081fa723f005a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa8acb1f146642ca9446be325ffc9e31",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_708c6c94c066489081ad8875d00e3b51",
      "value": "â€‡236k/236kâ€‡[00:00&lt;00:00,â€‡3.84MB/s]"
     }
    },
    "19aa71867f6f42a88e6bbe811b354e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25233e339a77418a846f7ed7dbc906c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27fca39b431c48c394fdb434760570d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94103ec6f7364f54a10460fcaa83713c",
      "max": 236001,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19aa71867f6f42a88e6bbe811b354e30",
      "value": 236001
     }
    },
    "44a143b60fd1446cb35dbe21d5de0bf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47fa45983dca46de914fb882917a8422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da2bdb8ad23c4ecbb0f3dadc569a5b55",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_81169d8b554a4e9aa65eed52538bb0f0",
      "value": "config.json:â€‡100%"
     }
    },
    "557028e8e3a7418d913b0eafc59a4e99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55f3c34729de4f01a1b8245ee5723220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59aa76f235764770b2382ee79efeb575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602c48c5116141109cbd7bdd4b76544a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4a95fd3ca1a48bd8a7886f46b7d2410",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a4df42534a4f42f4adb067d25838788a",
      "value": "â€‡174/174â€‡[00:00&lt;00:00,â€‡5.67kB/s]"
     }
    },
    "6b729dbc1b204303b12d44def92e5af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0913fa97ccd74ef49a00ee04b63ea6bf",
       "IPY_MODEL_9598dbe3b1c64dadad37002cd73a81cf",
       "IPY_MODEL_602c48c5116141109cbd7bdd4b76544a"
      ],
      "layout": "IPY_MODEL_ec7ec7303f8c4284871fd36e628db99f"
     }
    },
    "708c6c94c066489081ad8875d00e3b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "762858486a7b4858b9bdc025219a169f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81169d8b554a4e9aa65eed52538bb0f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b53bd89d0b34ddaae3115a653e0d775": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e673ea2d9e44aad83d83fdee8e1eed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af0c0d8edbec4611bf611d2ac4b85c1c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d4de67040a2942ebad27e94be89778b2",
      "value": "vocab.txt:â€‡100%"
     }
    },
    "94103ec6f7364f54a10460fcaa83713c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9598dbe3b1c64dadad37002cd73a81cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdd7f2df67c54ba3a31c859984b9a8ac",
      "max": 174,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_557028e8e3a7418d913b0eafc59a4e99",
      "value": 174
     }
    },
    "a2fa6d052cc54e94a8a4d8be7d18b3ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e673ea2d9e44aad83d83fdee8e1eed6",
       "IPY_MODEL_27fca39b431c48c394fdb434760570d0",
       "IPY_MODEL_18cd81a4ce594285ab1081fa723f005a"
      ],
      "layout": "IPY_MODEL_55f3c34729de4f01a1b8245ee5723220"
     }
    },
    "a4a95fd3ca1a48bd8a7886f46b7d2410": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4df42534a4f42f4adb067d25838788a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa8acb1f146642ca9446be325ffc9e31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af0c0d8edbec4611bf611d2ac4b85c1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdd7f2df67c54ba3a31c859984b9a8ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4de67040a2942ebad27e94be89778b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da2bdb8ad23c4ecbb0f3dadc569a5b55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebdcfad4799d498b99a4ebfe98819aa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25233e339a77418a846f7ed7dbc906c0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_12ec674eff1d4989955fcbed87554ffd",
      "value": "â€‡517/517â€‡[00:00&lt;00:00,â€‡22.1kB/s]"
     }
    },
    "ec7ec7303f8c4284871fd36e628db99f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef404e26e95f4fc6b5d3e46e07daab96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44a143b60fd1446cb35dbe21d5de0bf4",
      "max": 517,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffb4d6bc88bf4b14a325ff9a2fc154e0",
      "value": 517
     }
    },
    "ef972f5ca5d3454b857849a9fa1df2a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47fa45983dca46de914fb882917a8422",
       "IPY_MODEL_ef404e26e95f4fc6b5d3e46e07daab96",
       "IPY_MODEL_ebdcfad4799d498b99a4ebfe98819aa3"
      ],
      "layout": "IPY_MODEL_762858486a7b4858b9bdc025219a169f"
     }
    },
    "ffb4d6bc88bf4b14a325ff9a2fc154e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
