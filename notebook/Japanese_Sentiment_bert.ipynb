{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/notebookd4cfb385ee.b94b9b4d-feb6-476f-836e-b6a74e98ef5c.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250612/auto/storage/goog4_request&X-Goog-Date=20250612T131831Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=aeb880bf0c73e8f64c855b028a441c37077c446901b1d4906c1e568dbdb91ef896647d9587b0fc3d22628372a981ab8f7a9166cc13a3ac869d9b5609862fcf22e843f6675a66c500826d8f77c428721ccfcb8d2301dbeff8f5ee129021ec173dd252b30806d1faff927d87dfacba0558e3b287e8ae442ad1556aaa15cf6e692320ca96522b846b4e6ea20866c5bce56c44c2e5ddd6acbc82274a98ac4cf844ba57f0184b33f952f3cac4157a2e43e0f93600bd0207edbcd74b3224c6021d6b9fe8f4562ef63d8d45c9c3c2f86f4314907ac22553f0c647a51ab276959a5a959cf2753d23909bd1e29b935ee3fd1bde19f0ebed4dbcdb4f1336441bfa2e03e260","timestamp":1749748853251}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# ========================\n","# 0. Install libraries\n","# ========================\n","!pip install --quiet numpy spacy thinc\n","!pip install --quiet torch torchvision torchaudio\n","!pip install --quiet transformers fugashi ipadic accelerate peft sentencepiece matplotlib seaborn tqdm\n","!pip install --quiet xgboost optuna ace_tools_open shap unidic-lite mecab-python3"],"metadata":{"id":"yAQcBFr3IMYv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749816796297,"user_tz":-120,"elapsed":46886,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"38825dcd-53e5-4659-cbd9-a8893f802495"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.8/588.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","print(\"CUDA available:\", torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfTjYNifrfFP","executionInfo":{"status":"ok","timestamp":1749816796306,"user_tz":-120,"elapsed":5,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"32970a09-ceaa-4239-db9c-09ec0166452b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: True\n"]}]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Nr0KW0ZiCea","executionInfo":{"status":"ok","timestamp":1749816796313,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"685b1f82-ef1e-4a92-8949-7b7dc1053673"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["4.52.4\n"]}]},{"cell_type":"code","source":["!pip install certifi\n","!mkdir -p /usr/local/share/ca-certificates/\n","!cp /etc/ssl/certs/ca-certificates.crt /usr/local/share/ca-certificates/\n","!update-ca-certificates\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TK8QWC8kvZa","executionInfo":{"status":"ok","timestamp":1749816810326,"user_tz":-120,"elapsed":14011,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"8e637937-6d1a-4dbf-d22c-73ec921f26d0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.4.26)\n","Updating certificates in /etc/ssl/certs...\n","0 added, 0 removed; done.\n","Running hooks in /etc/ca-certificates/update.d...\n","\n","done.\n","done.\n"]}]},{"cell_type":"code","source":["# ========================\n","# 1. Imports & Setup\n","# ========================\n","from tqdm import tqdm\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n","from peft import get_peft_model, LoraConfig, TaskType\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","from sklearn.preprocessing import LabelEncoder, label_binarize\n","import optuna\n","import shap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","try:\n","    from ace_tools_open import display_dataframe_to_user\n","except ImportError:\n","    def display_dataframe_to_user(*args, **kwargs):\n","        print(\"ace_tools not installed; displaying DataFrame head:\")\n","        print(args[1].head())\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ========================\n","# 2. Kansai-ben & Directness Detection\n","# ========================\n","kansaiben_keywords = [\"〜やん\", \"〜やで\", \"〜せなあかん\", \"ちゃう\", \"ほんま\", \"めっちゃ\", \"〜せんと\", \"なんでやねん\"]\n","def detect_kansaiben(text):\n","    return any(k in text for k in kansaiben_keywords)\n","\n","def detect_directness(text):\n","    direct_phrases = [\"最悪\", \"ありえない\", \"めっちゃ\", \"だめ\", \"良い\", \"良くない\", \"おすすめ\", \"絶対\", \"微妙\"]\n","    return any(word in text for word in direct_phrases)\n","\n","# ========================\n","# 3. Load & Prepare Data\n","# ========================\n","def load_jsts_json(url):\n","    df = pd.read_json(url, lines=True)\n","    df['text'] = df['sentence1'] + \" \" + df['sentence2']\n","    df['sentiment'] = df['label'].apply(lambda x: 0 if x < 2 else (1 if x <= 3 else 2))\n","    return df[['text', 'sentiment']]\n","\n","df_train = load_jsts_json(\"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/train-v1.3.json\").sample(5000, random_state=42)\n","df_valid = load_jsts_json(\"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/valid-v1.3.json\").sample(500, random_state=42)\n","df_test = load_jsts_json(\"https://raw.githubusercontent.com/yahoojapan/JGLUE/refs/heads/main/datasets/jsts-v1.3/test-v1.3.json\").sample(300, random_state=42)\n","\n","model_name = \"cl-tohoku/bert-base-japanese-v2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_batch(texts):\n","    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')\n","\n","class SimpleDataset(torch.utils.data.Dataset):\n","    def __init__(self, df):\n","        self.encodings = tokenize_batch(df['text'])\n","        self.labels = torch.tensor(df['sentiment'].values)\n","    def __getitem__(self, idx):\n","        item = {k: v[idx] for k, v in self.encodings.items()}\n","        item[\"labels\"] = self.labels[idx]\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_ds = SimpleDataset(df_train)\n","valid_ds = SimpleDataset(df_valid)\n","test_ds = SimpleDataset(df_test)\n","\n","# ========================\n","# 4. LoRA Model Init & Quick Finetune (for demonstration)\n","# ========================\n","base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n","model = get_peft_model(base_model, peft_config).to(device)\n","\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","\n","train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","model.train()\n","for epoch in range(1):\n","    loop = tqdm(train_loader, desc=\"Training\")\n","    for batch in loop:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        loop.set_postfix(loss=loss.item())\n","\n","# ========================\n","# 5. Extract Transformer [CLS] Embeddings (All Sets)\n","# ========================\n","bert_encoder = AutoModel.from_pretrained(model_name).to(device)\n","bert_encoder.eval()\n","\n","def extract_cls_embeddings(encoder, texts, tokenizer, device):\n","    embeddings = []\n","    with torch.no_grad():\n","        for text in tqdm(texts, desc=\"Extracting embeddings\"):\n","            inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n","            outputs = encoder(**{k: v for k, v in inputs.items()})\n","            cls_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n","            embeddings.append(cls_emb)\n","    return np.vstack(embeddings)\n","\n","valid_embeddings = extract_cls_embeddings(bert_encoder, df_valid['text'], tokenizer, device)\n","test_embeddings = extract_cls_embeddings(bert_encoder, df_test['text'], tokenizer, device)\n","\n","le = LabelEncoder()\n","y_valid = le.fit_transform(df_valid['sentiment'])\n","y_test = le.transform(df_test['sentiment'])\n","\n","# ========================\n","# 6. Add Classical Features to Test Set\n","# ========================\n","df_test['length'] = df_test['text'].apply(len)\n","df_test['kansai_ben'] = df_test['text'].apply(detect_kansaiben).astype(int)\n","df_test['direct_tone'] = df_test['text'].apply(detect_directness).astype(int)\n","classic_feats_test = df_test[['length', 'kansai_ben', 'direct_tone']].values\n","combined_test_features = np.hstack([test_embeddings, classic_feats_test])\n","\n","# Also add to valid set for tuning\n","df_valid['length'] = df_valid['text'].apply(len)\n","df_valid['kansai_ben'] = df_valid['text'].apply(detect_kansaiben).astype(int)\n","df_valid['direct_tone'] = df_valid['text'].apply(detect_directness).astype(int)\n","classic_feats_valid = df_valid[['length', 'kansai_ben', 'direct_tone']].values\n","combined_valid_features = np.hstack([valid_embeddings, classic_feats_valid])\n","\n","# ========================\n","# 7. Optuna + K-Fold CV for XGBoost (validation only, with classic features)\n","# ========================\n","def objective(trial):\n","    params = {\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n","        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n","        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n","        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n","        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 2.0),\n","        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 2.0),\n","        \"use_label_encoder\": False,\n","        \"eval_metric\": \"mlogloss\",\n","        \"verbosity\": 0,\n","        \"tree_method\": \"gpu_hist\",\n","    }\n","    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","    scores = []\n","    for train_idx, valid_idx in skf.split(combined_valid_features, y_valid):\n","        X_tr, X_va = combined_valid_features[train_idx], combined_valid_features[valid_idx]\n","        y_tr, y_va = y_valid[train_idx], y_valid[valid_idx]\n","        clf = XGBClassifier(**params)\n","        clf.fit(X_tr, y_tr)\n","        preds = clf.predict(X_va)\n","        score = np.mean(preds == y_va)\n","        scores.append(score)\n","    return np.mean(scores)\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=20)\n","print(\"Best trial:\", study.best_trial.params)\n","\n","# ========================\n","# 8. Fit Final XGBoost on Validation, Evaluate on Test (with classic features)\n","# ========================\n","feat_names = [f'CLS_emb_{i}' for i in range(test_embeddings.shape[1])] + ['length', 'kansai_ben', 'direct_tone']\n","clf = XGBClassifier(**study.best_trial.params)\n","clf.fit(combined_valid_features, y_valid)\n","test_pred = clf.predict(combined_test_features)\n","df_test['xgb_pred'] = le.inverse_transform(test_pred)\n","test_pred_proba = clf.predict_proba(combined_test_features)\n","\n","print(\"\\nClassification Report (XGBoost + Optuna, Test Set):\")\n","print(classification_report(df_test['sentiment'], df_test['xgb_pred']))\n","\n","# ========================\n","# 9. Confusion Matrix (Test)\n","# ========================\n","plt.figure(figsize=(6,5))\n","cm = confusion_matrix(df_test['sentiment'], df_test['xgb_pred'])\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.title(\"Confusion Matrix (Test Set)\")\n","plt.show()\n","\n","# ========================\n","# 10. AUC-ROC Curve (Test, One-vs-Rest)\n","# ========================\n","y_test_bin = label_binarize(df_test['sentiment'], classes=[0,1,2])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","n_classes = y_test_bin.shape[1]\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_pred_proba[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","plt.figure(figsize=(7,5))\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i], label=f\"Class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n","plt.plot([0,1],[0,1],'k--')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"AUC-ROC Curve (Test Set, OvR)\")\n","plt.legend()\n","plt.show()\n","\n","# ========================\n","# 11. Feature Importance & SHAP (Test) with feature names\n","# ========================\n","plt.figure(figsize=(14,4))\n","imp = clf.feature_importances_\n","sorted_idx = np.argsort(imp)[::-1][:30] # Show top 30 features\n","plt.bar(np.array(feat_names)[sorted_idx], imp[sorted_idx])\n","plt.title(\"Feature Importance (Top 30, Test Set)\")\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()\n","\n","explainer = shap.Explainer(clf, combined_test_features)\n","shap_values = explainer(combined_test_features)\n","shap.summary_plot(shap_values, combined_test_features, feature_names=feat_names, plot_type=\"bar\", max_display=30, show=True)\n","\n","# ========================\n","# 12. Display Results (Test)\n","# ========================\n","display_dataframe_to_user(name=\"JGLUE Sentiment + Kansai-ben Analysis (Test Set)\", dataframe=df_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMxTA1g2UHy6","outputId":"788d9d6b-d1e3-43ff-85bd-eb88fb142bb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Training: 100%|██████████| 625/625 [01:10<00:00,  8.90it/s, loss=0.985]\n","Extracting embeddings: 100%|██████████| 500/500 [00:05<00:00, 99.06it/s]\n","Extracting embeddings: 100%|██████████| 300/300 [00:03<00:00, 98.71it/s]\n","[I 2025-06-13 13:35:48,846] A new study created in memory with name: no-name-5fca85c5-503e-4534-b614-d913730aa773\n","[I 2025-06-13 13:36:00,592] Trial 0 finished with value: 0.61 and parameters: {'n_estimators': 134, 'max_depth': 5, 'learning_rate': 0.025447318044836038, 'subsample': 0.7466105283296202, 'colsample_bytree': 0.5220742126422975, 'gamma': 4.006416053059103, 'reg_alpha': 0.11135123976281402, 'reg_lambda': 1.3594915915371013}. Best is trial 0 with value: 0.61.\n","[I 2025-06-13 13:36:17,491] Trial 1 finished with value: 0.5999999999999999 and parameters: {'n_estimators': 76, 'max_depth': 8, 'learning_rate': 0.015316190254262724, 'subsample': 0.7819551200240271, 'colsample_bytree': 0.989677424045238, 'gamma': 3.1493045550625878, 'reg_alpha': 1.2478008232062603, 'reg_lambda': 1.7857045206198168}. Best is trial 0 with value: 0.61.\n","[I 2025-06-13 13:36:41,652] Trial 2 finished with value: 0.592 and parameters: {'n_estimators': 93, 'max_depth': 5, 'learning_rate': 0.01590507899324639, 'subsample': 0.643265884394948, 'colsample_bytree': 0.938807380111801, 'gamma': 0.07380209259104764, 'reg_alpha': 0.06289285079173323, 'reg_lambda': 0.06691723823365758}. Best is trial 0 with value: 0.61.\n","[I 2025-06-13 13:36:47,194] Trial 3 finished with value: 0.568 and parameters: {'n_estimators': 125, 'max_depth': 6, 'learning_rate': 0.26126507478711475, 'subsample': 0.6762158177337927, 'colsample_bytree': 0.5671976027918226, 'gamma': 4.338777031314338, 'reg_alpha': 1.034723094415942, 'reg_lambda': 0.31983161437496954}. Best is trial 0 with value: 0.61.\n","[I 2025-06-13 13:36:57,317] Trial 4 finished with value: 0.6 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.09275039846163435, 'subsample': 0.7609648972945358, 'colsample_bytree': 0.560589979013832, 'gamma': 2.137478519420479, 'reg_alpha': 0.31578635650048814, 'reg_lambda': 0.8477021899999602}. Best is trial 0 with value: 0.61.\n","[I 2025-06-13 13:37:05,114] Trial 5 finished with value: 0.5740000000000001 and parameters: {'n_estimators': 174, 'max_depth': 3, 'learning_rate': 0.1873105139719057, 'subsample': 0.9212931823010918, 'colsample_bytree': 0.9470732327598419, 'gamma': 1.8996012637349096, 'reg_alpha': 0.17157638787940677, 'reg_lambda': 0.7820820563901416}. Best is trial 0 with value: 0.61.\n"]}]}]}