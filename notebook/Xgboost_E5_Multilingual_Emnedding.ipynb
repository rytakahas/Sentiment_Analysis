{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163060,
     "status": "ok",
     "timestamp": 1750858247085,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zALlol7-RZNW",
    "outputId": "efde0a15-b0bc-476e-9749-1d49e827f518"
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# JAPANESE SENTIMENT ANALYSIS WITH EMBEDDINGS + XGBOOST + OPTUNA + SHAP\n",
    "# Compatible with Kaggle T4 GPU and Colab T4 GPU\n",
    "# ========================\n",
    "\n",
    "# 0. SETUP & INSTALL\n",
    "!pip install transformers datasets optuna xgboost shap fugashi ipadic unidic-lite sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1d1a226dcd224edbab3b2cd933d931ae",
      "e2b7064a85ef40408ef43d76eeb547b2",
      "292b82cece8c4670982bc473d166b5fa",
      "ebb3a38629b34a14adf4d0e4cf9c5847",
      "9b231547e69445e7ad202bb72d03a298",
      "cde515dea536495c8ada55f35945b16c",
      "2e123c1e1dd24c169830b1914625c6d4",
      "bf4d9b8727a04c6d8a352056d0806517",
      "7735e519ce2645a29ea7442fe345cbf5",
      "c68e6c001a484fe6a6a3b05ea515ffa1",
      "73ce9f3a745e45aabca55fdef38c4a67"
     ]
    },
    "executionInfo": {
     "elapsed": 1096249,
     "status": "ok",
     "timestamp": 1750862743099,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4MfFl53GRZNY",
    "outputId": "33941890-38f6-4084-d73f-efdc67b9ba0d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install and import required packages\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import MedianPruner\n",
    "except ImportError:\n",
    "    print(\"Installing Optuna...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"optuna\"])\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import MedianPruner\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    print(\"Installing sentence-transformers...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"sentence-transformers\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"Installing tqdm...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# 1. LOAD DATASETS (WRIME)\n",
    "print(\"Loading WRIME dataset...\")\n",
    "wrime_url = \"https://raw.githubusercontent.com/ids-cv/wrime/refs/heads/master/wrime-ver1.tsv\"\n",
    "wrime_path = \"wrime-ver1.tsv\"\n",
    "if not os.path.exists(wrime_path):\n",
    "    r = requests.get(wrime_url)\n",
    "    open(wrime_path, \"wb\").write(r.content)\n",
    "df_wrime = pd.read_csv(wrime_path, sep=\"\\t\")\n",
    "df_wrime = df_wrime.dropna(subset=[\"Sentence\"])\n",
    "\n",
    "# Create sentiment labels\n",
    "def get_sentiment(row):\n",
    "    if row['Avg. Readers_Joy'] > max(row['Avg. Readers_Sadness'], row['Avg. Readers_Anger']):\n",
    "        return 2  # Positive\n",
    "    elif (row['Avg. Readers_Sadness'] > row['Avg. Readers_Joy']) or (row['Avg. Readers_Anger'] > row['Avg. Readers_Joy']):\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return 1  # Neutral\n",
    "\n",
    "df_wrime['sentiment'] = df_wrime.apply(get_sentiment, axis=1)\n",
    "\n",
    "# Enhanced synthetic labels for additional features\n",
    "joy_thresh = df_wrime['Avg. Readers_Joy'].median()\n",
    "sadness_thresh = df_wrime['Avg. Readers_Sadness'].median()\n",
    "anger_thresh = df_wrime['Avg. Readers_Anger'].median()\n",
    "\n",
    "df_wrime['high_joy'] = (df_wrime['Avg. Readers_Joy'] > joy_thresh).astype(int)\n",
    "df_wrime['high_sadness'] = (df_wrime['Avg. Readers_Sadness'] > sadness_thresh).astype(int)\n",
    "df_wrime['high_anger'] = (df_wrime['Avg. Readers_Anger'] > anger_thresh).astype(int)\n",
    "\n",
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_wrime, test_size=0.2, random_state=42, stratify=df_wrime['sentiment'])\n",
    "print(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")\n",
    "print(\"Sentiment distribution:\")\n",
    "print(f\"Train: {df_train['sentiment'].value_counts().sort_index().tolist()}\")\n",
    "print(f\"Test: {df_test['sentiment'].value_counts().sort_index().tolist()}\")\n",
    "\n",
    "# 2. ADVANCED EMBEDDING EXTRACTION WITH MULTIPLE MODELS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING EXTRACTION WITH MULTIPLE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Available models optimized for T4 GPU\n",
    "embedding_models = {\n",
    "    'japanese_bert': \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    'japanese_roberta': \"rinna/japanese-roberta-base\",\n",
    "    'multilingual_e5': \"intfloat/multilingual-e5-base\",  # E5 model - excellent for multilingual\n",
    "    'e5_small': \"intfloat/e5-small-v2\"  # Smaller E5 model for efficiency\n",
    "}\n",
    "\n",
    "print(\"Available embedding models:\")\n",
    "for name, model_path in embedding_models.items():\n",
    "    print(f\"  - {name}: {model_path}\")\n",
    "\n",
    "def get_embeddings_transformers(texts, model_name, tokenizer, model, batch_size=16, max_length=256):\n",
    "    \"\"\"Extract embeddings using transformers library\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    all_embeddings = []\n",
    "\n",
    "    print(f\"Extracting embeddings using {model_name}...\")\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Processing {model_name}\"):\n",
    "        batch = list(texts[i:i+batch_size])\n",
    "        tokens = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                outputs = model(**tokens, return_dict=True)\n",
    "                # Use mean pooling of last hidden states\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                all_embeddings.append(embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size}: {e}\")\n",
    "                # Fallback\n",
    "                embeddings = np.zeros((len(batch), 768))\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def get_embeddings_sentence_transformers(texts, model_name, batch_size=16):\n",
    "    \"\"\"Extract embeddings using sentence-transformers library (for E5 models)\"\"\"\n",
    "    print(f\"Loading SentenceTransformer model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Extracting embeddings using {model_name} (SentenceTransformer)...\")\n",
    "\n",
    "    # For E5 models, add instruction prefix\n",
    "    if 'e5' in model_name.lower():\n",
    "        # Add query prefix for E5 models to improve performance\n",
    "        processed_texts = [f\"query: {text}\" for text in texts]\n",
    "    else:\n",
    "        processed_texts = list(texts)\n",
    "\n",
    "    # Extract embeddings in batches\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(processed_texts), batch_size), desc=f\"Processing {model_name}\"):\n",
    "        batch = processed_texts[i:i+batch_size]\n",
    "        try:\n",
    "            embeddings = model.encode(batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "            all_embeddings.append(embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size}: {e}\")\n",
    "            # Fallback\n",
    "            embeddings = np.zeros((len(batch), model.get_sentence_embedding_dimension()))\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Extract embeddings using multiple models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Choose models based on available resources\n",
    "if torch.cuda.is_available():\n",
    "    selected_models = ['multilingual_e5', 'japanese_bert']  # Use best models for GPU\n",
    "else:\n",
    "    selected_models = ['e5_small']  # Use smaller model for CPU\n",
    "\n",
    "embedding_results = {}\n",
    "\n",
    "for model_key in selected_models:\n",
    "    model_path = embedding_models[model_key]\n",
    "    print(f\"\\n--- Processing {model_key} ---\")\n",
    "\n",
    "    try:\n",
    "        if 'e5' in model_key:\n",
    "            # Use sentence-transformers for E5 models (more optimized)\n",
    "            X_train_emb = get_embeddings_sentence_transformers(df_train['Sentence'], model_path)\n",
    "            X_test_emb = get_embeddings_sentence_transformers(df_test['Sentence'], model_path)\n",
    "        else:\n",
    "            # Use transformers library for BERT/RoBERTa models\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModel.from_pretrained(model_path)\n",
    "            model.to(device)\n",
    "\n",
    "            X_train_emb = get_embeddings_transformers(df_train['Sentence'], model_key, tokenizer, model)\n",
    "            X_test_emb = get_embeddings_transformers(df_test['Sentence'], model_key, tokenizer, model)\n",
    "\n",
    "        embedding_results[model_key] = {\n",
    "            'X_train': X_train_emb,\n",
    "            'X_test': X_test_emb,\n",
    "            'dim': X_train_emb.shape[1]\n",
    "        }\n",
    "\n",
    "        print(f\"  {model_key} embeddings shape: Train {X_train_emb.shape}, Test {X_test_emb.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error with {model_key}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 3. COMBINE EMBEDDINGS AND ORIGINAL FEATURES\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE COMBINATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original emotion features\n",
    "orig_feature_cols = [\n",
    "    \"Writer_Joy\", \"Writer_Sadness\", \"Writer_Anticipation\", \"Writer_Surprise\",\n",
    "    \"Writer_Anger\", \"Writer_Fear\", \"Writer_Disgust\", \"Writer_Trust\"\n",
    "]\n",
    "\n",
    "available_cols = [col for col in orig_feature_cols if col in df_train.columns]\n",
    "print(f\"Available original features: {available_cols}\")\n",
    "\n",
    "if available_cols:\n",
    "    X_train_orig = df_train[available_cols].fillna(0).to_numpy()\n",
    "    X_test_orig = df_test[available_cols].fillna(0).to_numpy()\n",
    "    print(f\"Original features shape: Train {X_train_orig.shape}, Test {X_test_orig.shape}\")\n",
    "else:\n",
    "    X_train_orig = None\n",
    "    X_test_orig = None\n",
    "    print(\"No original emotion features available\")\n",
    "\n",
    "# Combine all embeddings\n",
    "combined_train_embeddings = []\n",
    "combined_test_embeddings = []\n",
    "embedding_info = []\n",
    "\n",
    "for model_key, results in embedding_results.items():\n",
    "    combined_train_embeddings.append(results['X_train'])\n",
    "    combined_test_embeddings.append(results['X_test'])\n",
    "    embedding_info.append(f\"{model_key}({results['dim']})\")\n",
    "\n",
    "if combined_train_embeddings:\n",
    "    X_train_embeddings = np.hstack(combined_train_embeddings)\n",
    "    X_test_embeddings = np.hstack(combined_test_embeddings)\n",
    "    print(f\"Combined embeddings: {' + '.join(embedding_info)}\")\n",
    "    print(f\"Combined embedding shape: Train {X_train_embeddings.shape}, Test {X_test_embeddings.shape}\")\n",
    "else:\n",
    "    raise ValueError(\"No embeddings were successfully extracted!\")\n",
    "\n",
    "# Final feature combination\n",
    "if X_train_orig is not None:\n",
    "    X_train_final = np.hstack([X_train_embeddings, X_train_orig])\n",
    "    X_test_final = np.hstack([X_test_embeddings, X_test_orig])\n",
    "    print(f\"Final combined features: Train {X_train_final.shape}, Test {X_test_final.shape}\")\n",
    "else:\n",
    "    X_train_final = X_train_embeddings\n",
    "    X_test_final = X_test_embeddings\n",
    "    print(f\"Using embeddings only: Train {X_train_final.shape}, Test {X_test_final.shape}\")\n",
    "\n",
    "# Prepare target variables\n",
    "y_train = df_train['sentiment'].values\n",
    "y_test = df_test['sentiment'].values\n",
    "\n",
    "# 4. XGBOOST WITH OPTUNA HYPERPARAMETER OPTIMIZATION\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST WITH OPTUNA OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert to float32 to avoid device mismatch\n",
    "X_train_final = np.array(X_train_final, dtype=np.float32)\n",
    "X_test_final = np.array(X_test_final, dtype=np.float32)\n",
    "\n",
    "# Create optimization subset\n",
    "optimization_size = min(2000, len(X_train_final))\n",
    "opt_indices = np.random.choice(len(X_train_final), optimization_size, replace=False)\n",
    "X_opt = X_train_final[opt_indices]\n",
    "y_opt = y_train[opt_indices]\n",
    "\n",
    "print(f\"Using {optimization_size} samples for hyperparameter optimization...\")\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"Optuna objective function for XGBoost hyperparameter optimization\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # 3-fold cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_opt, y_opt,\n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"Starting Optuna hyperparameter optimization...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    optuna_objective,\n",
    "    n_trials=50,\n",
    "    timeout=600,  # 10 minutes timeout\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(f\"\\nOptuna Optimization Results:\")\n",
    "print(f\"Best F1 Score: {best_score:.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train final model\n",
    "print(f\"\\nTraining final XGBoost model...\")\n",
    "final_params = best_params.copy()\n",
    "final_params.update({\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "})\n",
    "\n",
    "final_xgb = xgb.XGBClassifier(**final_params)\n",
    "final_xgb.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_test_final, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 5. EVALUATION AND RESULTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = final_xgb.predict(X_test_final)\n",
    "y_proba_xgb = final_xgb.predict_proba(X_test_final)\n",
    "\n",
    "# Classification report\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=4, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# F1 Score\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "print(f\"\\nWeighted F1-Score: {xgb_f1:.4f}\")\n",
    "\n",
    "# 6. CONFUSION MATRIX\n",
    "print(\"\\nGenerating Confusion Matrix...\")\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(cm_xgb, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"XGBoost Confusion Matrix\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. AUC-ROC CURVES\n",
    "print(\"\\nGenerating AUC-ROC Curves...\")\n",
    "try:\n",
    "    auc_roc_xgb = roc_auc_score(y_test, y_proba_xgb, multi_class='ovr')\n",
    "    print(f\"XGBoost AUC-ROC (OvR macro-average): {auc_roc_xgb:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    for i, (color, label) in enumerate(zip(colors, sentiment_labels)):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            (y_test == i).astype(int),\n",
    "            y_proba_xgb[:, i],\n",
    "            name=f'{label} (AUC = {roc_auc_score((y_test == i).astype(int), y_proba_xgb[:, i]):.3f})',\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "    plt.title(\"XGBoost AUC-ROC Curves\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate AUC-ROC: {e}\")\n",
    "\n",
    "# 8. FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = final_xgb.feature_importances_\n",
    "n_embedding_features = X_train_embeddings.shape[1]\n",
    "n_original_features = len(available_cols) if available_cols else 0\n",
    "\n",
    "print(f\"Total features: {X_train_final.shape[1]}\")\n",
    "print(f\"Embedding features: {n_embedding_features}\")\n",
    "print(f\"Original emotion features: {n_original_features}\")\n",
    "\n",
    "# Analyze original features if available\n",
    "if n_original_features > 0:\n",
    "    original_feature_importance = feature_importance[n_embedding_features:]\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': available_cols,\n",
    "        'Importance': original_feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"\\nOriginal Feature Importance (Excluding Embeddings):\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in feature_importance_df.iterrows():\n",
    "        print(f\"{row['Feature']:25s}: {row['Importance']:.6f}\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('XGBoost Feature Importance (Original Features Only)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Top overall features\n",
    "top_features_idx = np.argsort(feature_importance)[-20:]  # Top 20 features\n",
    "print(f\"\\nTop 20 Feature Importances (All Features):\")\n",
    "for i, idx in enumerate(reversed(top_features_idx)):\n",
    "    feature_type = \"Original\" if idx >= n_embedding_features else \"Embedding\"\n",
    "    if idx >= n_embedding_features and available_cols:\n",
    "        feature_name = available_cols[idx - n_embedding_features]\n",
    "    else:\n",
    "        feature_name = f\"Feature_{idx}\"\n",
    "    print(f\"{i+1:2d}. {feature_type} - {feature_name}: {feature_importance[idx]:.6f}\")\n",
    "\n",
    "# 9. SHAP ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    print(\"Installing SHAP...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"shap\"])\n",
    "    import shap\n",
    "\n",
    "# Create model for SHAP analysis (use subset for efficiency)\n",
    "if n_original_features > 0:\n",
    "    # Use original features only for cleaner interpretation\n",
    "    X_train_shap = X_train_final[:, n_embedding_features:]\n",
    "    X_test_shap = X_test_final[:, n_embedding_features:]\n",
    "    feature_names_shap = available_cols\n",
    "\n",
    "    print(\"Training SHAP model on original features...\")\n",
    "    shap_params = final_params.copy()\n",
    "    shap_params['n_estimators'] = min(100, shap_params.get('n_estimators', 100))\n",
    "\n",
    "    shap_model = xgb.XGBClassifier(**shap_params)\n",
    "    shap_model.fit(X_train_shap, y_train)\n",
    "\n",
    "    # Evaluate original-features-only model\n",
    "    y_pred_orig = shap_model.predict(X_test_shap)\n",
    "    orig_f1 = f1_score(y_test, y_pred_orig, average='weighted')\n",
    "    print(f\"Original features only F1-Score: {orig_f1:.4f}\")\n",
    "    print(f\"Performance drop without embeddings: {((xgb_f1 - orig_f1) / xgb_f1 * 100):.2f}%\")\n",
    "\n",
    "else:\n",
    "    # Use subset of embedding features\n",
    "    print(\"Using subset of embedding features for SHAP analysis...\")\n",
    "    X_train_shap = X_train_final[:, :50]  # First 50 features\n",
    "    X_test_shap = X_test_final[:, :50]\n",
    "    feature_names_shap = [f'Embedding_{i}' for i in range(50)]\n",
    "\n",
    "    shap_params = final_params.copy()\n",
    "    shap_params['n_estimators'] = min(100, shap_params.get('n_estimators', 100))\n",
    "\n",
    "    shap_model = xgb.XGBClassifier(**shap_params)\n",
    "    shap_model.fit(X_train_shap, y_train)\n",
    "\n",
    "# SHAP explainer\n",
    "print(\"Initializing SHAP explainer...\")\n",
    "explainer = shap.Explainer(shap_model)\n",
    "\n",
    "# Sample for SHAP analysis\n",
    "n_shap_samples = min(300, len(X_test_shap))  # Reduced for speed\n",
    "shap_indices = np.random.choice(len(X_test_shap), n_shap_samples, replace=False)\n",
    "X_shap = X_test_shap[shap_indices]\n",
    "y_shap = y_test[shap_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for {n_shap_samples} samples...\")\n",
    "shap_values = explainer(X_shap)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "print(\"Generating SHAP summary plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "try:\n",
    "    if len(shap_values.values.shape) == 3:\n",
    "        # Multi-class: aggregate across classes\n",
    "        shap_values_agg = np.abs(shap_values.values).mean(axis=2)\n",
    "        shap.summary_plot(shap_values_agg, X_shap, feature_names=feature_names_shap, show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values.values, X_shap, feature_names=feature_names_shap, show=False)\n",
    "    plt.title(\"SHAP Summary Plot - Feature Importance\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not create SHAP summary plot: {e}\")\n",
    "    # Alternative manual plot\n",
    "    plt.clf()\n",
    "    mean_abs_shap = np.abs(shap_values.values).mean(axis=(0, 2)) if len(shap_values.values.shape) == 3 else np.abs(shap_values.values).mean(axis=0)\n",
    "    sorted_idx = np.argsort(mean_abs_shap)\n",
    "    plt.barh(range(len(sorted_idx)), mean_abs_shap[sorted_idx])\n",
    "    plt.yticks(range(len(sorted_idx)), [feature_names_shap[i] for i in sorted_idx])\n",
    "    plt.xlabel('Mean |SHAP value|')\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# SHAP Bar Plot\n",
    "print(\"Generating SHAP bar plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "try:\n",
    "    if len(shap_values.values.shape) == 3:\n",
    "        mean_abs_shap = np.abs(shap_values.values).mean(axis=(0, 2))\n",
    "        sorted_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "\n",
    "        plt.barh(range(len(sorted_idx)), mean_abs_shap[sorted_idx])\n",
    "        plt.yticks(range(len(sorted_idx)), [feature_names_shap[i] for i in sorted_idx])\n",
    "        plt.xlabel('Mean |SHAP value|')\n",
    "        plt.title('SHAP Feature Importance Bar Plot')\n",
    "        plt.gca().invert_yaxis()\n",
    "    else:\n",
    "        shap.summary_plot(shap_values.values, X_shap, feature_names=feature_names_shap, plot_type=\"bar\", show=False)\n",
    "        plt.title(\"SHAP Feature Importance Bar Plot\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not create SHAP bar plot: {e}\")\n",
    "\n",
    "# SHAP Waterfall plots\n",
    "print(\"Creating SHAP waterfall plots...\")\n",
    "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# Find examples from each class\n",
    "class_examples = {}\n",
    "for class_label in [0, 1, 2]:\n",
    "    class_mask = y_shap == class_label\n",
    "    if np.any(class_mask):\n",
    "        class_indices = np.where(class_mask)[0]\n",
    "        class_examples[class_label] = class_indices[0]\n",
    "\n",
    "for class_label, example_idx in class_examples.items():\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        if len(shap_values.values.shape) == 3:\n",
    "            explanation = shap.Explanation(\n",
    "                values=shap_values.values[example_idx, :, class_label],\n",
    "                base_values=explainer.expected_value[class_label],\n",
    "                data=X_shap[example_idx],\n",
    "                feature_names=feature_names_shap\n",
    "            )\n",
    "        else:\n",
    "            explanation = shap.Explanation(\n",
    "                values=shap_values.values[example_idx],\n",
    "                base_values=explainer.expected_value,\n",
    "                data=X_shap[example_idx],\n",
    "                feature_names=feature_names_shap\n",
    "            )\n",
    "\n",
    "        shap.waterfall_plot(explanation, show=False)\n",
    "        plt.title(f\"SHAP Waterfall - {sentiment_labels[class_label]} Example\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create waterfall plot for {sentiment_labels[class_label]}: {e}\")\n",
    "\n",
    "# 10. FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare AUC-ROC string\n",
    "if 'auc_roc_xgb' in locals() and auc_roc_xgb is not None:\n",
    "    auc_roc_str = f\"{auc_roc_xgb:.4f}\"\n",
    "else:\n",
    "    auc_roc_str = \"N/A\"\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset: WRIME Japanese Sentiment Analysis\n",
    "- Training samples: {len(df_train)}\n",
    "- Test samples: {len(df_test)}\n",
    "- Classes: 3 (Negative=0, Neutral=1, Positive=2)\n",
    "\n",
    "Embedding Models Used: {', '.join(embedding_results.keys())}\n",
    "- Total embedding dimensions: {n_embedding_features}\n",
    "- Original emotion features: {n_original_features}\n",
    "- Final feature count: {X_train_final.shape[1]}\n",
    "\n",
    "XGBoost Performance:\n",
    "- Best Optuna F1-Score: {best_score:.4f}\n",
    "- Final Test F1-Score: {xgb_f1:.4f}\n",
    "- AUC-ROC Score: {auc_roc_str}\n",
    "\n",
    "Key Insights:\n",
    "- Embedding features contributed {((xgb_f1 - orig_f1) / xgb_f1 * 100):.1f}% performance gain\n",
    "- SHAP analysis completed on {n_shap_samples} samples\n",
    "- All visualizations generated successfully\n",
    "\n",
    "Hyperparameter Optimization:\n",
    "- {study.trials} Optuna trials completed\n",
    "- Best parameters: {best_params}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Analysis complete! All visualizations have been generated.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "XGBOOST +E5-BASE MULTILINGUAL EMBEDDINGS",
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/misfits/xgboost-e5-base-multilingual-embeddings.4b899a8d-dddf-4617-bbd7-ab28ec26752f.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250625/auto/storage/goog4_request&X-Goog-Date=20250625T132722Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0468ca91e94f5908e5fd1edc8c501d0f94c94dd6f35abb684d3d55e519b0f9d47dee9e2469aba901a3643d4090a1f20d36c1e55c9ea3a48a91499d73096599bd63b2a69388a539adcc126019ff75be67f7614984e6ea19fe063f0ae27866759459ae777461c79d26976417ce19e1f5811c27910233210f42730e10fdef132a9128b48939dc3821a04e7fa5e998605de10752d6bdb8da83384e67cabc7a896a6b695ab0ab4f30de159eac0142e0e6ad1d66fdcb1395ce204f1705a15339149649b35177fbd036b2da27486ceddb0f16c204c7badbb46ec7db1d8f320caf18cbb20725e2281429a0b69e976cb64c6711532f7fec4e954c51c5bf0d5d4def61a19e",
     "timestamp": 1750863147309
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
