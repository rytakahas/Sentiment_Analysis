{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYZq3rnMdqHgKgovnweEs8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Complete Data Lakehouse Pipeline with BigQuery\n","# Data Lake -> ETL -> Data Quality -> Data Warehouse\n","\n","# For Kaggle or Colab T4\n","!pip install openpyxl boto3 confluent_kafka google-cloud-bigquery pandas pyarrow great-expectations datasets --quiet\n"],"metadata":{"id":"sJ7UM6aBwKbZ","executionInfo":{"status":"ok","timestamp":1751721813816,"user_tz":-120,"elapsed":14630,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip uninstall numpy pandas\n","!pip install numpy==1.24.3 pandas==2.0.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"s-beVJq1wLfd","executionInfo":{"status":"ok","timestamp":1751721959429,"user_tz":-120,"elapsed":29494,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"bda67ee8-7c6f-43c9-bef2-8e2e99b1199f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Would remove:\n","    /usr/local/bin/f2py\n","    /usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/*\n","    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n","    /usr/local/lib/python3.11/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n","    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n","    /usr/local/lib/python3.11/dist-packages/numpy/*\n","Proceed (Y/n)? Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n","    uninstall_pathset = req.uninstall(\n","                        ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n","    uninstalled_pathset.remove(auto_confirm, verbose)\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n","    if auto_confirm or self._allowed_to_proceed(verbose):\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n","    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n","    response = input(message)\n","               ^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n","    return command.main(cmd_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n","    return self._main(args)\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n","    return run(options, args)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n","    def critical(self, msg, *args, **kwargs):\n","\n","KeyboardInterrupt\n","^C\n","Collecting numpy==1.24.3\n","  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting pandas==2.0.3\n","  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n","Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, pandas\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.1.4\n","    Uninstalling pandas-2.1.4:\n","      Successfully uninstalled pandas-2.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n","albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n","xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n","albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n","mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n","pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n","blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n","xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.3 pandas-2.0.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"db1bc324a0224b16b4c99108a621fd47"}},"metadata":{}}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GORdRyJKvVGe","executionInfo":{"status":"ok","timestamp":1751722867034,"user_tz":-120,"elapsed":67060,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"4cbbe2cf-b0f4-46f8-c5c0-747e4e76f921"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dataset `superset_sandbox` already exists.\n","ğŸš€ STARTING DATA LAKEHOUSE PIPELINE\n","==================================================\n","\n","ğŸ“¥ PHASE 1: DATA INGESTION TO DATA LAKE\n","\n","ğŸ‡¯ğŸ‡µ INGESTING JAPANESE SENTIMENT DATA TO DATA LAKE\n","ğŸ“Š Loaded 43200 Japanese sentiment records\n","ğŸ—ï¸ Uploading Japanese Sentiment to DATA LAKE: able-balm-454718-n8.superset_sandbox.jp_sentiment_raw_db\n","âœ… Uploaded to DATA LAKE: able-balm-454718-n8.superset_sandbox.jp_sentiment_raw_db\n","\n","ğŸ›ï¸ INGESTING AMAZON REVIEWS DATA TO DATA LAKE\n","ğŸ“¥ Downloading Amazon reviews dataset...\n","ğŸ“Š Loaded 20000 Amazon review records\n","ğŸ—ï¸ Uploading Amazon Reviews to DATA LAKE: able-balm-454718-n8.superset_sandbox.amazon_reviews_raw_db\n","âœ… Uploaded to DATA LAKE: able-balm-454718-n8.superset_sandbox.amazon_reviews_raw_db\n","\n","ğŸ”„ PHASE 2: ETL PROCESSING\n","\n","ğŸ”„ ETL PIPELINE - JAPANESE SENTIMENT\n","ğŸ“Š Loaded 43200 rows from data lake\n","âœ… ETL completed. 43200 rows processed\n","\n","ğŸ”„ ETL PIPELINE - REVIEWS\n","\n","ğŸ”¥ SETTING UP GEN-Z TRANSLATION MAPPINGS\n","âœ… Loaded 223 translation mappings\n","ğŸ“Š Loaded 20000 rows from data lake\n","âœ… ETL completed. 20000 rows processed\n","\n","ğŸ” PHASE 3: DATA QUALITY CHECKS\n","\n","ğŸ” DATA QUALITY CHECKS - JP_SENTIMENT\n","âœ… Data quality validation completed\n","ğŸ“Š Total records: 43200\n","ğŸ“Š Null percentage: 1.79%\n","ğŸ“Š Duplicate records: 0\n","ğŸ“ˆ Quality score: 1.000\n","\n","ğŸ” DATA QUALITY CHECKS - REVIEWS\n","âœ… Data quality validation completed\n","ğŸ“Š Total records: 20000\n","ğŸ“Š Null percentage: 0.00%\n","ğŸ“Š Duplicate records: 0\n","ğŸ“ˆ Quality score: 1.000\n","\n","ğŸª PHASE 4: DATA WAREHOUSE STORAGE\n","\n","ğŸª STORING IN DATA WAREHOUSE - jp_sentiment\n","ğŸš€ Uploading to DATA WAREHOUSE: able-balm-454718-n8.superset_sandbox.jp_sentiment_cleaned_db\n","âœ… Uploaded to DATA WAREHOUSE: able-balm-454718-n8.superset_sandbox.jp_sentiment_cleaned_db\n","\n","ğŸª STORING IN DATA WAREHOUSE - reviews\n","ğŸš€ Uploading to DATA WAREHOUSE: able-balm-454718-n8.superset_sandbox.reviews_cleaned_db\n","âœ… Uploaded to DATA WAREHOUSE: able-balm-454718-n8.superset_sandbox.reviews_cleaned_db\n","\n","ğŸ“Š PIPELINE SUMMARY\n","==================================================\n","âœ… Data Lake Tables Created:\n","   - jp_sentiment_raw_db\n","   - reviews_raw_db\n","âœ… Data Warehouse Tables Created:\n","   - jp_sentiment_cleaned_db\n","   - reviews_cleaned_db\n","ğŸ” Quality Scores:\n","   - Japanese Sentiment: 1.000\n","   - Reviews: 1.000\n","ğŸ‰ DATA LAKEHOUSE PIPELINE COMPLETED SUCCESSFULLY!\n","\n","ğŸ“‹ ALL TABLES IN superset_sandbox:\n","  - amazon_reviews_raw_db\n","  - battery_data_sp1_initial_capacity_10_16_2015_raw_db\n","  - battery_data_sp2_25c_fuds_raw_db\n","  - jp_sentiment_cleaned_db\n","  - jp_sentiment_raw_db\n","  - reviews_cleaned_db\n","\n","ğŸ” QUERYING DATA WAREHOUSE TABLES\n","\n","ğŸ“Š Sample data from jp_sentiment_cleaned_db:\n","                                            Sentence  UserID  \\\n","0                     ã¼ã‘ã£ã¨ã—ã¦ãŸã‚‰ã“ã‚“ãªæ™‚é–“ï½¡ãƒãƒ£ãƒªã‚ã‚‹ã‹ã‚‰é£Ÿã¹ã«ã§ãŸã„ã®ã«â€¦       1   \n","1  ä»Šæ—¥ã®æœˆã‚‚ç™½ãã¦æ˜ã‚‹ã„ã€‚æ˜¨æ—¥ã‚ˆã‚Šé›²ãŒå°‘ãªãã¦ã‚­ãƒ¬ã‚¤ãª? ã¨ç«‹ã¡æ­¢ã¾ã‚‹å¸°ã‚Šé“ï½¡ãƒãƒ£ãƒªãªã—ç”Ÿæ´»ã‚‚...       1   \n","2                 æ—©å¯ã™ã‚‹ã¤ã‚‚ã‚ŠãŒé£²ã¿ç‰©ãŒãªããªã‚Šã‚³ãƒ³ãƒ“ãƒ‹ã¸ï½¡ã‚“ï½¤ä»Šæ—¥ã€é¢¨ãŒæ¶¼ã—ã„ãªã€‚       1   \n","3                                           çœ ã„ã€çœ ã‚Œãªã„ã€‚       1   \n","4    ãŸã ã„ã¾? ã£ã¦æ–°ä½“æ“ã—ã¦ã‚‹ã‚„ã‚“!å¤–é£Ÿã™ã‚‹æ°—æº€ã€…ã§å®¶ã«ä½•ã‚‚ãªã„ã®ã«!ãƒ†ãƒ¬ãƒ“ã‹ã‚‰é›¢ã‚Œã‚‰ã‚Œãªã„â€¦!       1   \n","\n","           Datetime Train_Dev_Test  Writer_Joy  Writer_Sadness  \\\n","0  2012/07/31 23:48          train           0               1   \n","1  2012/08/02 23:09          train           3               0   \n","2  2012/08/05 00:50          train           1               1   \n","3  2012/08/08 01:36          train           0               2   \n","4  2012/08/09 22:24          train           2               1   \n","\n","   Writer_Anticipation  Writer_Surprise  Writer_Anger  Writer_Fear  ...  \\\n","0                    2                1             1            0  ...   \n","1                    3                0             0            0  ...   \n","2                    1                1             0            0  ...   \n","3                    1                0             0            1  ...   \n","4                    3                2             0            1  ...   \n","\n","       data_source        ingestion_batch  \\\n","0  wrime_sentiment  batch_20250705_134001   \n","1  wrime_sentiment  batch_20250705_134001   \n","2  wrime_sentiment  batch_20250705_134001   \n","3  wrime_sentiment  batch_20250705_134001   \n","4  wrime_sentiment  batch_20250705_134001   \n","\n","                                    sentence_cleaned  sentence_length  \\\n","0                     ã¼ã‘ã£ã¨ã—ã¦ãŸã‚‰ã“ã‚“ãªæ™‚é–“ï½¡ãƒãƒ£ãƒªã‚ã‚‹ã‹ã‚‰é£Ÿã¹ã«ã§ãŸã„ã®ã«â€¦               30   \n","1  ä»Šæ—¥ã®æœˆã‚‚ç™½ãã¦æ˜ã‚‹ã„ã€‚æ˜¨æ—¥ã‚ˆã‚Šé›²ãŒå°‘ãªãã¦ã‚­ãƒ¬ã‚¤ãª? ã¨ç«‹ã¡æ­¢ã¾ã‚‹å¸°ã‚Šé“ï½¡ãƒãƒ£ãƒªãªã—ç”Ÿæ´»ã‚‚...               51   \n","2                 æ—©å¯ã™ã‚‹ã¤ã‚‚ã‚ŠãŒé£²ã¿ç‰©ãŒãªããªã‚Šã‚³ãƒ³ãƒ“ãƒ‹ã¸ï½¡ã‚“ï½¤ä»Šæ—¥ã€é¢¨ãŒæ¶¼ã—ã„ãªã€‚               34   \n","3                                           çœ ã„ã€çœ ã‚Œãªã„ã€‚                8   \n","4    ãŸã ã„ã¾? ã£ã¦æ–°ä½“æ“ã—ã¦ã‚‹ã‚„ã‚“!å¤–é£Ÿã™ã‚‹æ°—æº€ã€…ã§å®¶ã«ä½•ã‚‚ãªã„ã®ã«!ãƒ†ãƒ¬ãƒ“ã‹ã‚‰é›¢ã‚Œã‚‰ã‚Œãªã„â€¦!               47   \n","\n","   word_count  emotion_intensity  dominant_emotion  \\\n","0           1                  0              None   \n","1           2                  0              None   \n","2           1                  0              None   \n","3           1                  0              None   \n","4           2                  0              None   \n","\n","                      processed_at  etl_version  quality_score  \n","0 2025-07-05 13:40:13.755808+00:00          1.0            1.0  \n","1 2025-07-05 13:40:13.755808+00:00          1.0            1.0  \n","2 2025-07-05 13:40:13.755808+00:00          1.0            1.0  \n","3 2025-07-05 13:40:13.755808+00:00          1.0            1.0  \n","4 2025-07-05 13:40:13.755808+00:00          1.0            1.0  \n","\n","[5 rows x 56 columns]\n","\n","ğŸ“Š Sample data from reviews_cleaned_db:\n","  reviewText  Positive     id                        loaded_at  \\\n","0        fun         1    766 2025-07-05 13:40:06.100947+00:00   \n","1        Fun         1  11381 2025-07-05 13:40:06.100947+00:00   \n","2       Bleh         0   1023 2025-07-05 13:40:06.100947+00:00   \n","3       lame         0   3465 2025-07-05 13:40:06.100947+00:00   \n","4       No:(         0  16380 2025-07-05 13:40:06.100947+00:00   \n","\n","      data_source        ingestion_batch review_text_cleaned  \\\n","0  amazon_reviews  batch_20250705_134006                 fun   \n","1  amazon_reviews  batch_20250705_134006                 Fun   \n","2  amazon_reviews  batch_20250705_134006                Bleh   \n","3  amazon_reviews  batch_20250705_134006                lame   \n","4  amazon_reviews  batch_20250705_134006                No:(   \n","\n","  review_text_translated  review_length  word_count  has_review_text  \\\n","0                    fun              3           1             True   \n","1                    Fun              3           1             True   \n","2                   Bleh              4           1             True   \n","3                   lame              4           1             True   \n","4                   No:(              4           1             True   \n","\n","  review_sentiment                     processed_at etl_version  quality_score  \n","0          neutral 2025-07-05 13:40:50.393245+00:00         1.0            1.0  \n","1          neutral 2025-07-05 13:40:50.393245+00:00         1.0            1.0  \n","2          neutral 2025-07-05 13:40:50.393245+00:00         1.0            1.0  \n","3          neutral 2025-07-05 13:40:50.393245+00:00         1.0            1.0  \n","4          neutral 2025-07-05 13:40:50.393245+00:00         1.0            1.0  \n"]}],"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","import os\n","import requests\n","import pandas as pd\n","import io\n","import re\n","from google.cloud import bigquery\n","from google.cloud.exceptions import NotFound\n","import zipfile\n","import json\n","\n","# ========================================\n","# CONFIGURATION\n","# ========================================\n","\n","project_id = \"able-balm-454718-n8\"\n","dataset_id = \"superset_sandbox\"\n","data_lake_suffix = \"_raw_db\"\n","data_warehouse_suffix = \"_cleaned_db\"\n","\n","client = bigquery.Client(project=project_id)\n","\n","# Make sure dataset exists\n","try:\n","    client.get_dataset(dataset_id)\n","    print(f\"âœ… Dataset `{dataset_id}` already exists.\")\n","except NotFound:\n","    dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n","    dataset = client.create_dataset(dataset)\n","    print(f\"âœ… Created dataset `{dataset_id}`\")\n","\n","# ========================================\n","# COLUMN SANITIZER for BigQuery\n","# ========================================\n","\n","def sanitize_col(col):\n","    # Lowercase, replace non-alphanum with _, strip repeated _, strip _ ends, max 128 chars (BQ limit)\n","    new_col = re.sub(r'[^a-zA-Z0-9_]', '_', str(col))\n","    new_col = re.sub(r'_+', '_', new_col)\n","    new_col = new_col.strip('_')\n","    return new_col[:128] if new_col else \"col\"\n","\n","def sanitize_columns(df):\n","    df.columns = [sanitize_col(col) for col in df.columns]\n","    return df\n","\n","# ========================================\n","# DATA INGESTION - JAPANESE SENTIMENT DATA TO DATA LAKE\n","# ========================================\n","\n","def ingest_japanese_sentiment_to_lake():\n","    print(\"\\nğŸ‡¯ğŸ‡µ INGESTING JAPANESE SENTIMENT DATA TO DATA LAKE\")\n","    wrime_url = \"https://raw.githubusercontent.com/ids-cv/wrime/refs/heads/master/wrime-ver1.tsv\"\n","    wrime_path = \"wrime-ver1.tsv\"\n","\n","    try:\n","        if not os.path.exists(wrime_path):\n","            print(\"ğŸ“¥ Downloading WRIME dataset...\")\n","            r = requests.get(wrime_url)\n","            r.raise_for_status()\n","            with open(wrime_path, \"wb\") as f:\n","                f.write(r.content)\n","\n","        df_wrime = pd.read_csv(wrime_path, sep=\"\\t\")\n","        df_wrime = df_wrime.dropna(subset=[\"Sentence\"])\n","        print(f\"ğŸ“Š Loaded {len(df_wrime)} Japanese sentiment records\")\n","\n","        # Sanitize columns\n","        df_wrime = sanitize_columns(df_wrime)\n","\n","        # Add data lake metadata\n","        df_wrime[\"id\"] = df_wrime.index\n","        df_wrime[\"loaded_at\"] = pd.Timestamp.utcnow()\n","        df_wrime[\"data_source\"] = \"wrime_sentiment\"\n","        df_wrime[\"ingestion_batch\"] = f\"batch_{pd.Timestamp.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n","\n","        for col in df_wrime.columns:\n","            if df_wrime[col].dtype == \"object\":\n","                df_wrime[col] = df_wrime[col].astype(str)\n","\n","        table_id = f\"{project_id}.{dataset_id}.jp_sentiment{data_lake_suffix}\"\n","        print(f\"ğŸ—ï¸ Uploading Japanese Sentiment to DATA LAKE: {table_id}\")\n","        job_config = bigquery.LoadJobConfig(\n","            autodetect=True,\n","            write_disposition=\"WRITE_TRUNCATE\"\n","        )\n","        job = client.load_table_from_dataframe(df_wrime, table_id, job_config=job_config)\n","        job.result()\n","        print(f\"âœ… Uploaded to DATA LAKE: {table_id}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error ingesting Japanese sentiment data: {str(e)}\")\n","        raise\n","\n","# ========================================\n","# DATA INGESTION - AMAZON REVIEWS TO DATA LAKE (Alternative to HuggingFace)\n","# ========================================\n","\n","def ingest_amazon_reviews_to_lake():\n","    print(\"\\nğŸ›ï¸ INGESTING AMAZON REVIEWS DATA TO DATA LAKE\")\n","\n","    try:\n","        # Using Amazon product reviews dataset from a direct CSV source\n","        amazon_url = \"https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv\"\n","\n","        print(\"ğŸ“¥ Downloading Amazon reviews dataset...\")\n","        df_amazon = pd.read_csv(amazon_url)\n","        print(f\"ğŸ“Š Loaded {len(df_amazon)} Amazon review records\")\n","\n","        # Sanitize columns\n","        df_amazon = sanitize_columns(df_amazon)\n","\n","        # Add data lake metadata\n","        df_amazon[\"id\"] = df_amazon.index\n","        df_amazon[\"loaded_at\"] = pd.Timestamp.utcnow()\n","        df_amazon[\"data_source\"] = \"amazon_reviews\"\n","        df_amazon[\"ingestion_batch\"] = f\"batch_{pd.Timestamp.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n","\n","        for col in df_amazon.columns:\n","            if df_amazon[col].dtype == \"object\":\n","                df_amazon[col] = df_amazon[col].astype(str)\n","\n","        table_id = f\"{project_id}.{dataset_id}.amazon_reviews{data_lake_suffix}\"\n","        print(f\"ğŸ—ï¸ Uploading Amazon Reviews to DATA LAKE: {table_id}\")\n","        job_config = bigquery.LoadJobConfig(\n","            autodetect=True,\n","            write_disposition=\"WRITE_TRUNCATE\"\n","        )\n","        job = client.load_table_from_dataframe(df_amazon, table_id, job_config=job_config)\n","        job.result()\n","        print(f\"âœ… Uploaded to DATA LAKE: {table_id}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error ingesting Amazon reviews data: {str(e)}\")\n","        print(\"ğŸ”„ Trying alternative dataset source...\")\n","        ingest_alternative_reviews_to_lake()\n","\n","def ingest_alternative_reviews_to_lake():\n","    \"\"\"Alternative review data source if primary fails\"\"\"\n","    print(\"\\nğŸ›ï¸ INGESTING ALTERNATIVE REVIEWS DATA TO DATA LAKE\")\n","\n","    try:\n","        # Create a sample dataset from publicly available movie reviews\n","        movie_reviews_url = \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\"\n","\n","        print(\"ğŸ“¥ Downloading movie reviews dataset...\")\n","        df_reviews = pd.read_csv(movie_reviews_url, sep='\\t', header=None, names=['sentiment', 'review_text'])\n","        print(f\"ğŸ“Š Loaded {len(df_reviews)} movie review records\")\n","\n","        # Add additional columns to simulate e-commerce data\n","        df_reviews['product_category'] = pd.Categorical(['Electronics', 'Books', 'Clothing', 'Home', 'Sports']).take(\n","            pd.array([i % 5 for i in range(len(df_reviews))])\n","        )\n","        df_reviews['rating'] = df_reviews['sentiment'].apply(lambda x: 5 if x == 1 else 1)\n","        df_reviews['verified_purchase'] = pd.Categorical(['Yes', 'No']).take(\n","            pd.array([i % 2 for i in range(len(df_reviews))])\n","        )\n","\n","        # Sanitize columns\n","        df_reviews = sanitize_columns(df_reviews)\n","\n","        # Add data lake metadata\n","        df_reviews[\"id\"] = df_reviews.index\n","        df_reviews[\"loaded_at\"] = pd.Timestamp.utcnow()\n","        df_reviews[\"data_source\"] = \"movie_reviews\"\n","        df_reviews[\"ingestion_batch\"] = f\"batch_{pd.Timestamp.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n","\n","        for col in df_reviews.columns:\n","            if df_reviews[col].dtype == \"object\":\n","                df_reviews[col] = df_reviews[col].astype(str)\n","\n","        table_id = f\"{project_id}.{dataset_id}.movie_reviews{data_lake_suffix}\"\n","        print(f\"ğŸ—ï¸ Uploading Movie Reviews to DATA LAKE: {table_id}\")\n","        job_config = bigquery.LoadJobConfig(\n","            autodetect=True,\n","            write_disposition=\"WRITE_TRUNCATE\"\n","        )\n","        job = client.load_table_from_dataframe(df_reviews, table_id, job_config=job_config)\n","        job.result()\n","        print(f\"âœ… Uploaded to DATA LAKE: {table_id}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error ingesting alternative reviews data: {str(e)}\")\n","        raise\n","\n","# ========================================\n","# GEN-Z SLANG TRANSLATION SETUP\n","# ========================================\n","\n","def setup_genz_translation():\n","    print(\"\\nğŸ”¥ SETTING UP GEN-Z TRANSLATION MAPPINGS\")\n","\n","    try:\n","        # Try to get the Gen-Z slang dataset\n","        url_slang = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_slang.csv\"\n","        resp_slang = requests.get(url_slang)\n","        resp_slang.raise_for_status()\n","\n","        df_slang = pd.read_csv(io.StringIO(resp_slang.text))\n","        slang_map = {\n","            str(row['keyword']).strip().lower(): str(row['description']).strip()\n","            for _, row in df_slang.iterrows()\n","            if pd.notnull(row['keyword']) and pd.notnull(row['description'])\n","        }\n","\n","        # Try to get emoji dataset\n","        url_emoji = \"https://raw.githubusercontent.com/kaspercools/genz-dataset/refs/heads/main/genz_emojis.csv\"\n","        resp_emoji = requests.get(url_emoji)\n","        resp_emoji.raise_for_status()\n","\n","        df_emoji = pd.read_csv(io.StringIO(resp_emoji.text))\n","        emoji_map = {\n","            str(row['emoji']).strip(): str(row['Description']).strip()\n","            for _, row in df_emoji.iterrows()\n","            if pd.notnull(row['emoji']) and pd.notnull(row['Description'])\n","        }\n","\n","    except Exception as e:\n","        print(f\"âš ï¸ Could not load external slang data: {str(e)}\")\n","        print(\"ğŸ”„ Using built-in translation mappings...\")\n","\n","        # Fallback to built-in mappings\n","        slang_map = {\n","            \"lit\": \"amazing, exciting, excellent\",\n","            \"salty\": \"bitter, angry, upset\",\n","            \"flex\": \"show off, boast\",\n","            \"ghost\": \"suddenly stop communicating\",\n","            \"vibe\": \"feeling, atmosphere\",\n","            \"stan\": \"be a big fan of\",\n","            \"periodt\": \"period, end of discussion\",\n","            \"bet\": \"okay, sure, yes\",\n","            \"cap\": \"lie, false statement\",\n","            \"fire\": \"excellent, amazing\",\n","            \"sus\": \"suspicious, questionable\",\n","            \"slaps\": \"is really good\",\n","            \"hits different\": \"is exceptionally good\",\n","            \"bussin\": \"really good, excellent\",\n","            \"no cap\": \"no lie, for real\"\n","        }\n","\n","        emoji_map = {\n","            \"ğŸ’¯\": \"one hundred percent, perfect\",\n","            \"ğŸ”¥\": \"fire, excellent, amazing\",\n","            \"ğŸ˜¤\": \"frustrated, determined\",\n","            \"ğŸ’€\": \"dead, dying of laughter\",\n","            \"ğŸ¤¡\": \"clown, foolish\",\n","            \"ğŸ‘‘\": \"queen, king, royalty\",\n","            \"âœ¨\": \"sparkle, magical, special\"\n","        }\n","\n","    # Add variant patterns\n","    variant_patterns = {\n","        \"fleek\": [\"on fleek\"],\n","        \"cap\": [\"no cap\"],\n","        \"shade\": [\"throw shade\"],\n","        \"tea\": [\"spill the tea\"],\n","        \"key\": [\"low key\", \"high key\"],\n","        \"bestie\": [\"bestie vibes\"],\n","        \"grass\": [\"touch grass\"]\n","    }\n","\n","    custom_phrase_map = {}\n","    for base, phrases in variant_patterns.items():\n","        if base in slang_map:\n","            for phrase in phrases:\n","                custom_phrase_map[phrase] = slang_map[base]\n","\n","    # Manual phrase mappings\n","    manual_phrase_map = {\n","        \"on fleek\": \"something that is perfect or done really well\",\n","        \"no cap\": \"no lie, for real, telling the truth\",\n","        \"throw shade\": \"to publicly criticize or express contempt\",\n","        \"spill the tea\": \"to gossip or share juicy information\",\n","        \"low key\": \"somewhat, slightly, or secretly\",\n","        \"high key\": \"very much, obviously, or openly\",\n","        \"hits different\": \"is exceptionally good in a unique way\",\n","        \"periodt\": \"period, end of discussion, final word\"\n","    }\n","\n","    translation_map = {**manual_phrase_map, **custom_phrase_map, **slang_map, **emoji_map}\n","    print(f\"âœ… Loaded {len(translation_map)} translation mappings\")\n","    return translation_map\n","\n","def replace_slang_and_emoji(text, translation_map):\n","    if pd.isna(text):\n","        return \"\"\n","    text = str(text)\n","    for key in sorted(translation_map.keys(), key=lambda x: -len(x)):\n","        val = translation_map[key]\n","        if re.match(r'^\\W+$', key):\n","            text = text.replace(key, val)\n","        else:\n","            pattern = r'(?i)(?<!\\w){}(?=\\W|$)'.format(re.escape(key))\n","            text = re.sub(pattern, val, text)\n","    return text\n","\n","# ========================================\n","# ETL PIPELINE\n","# ========================================\n","\n","def etl_japanese_sentiment():\n","    print(\"\\nğŸ”„ ETL PIPELINE - JAPANESE SENTIMENT\")\n","    source_table = f\"{project_id}.{dataset_id}.jp_sentiment{data_lake_suffix}\"\n","    query = f\"SELECT * FROM `{source_table}`\"\n","    df = client.query(query).to_dataframe()\n","    print(f\"ğŸ“Š Loaded {len(df)} rows from data lake\")\n","\n","    # Sanitize columns again (in case query returns non-standard)\n","    df = sanitize_columns(df)\n","\n","    # Text processing\n","    text_cols = [col for col in df.columns if 'sentence' in col.lower() or 'text' in col.lower()]\n","    if text_cols:\n","        main_text_col = text_cols[0]\n","        df['sentence_cleaned'] = df[main_text_col].str.strip()\n","        df['sentence_length'] = df['sentence_cleaned'].str.len()\n","        df['word_count'] = df['sentence_cleaned'].str.split().str.len()\n","\n","    # Emotion processing\n","    emotion_cols = [col for col in df.columns if col.lower() in ['joy', 'sadness', 'anticipation', 'surprise', 'anger', 'fear', 'disgust', 'trust']]\n","    if emotion_cols:\n","        for col in emotion_cols:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","            df[f'{col.lower()}_normalized'] = df[col] / df[col].max() if df[col].max() > 0 else 0\n","\n","        df['emotion_intensity'] = df[emotion_cols].sum(axis=1)\n","        df['dominant_emotion'] = df[emotion_cols].idxmax(axis=1)\n","    else:\n","        df['emotion_intensity'] = 0\n","        df['dominant_emotion'] = None\n","\n","    # Metadata\n","    df['processed_at'] = pd.Timestamp.utcnow()\n","    df['etl_version'] = '1.0'\n","    df['quality_score'] = 1.0\n","\n","    print(f\"âœ… ETL completed. {len(df)} rows processed\")\n","    return df\n","\n","def etl_reviews():\n","    print(\"\\nğŸ”„ ETL PIPELINE - REVIEWS\")\n","    translation_map = setup_genz_translation()\n","\n","    # Try to find the reviews table\n","    tables = client.list_tables(dataset_id)\n","    review_tables = [t for t in tables if 'review' in t.table_id and t.table_id.endswith(data_lake_suffix)]\n","\n","    if not review_tables:\n","        print(\"âŒ No review tables found in data lake\")\n","        return pd.DataFrame()\n","\n","    source_table = f\"{project_id}.{dataset_id}.{review_tables[0].table_id}\"\n","    query = f\"SELECT * FROM `{source_table}`\"\n","    df = client.query(query).to_dataframe()\n","    print(f\"ğŸ“Š Loaded {len(df)} rows from data lake\")\n","\n","    # Sanitize columns again\n","    df = sanitize_columns(df)\n","\n","    # Text processing - find text columns\n","    text_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['text', 'review', 'comment', 'content'])]\n","    if text_cols:\n","        main_text_col = text_cols[0]\n","        df['review_text_cleaned'] = df[main_text_col].str.strip()\n","        df['review_text_translated'] = df['review_text_cleaned'].apply(\n","            lambda x: replace_slang_and_emoji(x, translation_map)\n","        )\n","        df['review_length'] = df['review_text_cleaned'].str.len()\n","        df['word_count'] = df['review_text_cleaned'].str.split().str.len()\n","\n","    # Rating processing - find rating columns\n","    rating_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['rating', 'score', 'sentiment'])]\n","    if rating_cols:\n","        main_rating_col = rating_cols[0]\n","        df['rating'] = pd.to_numeric(df[main_rating_col], errors='coerce')\n","\n","        # Normalize rating to 0-1 scale\n","        if df['rating'].max() > 1:\n","            df['rating_normalized'] = df['rating'] / df['rating'].max()\n","        else:\n","            df['rating_normalized'] = df['rating']\n","\n","        # Create rating categories\n","        df['rating_category'] = pd.cut(df['rating'], bins=3, labels=['Poor', 'Fair', 'Good'])\n","\n","    # Category processing - find category columns\n","    category_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['category', 'department', 'type', 'class'])]\n","    if category_cols:\n","        main_category_col = category_cols[0]\n","        df['category_cleaned'] = df[main_category_col].str.strip().str.title()\n","\n","    # Feature engineering\n","    df['has_review_text'] = df[text_cols[0]].notna() if text_cols else False\n","\n","    if 'rating' in df.columns:\n","        df['review_sentiment'] = df['rating'].apply(\n","            lambda x: 'positive' if x >= 0.6 else 'negative' if x <= 0.4 else 'neutral'\n","        )\n","    else:\n","        df['review_sentiment'] = 'neutral'\n","\n","    # Metadata\n","    df['processed_at'] = pd.Timestamp.utcnow()\n","    df['etl_version'] = '1.0'\n","    df['quality_score'] = 1.0\n","\n","    print(f\"âœ… ETL completed. {len(df)} rows processed\")\n","    return df\n","\n","# ========================================\n","# DATA QUALITY WITH BASIC CHECKS\n","# ========================================\n","\n","def run_data_quality_checks(df, dataset_name):\n","    print(f\"\\nğŸ” DATA QUALITY CHECKS - {dataset_name.upper()}\")\n","\n","    quality_checks = {\n","        'total_records': len(df),\n","        'null_percentage': df.isnull().sum().sum() / (len(df) * len(df.columns)),\n","        'duplicate_records': df.duplicated().sum(),\n","        'empty_strings': 0\n","    }\n","\n","    # Check for empty strings in text columns\n","    text_cols = df.select_dtypes(include=['object']).columns\n","    for col in text_cols:\n","        quality_checks['empty_strings'] += (df[col] == '').sum()\n","\n","    # Dataset-specific checks\n","    if 'jp_sentiment' in dataset_name:\n","        if 'sentence_cleaned' in df.columns:\n","            quality_checks['avg_sentence_length'] = df['sentence_cleaned'].str.len().mean()\n","            quality_checks['sentences_too_short'] = (df['sentence_cleaned'].str.len() < 5).sum()\n","        if 'emotion_intensity' in df.columns:\n","            quality_checks['avg_emotion_intensity'] = df['emotion_intensity'].mean()\n","\n","    elif 'review' in dataset_name:\n","        if 'rating' in df.columns:\n","            quality_checks['avg_rating'] = df['rating'].mean()\n","            quality_checks['ratings_out_of_range'] = ((df['rating'] < 0) | (df['rating'] > df['rating'].max())).sum()\n","        if 'review_length' in df.columns:\n","            quality_checks['avg_review_length'] = df['review_length'].mean()\n","\n","    # Calculate overall quality score\n","    quality_score = 1.0\n","    if quality_checks['null_percentage'] > 0.1:\n","        quality_score -= 0.2\n","    if quality_checks['duplicate_records'] > len(df) * 0.05:\n","        quality_score -= 0.1\n","    if quality_checks['empty_strings'] > len(df) * 0.05:\n","        quality_score -= 0.1\n","\n","    quality_score = max(0.5, quality_score)  # Minimum score of 0.5\n","    df['quality_score'] = quality_score\n","\n","    print(f\"âœ… Data quality validation completed\")\n","    print(f\"ğŸ“Š Total records: {quality_checks['total_records']}\")\n","    print(f\"ğŸ“Š Null percentage: {quality_checks['null_percentage']:.2%}\")\n","    print(f\"ğŸ“Š Duplicate records: {quality_checks['duplicate_records']}\")\n","    print(f\"ğŸ“ˆ Quality score: {quality_score:.3f}\")\n","\n","    return df, quality_checks\n","\n","# ========================================\n","# DATA WAREHOUSE STORAGE\n","# ========================================\n","\n","def store_in_data_warehouse(df, table_name):\n","    print(f\"\\nğŸª STORING IN DATA WAREHOUSE - {table_name}\")\n","    warehouse_table = f\"{project_id}.{dataset_id}.{table_name}{data_warehouse_suffix}\"\n","\n","    # Ensure all object columns are strings\n","    for col in df.columns:\n","        if df[col].dtype == \"object\":\n","            df[col] = df[col].astype(str)\n","\n","    print(f\"ğŸš€ Uploading to DATA WAREHOUSE: {warehouse_table}\")\n","    job_config = bigquery.LoadJobConfig(\n","        autodetect=True,\n","        write_disposition=\"WRITE_TRUNCATE\"\n","    )\n","    job = client.load_table_from_dataframe(df, warehouse_table, job_config=job_config)\n","    job.result()\n","    print(f\"âœ… Uploaded to DATA WAREHOUSE: {warehouse_table}\")\n","\n","# ========================================\n","# MAIN EXECUTION PIPELINE\n","# ========================================\n","\n","def main():\n","    print(\"ğŸš€ STARTING DATA LAKEHOUSE PIPELINE\")\n","    print(\"=\"*50)\n","\n","    try:\n","        print(\"\\nğŸ“¥ PHASE 1: DATA INGESTION TO DATA LAKE\")\n","        ingest_japanese_sentiment_to_lake()\n","\n","        # Try Amazon reviews first, fall back to movie reviews if needed\n","        try:\n","            ingest_amazon_reviews_to_lake()\n","        except:\n","            print(\"ğŸ”„ Amazon reviews failed, using alternative dataset...\")\n","            ingest_alternative_reviews_to_lake()\n","\n","        print(\"\\nğŸ”„ PHASE 2: ETL PROCESSING\")\n","        df_jp_sentiment = etl_japanese_sentiment()\n","        df_reviews = etl_reviews()\n","\n","        if df_reviews.empty:\n","            print(\"âš ï¸ No review data processed, skipping review pipeline\")\n","            review_quality_score = 0.0\n","        else:\n","            print(\"\\nğŸ” PHASE 3: DATA QUALITY CHECKS\")\n","            df_jp_sentiment, jp_validation = run_data_quality_checks(df_jp_sentiment, \"jp_sentiment\")\n","            df_reviews, review_validation = run_data_quality_checks(df_reviews, \"reviews\")\n","            review_quality_score = df_reviews['quality_score'].iloc[0] if not df_reviews.empty else 0.0\n","\n","        print(\"\\nğŸª PHASE 4: DATA WAREHOUSE STORAGE\")\n","        store_in_data_warehouse(df_jp_sentiment, \"jp_sentiment\")\n","\n","        if not df_reviews.empty:\n","            store_in_data_warehouse(df_reviews, \"reviews\")\n","\n","        print(\"\\nğŸ“Š PIPELINE SUMMARY\")\n","        print(\"=\"*50)\n","        print(f\"âœ… Data Lake Tables Created:\")\n","        print(f\"   - jp_sentiment{data_lake_suffix}\")\n","        if not df_reviews.empty:\n","            print(f\"   - reviews{data_lake_suffix}\")\n","\n","        print(f\"âœ… Data Warehouse Tables Created:\")\n","        print(f\"   - jp_sentiment{data_warehouse_suffix}\")\n","        if not df_reviews.empty:\n","            print(f\"   - reviews{data_warehouse_suffix}\")\n","\n","        print(f\"ğŸ” Quality Scores:\")\n","        print(f\"   - Japanese Sentiment: {df_jp_sentiment['quality_score'].iloc[0]:.3f}\")\n","        if not df_reviews.empty:\n","            print(f\"   - Reviews: {review_quality_score:.3f}\")\n","\n","        print(\"ğŸ‰ DATA LAKEHOUSE PIPELINE COMPLETED SUCCESSFULLY!\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Pipeline failed with error: {str(e)}\")\n","        raise\n","\n","# ========================================\n","# UTILITY FUNCTIONS\n","# ========================================\n","\n","def list_all_tables():\n","    print(f\"\\nğŸ“‹ ALL TABLES IN {dataset_id}:\")\n","    tables = client.list_tables(dataset_id)\n","    for table in tables:\n","        print(f\"  - {table.table_id}\")\n","\n","def query_data_warehouse(table_suffix=\"_cleaned_db\", limit=5):\n","    print(f\"\\nğŸ” QUERYING DATA WAREHOUSE TABLES\")\n","    tables = client.list_tables(dataset_id)\n","    warehouse_tables = [t for t in tables if t.table_id.endswith(table_suffix)]\n","\n","    for table in warehouse_tables:\n","        print(f\"\\nğŸ“Š Sample data from {table.table_id}:\")\n","        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table.table_id}` LIMIT {limit}\"\n","        try:\n","            df = client.query(query).to_dataframe()\n","            print(df.head())\n","        except Exception as e:\n","            print(f\"âŒ Error querying {table.table_id}: {str(e)}\")\n","\n","# ========================================\n","# EXECUTION\n","# ========================================\n","\n","if __name__ == \"__main__\":\n","    main()\n","    list_all_tables()\n","    query_data_warehouse()"]}]}