{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GUNhOvAA7gL9pEgjoZdrIJjjMQ__6kWc","timestamp":1752478787228}],"authorship_tag":"ABX9TyPqhOVhd2MpUDpPYNt2V8uz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1AYSOnr5WJd","executionInfo":{"status":"ok","timestamp":1752480456207,"user_tz":-120,"elapsed":43159,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"66174333-6cbc-45b7-a7cb-5d46b12d2282"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting en-core-web-lg==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.8.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!pip install spacy pysbd pandas tqdm requests nltk conllu --quiet\n","!python -m spacy download en_core_web_lg"]},{"cell_type":"code","source":["import requests\n","from conllu import parse\n","from io import StringIO\n","\n","# Download UD English EWT test set (CoNLL-U format)\n","url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/r2.12/en_ewt-ud-test.conllu\"\n","response = requests.get(url)\n","data = response.text\n","\n","# Parse sentences\n","sentences = parse(data)\n","print(f\"Loaded {len(sentences)} sentences.\")\n","\n","# Print a few sample sentences (as words)\n","for i, sent in enumerate(sentences[:5]):\n","    print(f\"{i+1}: {' '.join(token['form'] for token in sent)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNg3awMhC1Sh","executionInfo":{"status":"ok","timestamp":1752483835565,"user_tz":-120,"elapsed":754,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"ea8b79e2-fbd3-4774-d382-7698cd2b8cb1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 2077 sentences.\n","1: What if Google Morphed Into GoogleOS ?\n","2: What if Google expanded on its search - engine ( and now e-mail ) wares into a full - fledged operating system ?\n","3: [ via Microsoft Watch from Mary Jo Foley ]\n","4: ( And , by the way , is anybody else just a little nostalgic for the days when that was a good thing ? )\n","5: This BuzzMachine post argues that Google's Google 's rush toward ubiquity might backfire -- which we've we 've all heard before , but it's it 's particularly well - put in this post .\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import random\n","from tqdm import tqdm\n","import spacy\n","from typing import List, Dict, Set, Tuple, Optional\n","import logging\n","from collections import defaultdict\n","import requests\n","from urllib.parse import urlparse\n","import time\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","nltk.download('punkt')\n","\n","# Install and import PySBD\n","try:\n","    import pysbd\n","    PYSBD_AVAILABLE = True\n","    print(\"✓ PySBD is available\")\n","except ImportError:\n","    PYSBD_AVAILABLE = False\n","    print(\"✗ PySBD not found. Install with: pip install pysbd\")\n","\n","# Set up enhanced logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.StreamHandler(),\n","        logging.FileHandler('splitter_evaluation.log', mode='w', encoding='utf-8')\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","# ===============================\n","# 1. Enhanced Sentiment-based Switching Detection (English)\n","# ===============================\n","class SentimentSwitchDetector:\n","    \"\"\"\n","    Enhanced sentiment switching detector for English with quote handling\n","    \"\"\"\n","\n","    def __init__(self):\n","        # Extended explicit contrasting switchers\n","        self.explicit_switchers = {\n","            'but': 'contrasting',\n","            'however': 'contrasting',\n","            'although': 'contrasting',\n","            'though': 'contrasting',\n","            'nevertheless': 'contrasting',\n","            'nonetheless': 'contrasting',\n","            'yet': 'contrasting',\n","            'despite': 'contrasting',\n","            'in contrast': 'contrasting',\n","            'on the other hand': 'contrasting',\n","            'whereas': 'contrasting',\n","            'while': 'contrasting',\n","            'even though': 'contrasting',\n","            'regardless': 'contrasting',\n","            'conversely': 'contrasting',\n","            'instead': 'contrasting',\n","            'rather': 'contrasting',\n","            'still': 'contrasting',\n","            'except': 'contrasting',\n","            'otherwise': 'contrasting'\n","        }\n","\n","        # Enhanced positive sentiment indicators\n","        self.positive_indicators = {\n","            'good', 'great', 'excellent', 'wonderful', 'fantastic', 'awesome', 'amazing', 'perfect',\n","            'love', 'like', 'best', 'superb', 'outstanding', 'fabulous', 'terrific', 'brilliant',\n","            'recommend', 'satisfied', 'happy', 'pleased', 'delighted', 'impressed', 'enjoy',\n","            'smooth', 'easy', 'fast', 'quick', 'reliable', 'affordable', 'valuable', 'worth',\n","            'beautiful', 'comfortable', 'convenient', 'efficient', 'effective', 'powerful',\n","            'improved', 'better', 'superior', 'exceptional', 'flawless', 'ideal', 'incredible'\n","        }\n","\n","        # Enhanced negative sentiment indicators\n","        self.negative_indicators = {\n","            'bad', 'poor', 'terrible', 'awful', 'horrible', 'worst', 'disappointing', 'disappointed',\n","            'hate', 'dislike', 'problem', 'issue', 'fault', 'defect', 'broken', 'damaged',\n","            'slow', 'difficult', 'hard', 'complex', 'confusing', 'frustrating', 'annoying',\n","            'expensive', 'overpriced', 'waste', 'useless', 'pointless', 'worthless', 'unreliable',\n","            'inferior', 'mediocre', 'average', 'ordinary', 'subpar', 'lacking', 'missing',\n","            'noisy', 'loud', 'uncomfortable', 'painful', 'dangerous', 'risky', 'flawed'\n","        }\n","\n","        # Enhanced implicit switching patterns\n","        self.implicit_patterns = [\n","            r'\\.\\s*[Bb]ut\\s',          # But after period\n","            r'\\.\\s*[Hh]owever\\s',      # However after period\n","            r'\\.\\s*[Tt]hat said\\s',    # That said after period\n","            r'\\.\\s*[Oo]n the other hand\\s',\n","            r'\\.\\s*[Uu]nfortunately\\s',\n","            r'\\.\\s*[Dd]isappointingly\\s',\n","            r'\\.\\s*[Rr]egrettably\\s',\n","            r'\\.\\s*[Ww]hereas\\s',\n","            r'\\.\\s*[Yy]et\\s',\n","            r'\\.\\s*[Nn]evertheless\\s',\n","            r'\\.\\s*[Oo]therwise\\s',\n","            r'\\.\\s*[Cc]onversely\\s',\n","            r'\\.\\s*[Ii]n contrast\\s',\n","            r'\\.\\s*[Aa]lthough\\s',\n","            r'\\.\\s*[Dd]espite\\s',\n","            r'\\.\\s*[Ee]ven though\\s'\n","        ]\n","\n","        # Context-aware sentiment patterns\n","        self.context_patterns = {\n","            'disappointment': [\n","                r'expected.*?but.*?disappointed',\n","                r'hoped.*?but.*?let down',\n","                r'looked forward.*?but.*?disappointing'\n","            ],\n","            'mixed_feelings': [\n","                r'good.*?but.*?bad',\n","                r'great.*?except.*?issue',\n","                r'love.*?but.*?hate'\n","            ],\n","            'conditional_positive': [\n","                r'not bad.*?but.*?great',\n","                r'okay.*?but.*?excellent',\n","                r'acceptable.*?but.*?wonderful'\n","            ]\n","        }\n","\n","    def detect_sentiment(self, text: str) -> str:\n","        \"\"\"\n","        Enhanced sentiment detection for English\n","        \"\"\"\n","        text_lower = text.lower()\n","\n","        # Count positive and negative indicators\n","        positive_count = sum(1 for indicator in self.positive_indicators\n","                           if re.search(r'\\b' + re.escape(indicator) + r'\\b', text_lower))\n","        negative_count = sum(1 for indicator in self.negative_indicators\n","                           if re.search(r'\\b' + re.escape(indicator) + r'\\b', text_lower))\n","\n","        # Check for context patterns and adjust counts\n","        context_bonus = 0\n","        if any(re.search(pattern, text_lower) for pattern in self.context_patterns['disappointment']):\n","            negative_count += 1\n","        if any(re.search(pattern, text_lower) for pattern in self.context_patterns['mixed_feelings']):\n","            context_bonus = 0.5\n","        if any(re.search(pattern, text_lower) for pattern in self.context_patterns['conditional_positive']):\n","            positive_count += 0.5\n","\n","        # Determine overall sentiment\n","        if positive_count > negative_count + context_bonus:\n","            return 'positive'\n","        elif negative_count > positive_count + context_bonus:\n","            return 'negative'\n","        else:\n","            return 'neutral'\n","\n","    def has_explicit_switcher(self, text: str) -> bool:\n","        \"\"\"\n","        Checks for explicit switchers\n","        \"\"\"\n","        for switcher in self.explicit_switchers.keys():\n","            if re.search(r'\\b' + re.escape(switcher) + r'\\b', text, re.IGNORECASE):\n","                return True\n","        return False\n","\n","    def has_implicit_switcher(self, text: str) -> bool:\n","        \"\"\"\n","        Checks for implicit switching patterns\n","        \"\"\"\n","        for pattern in self.implicit_patterns:\n","            if re.search(pattern, text):\n","                return True\n","        return False\n","\n","    def should_split_by_sentiment(self, text: str) -> bool:\n","        \"\"\"\n","        Determines if a sentence should be split based on sentiment switching\n","        \"\"\"\n","        # Handle quotes as special case\n","        if '\"' in text or \"'\" in text:\n","            return True\n","\n","        if self.has_explicit_switcher(text):\n","            logger.debug(f\"Explicit switcher found in: {text[:50]}...\")\n","            return True\n","\n","        if self.has_implicit_switcher(text):\n","            logger.debug(f\"Implicit switcher found in: {text[:50]}...\")\n","            return True\n","\n","        # Split by punctuation for sentiment analysis\n","        punctuation_splits = re.split(r'([.!?]+)', text)\n","        sentences_for_sentiment = []\n","        for i in range(0, len(punctuation_splits), 2):\n","            if i + 1 < len(punctuation_splits):\n","                sentence = punctuation_splits[i] + punctuation_splits[i+1]\n","                if sentence.strip():\n","                    sentences_for_sentiment.append(sentence.strip())\n","            else:\n","                if punctuation_splits[i].strip():\n","                    sentences_for_sentiment.append(punctuation_splits[i].strip())\n","\n","        if len(sentences_for_sentiment) < 2:\n","            return False\n","\n","        sentiments = [self.detect_sentiment(segment) for segment in sentences_for_sentiment]\n","        has_positive = 'positive' in sentiments\n","        has_negative = 'negative' in sentiments\n","\n","        result = has_positive and has_negative\n","        if result:\n","            logger.debug(f\"Sentiment switching detected: {sentiments} in {text[:50]}...\")\n","\n","        return result\n","\n","# Initialize enhanced sentiment detector\n","sentiment_detector = SentimentSwitchDetector()\n","\n","# ===============================\n","# 2. Enhanced Data Loading with Custom Dataset\n","# ===============================\n","def load_custom_dataset() -> List[str]:\n","    \"\"\"\n","    Load custom English dataset from the specified URL\n","    \"\"\"\n","    logger.info(\"=\" * 60)\n","    logger.info(\"LOADING CUSTOM ENGLISH DATASET\")\n","    logger.info(\"=\" * 60)\n","\n","    try:\n","        # Load the custom dataset\n","        logger.info(\"Downloading custom dataset from GitHub...\")\n","        url = \"https://raw.githubusercontent.com/thunlp/QuoteR/main/data/english.txt\"\n","        response = requests.get(url)\n","        response.raise_for_status()\n","\n","        # Parse the content\n","        content = response.text\n","        logger.info(f\"Successfully downloaded {len(content)} characters\")\n","\n","        # Split into sentences/lines\n","        lines = content.strip().split('\\n')\n","        sentences = [line.strip() for line in lines if line.strip()]\n","        logger.info(f\"Extracted {len(sentences)} sentences from custom dataset\")\n","\n","        # Filter for quality and length\n","        quality_sentences = []\n","        for sentence in sentences:\n","            # Basic quality filters\n","            if (len(sentence) >= 20 and  # Minimum length\n","                len(sentence) <= 500 and  # Maximum length\n","                sentence.count('.') <= 5 and  # Not too many periods\n","                not sentence.startswith(('#', '//', '/*')) and  # Not comments\n","                re.search(r'[a-zA-Z]', sentence)):  # Contains letters\n","                quality_sentences.append(sentence)\n","\n","        logger.info(f\"Filtered to {len(quality_sentences)} quality sentences\")\n","\n","        # Prioritize sentences with quotes and contrasting patterns\n","        quote_sentences = [s for s in quality_sentences if '\"' in s or \"'\" in s]\n","        contrast_sentences = [s for s in quality_sentences if sentiment_detector.has_explicit_switcher(s)]\n","        complex_sentences = [s for s in quality_sentences if len(s) > 100]\n","\n","        logger.info(f\"Found {len(quote_sentences)} sentences with quotes\")\n","        logger.info(f\"Found {len(contrast_sentences)} sentences with contrasting words\")\n","        logger.info(f\"Found {len(complex_sentences)} complex sentences\")\n","\n","        # Create a balanced dataset\n","        final_sentences = []\n","\n","        # Add all quote sentences\n","        final_sentences.extend(quote_sentences)\n","\n","        # Add contrast sentences not already included\n","        for s in contrast_sentences:\n","            if s not in final_sentences:\n","                final_sentences.append(s)\n","\n","        # Add complex sentences not already included\n","        for s in complex_sentences:\n","            if s not in final_sentences and len(final_sentences) < 8000:\n","                final_sentences.append(s)\n","\n","        # Fill remaining with random quality sentences\n","        remaining = [s for s in quality_sentences if s not in final_sentences]\n","        sample_size = min(10000 - len(final_sentences), len(remaining))\n","        if sample_size > 0:\n","            final_sentences.extend(random.sample(remaining, sample_size))\n","\n","        logger.info(f\"Final dataset size: {len(final_sentences)} sentences\")\n","\n","        # Show examples\n","        logger.info(\"\\nSample sentences from custom dataset:\")\n","        for i, s in enumerate(final_sentences[:5]):\n","            logger.info(f\"{i+1}. {s}\")\n","\n","        return final_sentences\n","\n","    except Exception as e:\n","        logger.error(f\"Error loading custom dataset: {e}\")\n","        logger.info(\"Using fallback English sentences\")\n","        return create_fallback_english_data()\n","\n","def create_fallback_english_data() -> List[str]:\n","    \"\"\"\n","    Create fallback English sentences with quotes and contrasting patterns\n","    \"\"\"\n","    logger.info(\"Creating fallback English sentences...\")\n","\n","    sentences = [\n","        \"The manager said, 'We need to improve our service,' but customers seem happy overall.\",\n","        \"She claimed, \\\"This is the best product ever,\\\" however I found several flaws.\",\n","        \"While the book was interesting, the ending felt rushed.\",\n","        \"He shouted, 'I won't accept this!' and stormed out of the meeting.\",\n","        \"The instructions said, 'Turn left at the intersection,' but the sign indicated right.\",\n","        \"The review stated, 'The food was delicious,' although the service was slow.\",\n","        \"They announced, 'We're launching a new product next week,' which surprised everyone.\",\n","        \"I thought, 'This will be easy,' but it turned out to be quite challenging.\",\n","        \"The sign read, 'Do not enter,' yet people kept walking through the door.\",\n","        \"She whispered, 'I know the secret,' and then disappeared into the crowd.\",\n","        \"The contract states, 'Payment is due upon delivery,' however we allow a 30-day grace period.\",\n","        \"He declared, 'This changes everything!' but in reality, nothing changed at all.\",\n","        \"The warning said, 'High voltage—do not touch,' nevertheless someone tried to grab it.\",\n","        \"I remember thinking, 'This is too good to be true,' and unfortunately, I was right.\",\n","        \"The label claims, 'All natural ingredients,' whereas the ingredients list shows several chemicals.\",\n","        \"My father always said, 'Honesty is the best policy,' which I've found to be absolutely true.\",\n","        \"The email stated, 'Your account has been suspended,' causing immediate panic among users.\",\n","        \"She commented, 'The design is beautiful,' but the functionality really needs improvement.\",\n","        \"The forecast predicted, 'Sunny skies all day,' yet it started raining heavily by noon.\",\n","        \"He argued, 'We should invest more in marketing,' while others wanted to cut costs significantly.\",\n","        \"The movie was entertaining and well-acted. However, the plot had several logical inconsistencies.\",\n","        \"I love the convenience of online shopping. On the other hand, I miss the personal touch of local stores.\",\n","        \"The restaurant has excellent food and great atmosphere. Nevertheless, the service can be quite slow.\",\n","        \"The software is powerful and feature-rich. Unfortunately, it has a steep learning curve.\",\n","        \"She's incredibly talented and hardworking. Despite this, she often doubts her abilities.\",\n","        \"The weather was perfect for hiking. Even though we were well-prepared, we encountered unexpected challenges.\",\n","        \"The hotel room was spacious and comfortable. Conversely, the bathroom was cramped and outdated.\",\n","        \"I expected the concert to be amazing. Regrettably, the sound quality was poor throughout.\",\n","        \"The book starts with an engaging premise. Disappointingly, it fails to deliver on its initial promise.\",\n","        \"The product works exactly as advertised. That said, the price point is quite high for most consumers.\"\n","    ]\n","\n","    # Extend with variations and longer examples\n","    extended = []\n","    for s in sentences:\n","        extended.append(s)\n","        # Create variations with different structures\n","        if ' but ' in s:\n","            extended.append(s.replace(' but ', ' however '))\n","        if ' however ' in s:\n","            extended.append(s.replace(' however ', ' yet '))\n","\n","        # Create compound sentences\n","        extended.append(\"First, \" + s)\n","        extended.append(s + \" This is important to remember.\")\n","\n","        # Create sentences with multiple clauses\n","        if len(s.split('.')) == 2:  # Two sentences\n","            parts = s.split('.')\n","            if len(parts[0]) > 20 and len(parts[1]) > 20:\n","                extended.append(parts[0] + ', although ' + parts[1].strip().lower())\n","\n","    # Add more challenging examples\n","    challenging_examples = [\n","        \"The CEO announced, 'We're expanding globally,' but employees worry about job security in the current market.\",\n","        \"She exclaimed, 'This is revolutionary!' though critics argue it's merely an incremental improvement.\",\n","        \"The study concluded, 'Exercise improves mental health,' while acknowledging that more research is needed.\",\n","        \"He insisted, 'The project will be completed on time,' despite numerous delays and setbacks.\",\n","        \"The advertisement promises, 'Instant results guaranteed,' whereas actual results may take weeks to appear.\",\n","        \"They claimed, 'Our product is environmentally friendly,' but the manufacturing process raises concerns.\",\n","        \"The teacher said, 'Everyone did well on the exam,' although several students struggled significantly.\",\n","        \"She declared, 'I'm confident in this decision,' yet her body language suggested otherwise.\",\n","        \"The manual states, 'Assembly takes 30 minutes,' however most users report it takes much longer.\",\n","        \"He announced, 'We're ahead of schedule,' while the team knew they were actually falling behind.\"\n","    ]\n","\n","    extended.extend(challenging_examples)\n","\n","    logger.info(f\"Created {len(extended)} fallback English sentences\")\n","    return extended\n","\n","# ===============================\n","# 3. Enhanced Ground Truth Creation with Improved Accuracy\n","# ===============================\n","def create_accurate_ground_truth(sentences: List[str], sample_size: int = 1000) -> Dict[str, List[str]]:\n","    \"\"\"\n","    Create highly accurate ground truth splits for English sentences\n","    \"\"\"\n","    logger.info(\"=\" * 60)\n","    logger.info(\"CREATING ACCURATE GROUND TRUTH\")\n","    logger.info(\"=\" * 60)\n","\n","    actual_sample_size = min(sample_size, len(sentences))\n","    logger.info(f\"Creating ground truth for {actual_sample_size} sentences...\")\n","\n","    sample_sentences = random.sample(sentences, actual_sample_size)\n","    ground_truth = {}\n","\n","    # Statistics tracking\n","    stats = {\n","        'quote_splits': 0,\n","        'explicit_switcher_splits': 0,\n","        'implicit_switcher_splits': 0,\n","        'punctuation_splits': 0,\n","        'clause_boundary_splits': 0,\n","        'sentiment_based_splits': 0,\n","        'no_splits': 0\n","    }\n","\n","    for i, sentence in enumerate(tqdm(sample_sentences, desc=\"Creating accurate ground truth\")):\n","        splits = []\n","        split_method = \"no_split\"\n","\n","        # Clean the sentence\n","        sentence = sentence.strip()\n","        if not sentence:\n","            continue\n","\n","        # Strategy 1: Quote-based splitting (highest priority)\n","        if '\"' in sentence or \"'\" in sentence:\n","            splits = split_by_quotes(sentence)\n","            if len(splits) > 1:\n","                split_method = \"quote_split\"\n","                stats['quote_splits'] += 1\n","\n","        # Strategy 2: Explicit switcher splitting\n","        if not splits and sentiment_detector.has_explicit_switcher(sentence):\n","            splits = split_by_explicit_switcher(sentence)\n","            if len(splits) > 1:\n","                split_method = \"explicit_switcher\"\n","                stats['explicit_switcher_splits'] += 1\n","\n","        # Strategy 3: Implicit switcher splitting\n","        if not splits and sentiment_detector.has_implicit_switcher(sentence):\n","            splits = split_by_implicit_switcher(sentence)\n","            if len(splits) > 1:\n","                split_method = \"implicit_switcher\"\n","                stats['implicit_switcher_splits'] += 1\n","\n","        # Strategy 4: Punctuation-based splitting\n","        if not splits and contains_multiple_sentences(sentence):\n","            splits = smart_punctuation_split(sentence)\n","            if len(splits) > 1:\n","                split_method = \"punctuation\"\n","                stats['punctuation_splits'] += 1\n","\n","        # Strategy 5: Clause boundary splitting for complex sentences\n","        if not splits and len(sentence) > 80:\n","            splits = split_by_clause_boundary(sentence)\n","            if len(splits) > 1:\n","                split_method = \"clause_boundary\"\n","                stats['clause_boundary_splits'] += 1\n","\n","        # Strategy 6: Sentiment-based splitting\n","        if not splits and sentiment_detector.should_split_by_sentiment(sentence):\n","            splits = split_by_sentiment_change(sentence)\n","            if len(splits) > 1:\n","                split_method = \"sentiment_based\"\n","                stats['sentiment_based_splits'] += 1\n","\n","        # Fallback: keep original if no good splits found\n","        if not splits or len(splits) == 0:\n","            splits = [sentence]\n","            stats['no_splits'] += 1\n","\n","        # Clean and validate splits\n","        splits = [s.strip() for s in splits if s.strip()]\n","        if not splits:\n","            splits = [sentence]\n","\n","        ground_truth[sentence] = splits\n","\n","        # Log examples\n","        if i < 5:\n","            logger.info(f\"\\nExample {i+1} (Ground Truth):\")\n","            logger.info(f\"  Original: {sentence}\")\n","            logger.info(f\"  Splits ({len(splits)}): {splits}\")\n","            logger.info(f\"  Method: {split_method}\")\n","\n","    # Log comprehensive statistics\n","    logger.info(f\"\\nGround Truth Statistics:\")\n","    logger.info(f\"  Total sentences: {len(ground_truth)}\")\n","    for method, count in stats.items():\n","        percentage = (count / len(ground_truth)) * 100\n","        logger.info(f\"  {method}: {count} ({percentage:.1f}%)\")\n","\n","    # Distribution of split counts\n","    split_counts = defaultdict(int)\n","    for splits in ground_truth.values():\n","        split_counts[len(splits)] += 1\n","\n","    logger.info(f\"\\nSplit count distribution:\")\n","    for count, freq in sorted(split_counts.items()):\n","        percentage = (freq / len(ground_truth)) * 100\n","        logger.info(f\"  {count} splits: {freq} sentences ({percentage:.1f}%)\")\n","\n","    logger.info(\"=\" * 60)\n","    logger.info(\"ACCURATE GROUND TRUTH CREATION COMPLETED\")\n","    logger.info(\"=\" * 60)\n","\n","    return ground_truth\n","\n","def split_by_quotes(text: str) -> List[str]:\n","    \"\"\"\n","    Split text by quotes while preserving quote boundaries\n","    \"\"\"\n","    # Handle both single and double quotes\n","    quote_pattern = r'([\\'\"].*?[\\'\"])'\n","    parts = re.split(quote_pattern, text)\n","\n","    result = []\n","    current = \"\"\n","\n","    for part in parts:\n","        part = part.strip()\n","        if not part:\n","            continue\n","\n","        if part.startswith(('\"', \"'\")):\n","            # This is a quoted part\n","            if current:\n","                result.append(current.strip())\n","                current = \"\"\n","            result.append(part)\n","        else:\n","            # This is non-quoted text\n","            if current and part:\n","                current += \" \" + part\n","            elif part:\n","                current = part\n","\n","    if current:\n","        result.append(current.strip())\n","\n","    return [s for s in result if s]\n","\n","def split_by_explicit_switcher(text: str) -> List[str]:\n","    \"\"\"\n","    Split text by explicit contrasting switchers\n","    \"\"\"\n","    for switcher in sentiment_detector.explicit_switchers.keys():\n","        pattern = r'\\b' + re.escape(switcher) + r'\\b'\n","        if re.search(pattern, text, re.IGNORECASE):\n","            # Find the position of the switcher\n","            match = re.search(pattern, text, re.IGNORECASE)\n","            if match:\n","                pos = match.start()\n","                # Split at the switcher, keeping it with the second part\n","                part1 = text[:pos].strip()\n","                part2 = text[pos:].strip()\n","\n","                if part1 and part2:\n","                    return [part1, part2]\n","\n","    return [text]\n","\n","def split_by_implicit_switcher(text: str) -> List[str]:\n","    \"\"\"\n","    Split text by implicit switching patterns\n","    \"\"\"\n","    for pattern in sentiment_detector.implicit_patterns:\n","        if re.search(pattern, text):\n","            # Split at the pattern\n","            parts = re.split(pattern, text, 1)\n","            if len(parts) >= 2:\n","                # Find the actual switcher word\n","                match = re.search(pattern, text)\n","                if match:\n","                    switcher = match.group(0)\n","                    part1 = parts[0].strip()\n","                    part2 = (switcher + parts[1]).strip() if len(parts) > 1 else \"\"\n","\n","                    if part1 and part2:\n","                        return [part1, part2]\n","\n","    return [text]\n","\n","def contains_multiple_sentences(text: str) -> bool:\n","    \"\"\"\n","    Check if text contains multiple sentences\n","    \"\"\"\n","    # Count sentence-ending punctuation\n","    sentence_endings = re.findall(r'[.!?]+', text)\n","\n","    # Must have at least 2 sentence endings, or 1 ending not at the very end\n","    if len(sentence_endings) > 1:\n","        return True\n","    elif len(sentence_endings) == 1:\n","        # Check if the ending is not at the very end\n","        last_punct_pos = text.rfind(sentence_endings[0])\n","        return last_punct_pos < len(text) - len(sentence_endings[0])\n","\n","    return False\n","\n","def smart_punctuation_split(text: str) -> List[str]:\n","    \"\"\"\n","    Intelligently split text by punctuation\n","    \"\"\"\n","    # Split by sentence-ending punctuation\n","    sentences = re.split(r'([.!?]+)', text)\n","\n","    result = []\n","    for i in range(0, len(sentences), 2):\n","        if i + 1 < len(sentences):\n","            sentence = sentences[i] + sentences[i + 1]\n","            if sentence.strip():\n","                result.append(sentence.strip())\n","        else:\n","            if sentences[i].strip():\n","                result.append(sentences[i].strip())\n","\n","    return [s for s in result if s]\n","\n","def split_by_clause_boundary(text: str) -> List[str]:\n","    \"\"\"\n","    Split text by clause boundaries\n","    \"\"\"\n","    # Look for clause boundary patterns\n","    clause_patterns = [\n","        r'(.*?[,;:])\\s+(?:but|however|although|though|yet|while|whereas|nevertheless|nonetheless|despite|in contrast|on the other hand|even though|regardless|conversely|instead|rather|still|except|otherwise)\\b',\n","        r'(.*?)\\s+(?:but|however|although|though|yet|while|whereas|nevertheless|nonetheless|despite|in contrast|on the other hand|even though|regardless|conversely|instead|rather|still|except|otherwise)\\b\\s+(.*)',\n","    ]\n","\n","    for pattern in clause_patterns:\n","        matches = re.findall(pattern, text, re.IGNORECASE)\n","        if matches:\n","            if isinstance(matches[0], tuple):\n","                # Multiple capturing groups\n","                parts = [part.strip() for part in matches[0] if part.strip()]\n","                if len(parts) >= 2:\n","                    return parts[:2]  # Return first two parts\n","            else:\n","                # Single capturing group\n","                match_text = matches[0].strip()\n","                remaining = text[text.find(match_text) + len(match_text):].strip()\n","                if remaining:\n","                    return [match_text, remaining]\n","\n","    return [text]\n","\n","def split_by_sentiment_change(text: str) -> List[str]:\n","    \"\"\"\n","    Split text by sentiment changes\n","    \"\"\"\n","    # First try punctuation-based splitting\n","    punct_splits = smart_punctuation_split(text)\n","    if len(punct_splits) > 1:\n","        return punct_splits\n","\n","    # If no punctuation splits, try clause boundaries\n","    return split_by_clause_boundary(text)\n","\n","# ===============================\n","# 4. Enhanced Splitter Functions\n","# ===============================\n","def load_spacy_model() -> Optional[spacy.Language]:\n","    \"\"\"\n","    Loads spaCy English model\n","    \"\"\"\n","    try:\n","        logger.info(\"Loading spaCy model 'en_core_web_lg'...\")\n","        nlp = spacy.load(\"en_core_web_lg\")\n","        logger.info(\"✓ spaCy en_core_web_lg model loaded successfully\")\n","        return nlp\n","    except IOError:\n","        logger.warning(\"✗ spaCy en_core_web_lg model not found!\")\n","        try:\n","            logger.info(\"Attempting to load fallback spaCy model 'en_core_web_sm'...\")\n","            nlp = spacy.load(\"en_core_web_sm\")\n","            logger.info(\"✓ spaCy en_core_web_sm model loaded as fallback\")\n","            return nlp\n","        except IOError:\n","            logger.warning(\"✗ No spaCy English model found!\")\n","            return None\n","\n","# Initialize spaCy model\n","nlp_spacy = load_spacy_model()\n","\n","def sentiment_based_split(text: str) -> List[str]:\n","    \"\"\"\n","    Enhanced sentiment-based splitting with quote handling\n","    \"\"\"\n","    return split_by_quotes(text) if ('\"' in text or \"'\" in text) else (\n","        split_by_explicit_switcher(text) if sentiment_detector.has_explicit_switcher(text) else\n","        split_by_implicit_switcher(text) if sentiment_detector.has_implicit_switcher(text) else\n","        smart_punctuation_split(text) if contains_multiple_sentences(text) else [text]\n","    )\n","\n","def spacy_split(text: str) -> List[str]:\n","    \"\"\"\n","    Enhanced spaCy-based splitting\n","    \"\"\"\n","    if nlp_spacy is None:\n","        return [text]\n","    try:\n","        doc = nlp_spacy(text)\n","        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n","        return sentences if sentences else [text]\n","    except Exception as e:\n","        logger.debug(f\"spaCy error: {e}\")\n","        return [text]\n","\n","def nltk_split(text: str) -> List[str]:\n","    \"\"\"\n","    Enhanced NLTK-based splitting\n","    \"\"\"\n","    try:\n","        sentences = sent_tokenize(text)\n","        return [s.strip() for s in sentences if s.strip()]\n","    except Exception as e:\n","        logger.debug(f\"NLTK error: {e}\")\n","        return [text]\n","\n","def pysbd_split(text: str) -> List[str]:\n","    \"\"\"\n","    Enhanced PySBD splitting\n","    \"\"\"\n","    if not PYSBD_AVAILABLE:\n","        return smart_punctuation_split(text)\n","    try:\n","        seg = pysbd.Segmenter(language=\"en\", clean=False)\n","        sentences = seg.segment(text)\n","        return [s.strip() for s in sentences if s.strip()]\n","    except Exception as e:\n","        logger.debug(f\"PySBD error: {e}\")\n","        return [text]\n","\n","def advanced_regex_split(text: str) -> List[str]:\n","    \"\"\"\n","    Advanced regex-based splitting\n","    \"\"\"\n","    # Priority order: quotes, explicit switchers, punctuation, clause boundaries\n","    if '\"' in text or \"'\" in text:\n","        return split_by_quotes(text)\n","    elif sentiment_detector.has_explicit_switcher(text):\n","        return split_by_explicit_switcher(text)\n","    elif contains_multiple_sentences(text):\n","        return smart_punctuation_split(text)\n","    else:\n","        return split_by_clause_boundary(text)\n","\n","def ensemble_split(text: str) -> List[str]:\n","    \"\"\"\n","    Ensemble splitter: majority or best of several splitters\n","    \"\"\"\n","    results = [\n","        sentiment_based_split(text),\n","        spacy_split(text),\n","        nltk_split(text),\n","        pysbd_split(text) if PYSBD_AVAILABLE else smart_punctuation_split(text),\n","        advanced_regex_split(text),\n","        smart_punctuation_split(text),\n","    ]\n","    # Choose the split with most splits (but not just 1-word splits)\n","    results_multi = [r for r in results if len(r) > 1 and all(len(s) > 2 for s in r)]\n","    if results_multi:\n","        # Return the split with number of segments closest to 2 (most common in quote splitting)\n","        return min(results_multi, key=lambda x: abs(len(x) - 2))\n","    else:\n","        return [text]\n","\n","splitters = {\n","    \"spaCy\": spacy_split,\n","    \"NLTK\": nltk_split,\n","    \"PySBD\": pysbd_split if PYSBD_AVAILABLE else smart_punctuation_split,\n","    \"Sentiment-based\": sentiment_based_split,\n","    \"Advanced Regex\": advanced_regex_split,\n","    \"Ensemble\": ensemble_split,\n","}\n","\n","# ===============================\n","# 5. Evaluation Metrics & Loop\n","# ===============================\n","def calculate_split_similarity(predicted: List[str], true: List[str]) -> Dict[str, float]:\n","    # Exact match\n","    exact_match = predicted == true\n","    # Count similarity\n","    count_diff = abs(len(predicted) - len(true))\n","    count_similarity = 1.0 / (1.0 + count_diff)\n","    # Content match\n","    predicted_content = ''.join(predicted)\n","    true_content = ''.join(true)\n","    content_match = predicted_content == true_content\n","    # Boundary positions\n","    predicted_boundaries = set()\n","    true_boundaries = set()\n","    pos = 0\n","    for split in predicted[:-1]:\n","        pos += len(split)\n","        predicted_boundaries.add(pos)\n","    pos = 0\n","    for split in true[:-1]:\n","        pos += len(split)\n","        true_boundaries.add(pos)\n","    # Boundary metrics\n","    if len(true_boundaries) == 0 and len(predicted_boundaries) == 0:\n","        boundary_precision = boundary_recall = boundary_f1 = 1.0\n","    elif len(predicted_boundaries) == 0:\n","        boundary_precision = boundary_recall = boundary_f1 = 0.0\n","    elif len(true_boundaries) == 0:\n","        boundary_precision = 0.0\n","        boundary_recall = 1.0\n","        boundary_f1 = 0.0\n","    else:\n","        common = len(predicted_boundaries & true_boundaries)\n","        boundary_precision = common / len(predicted_boundaries)\n","        boundary_recall = common / len(true_boundaries)\n","        boundary_f1 = (2 * boundary_precision * boundary_recall) / (boundary_precision + boundary_recall) if (boundary_precision + boundary_recall) > 0 else 0.0\n","    return {\n","        'exact_match': exact_match,\n","        'count_similarity': count_similarity,\n","        'content_match': content_match,\n","        'boundary_precision': boundary_precision,\n","        'boundary_recall': boundary_recall,\n","        'boundary_f1': boundary_f1\n","    }\n","\n","def evaluate_splitter_accuracy(splitter_func, ground_truth: Dict[str, List[str]], name: str = \"splitter\") -> Dict:\n","    results = []\n","    total_exact_matches = 0\n","    total_sentences = 0\n","    boundary_precisions = []\n","    boundary_recalls = []\n","    boundary_f1s = []\n","    count_similarities = []\n","    for sentence, true_splits in tqdm(ground_truth.items(), desc=f\"Evaluating {name}\"):\n","        try:\n","            predicted_splits = splitter_func(sentence)\n","            if not predicted_splits or all(not s.strip() for s in predicted_splits):\n","                predicted_splits = [sentence]\n","            similarities = calculate_split_similarity(predicted_splits, true_splits)\n","            if similarities['exact_match']:\n","                total_exact_matches += 1\n","            boundary_precisions.append(similarities['boundary_precision'])\n","            boundary_recalls.append(similarities['boundary_recall'])\n","            boundary_f1s.append(similarities['boundary_f1'])\n","            count_similarities.append(similarities['count_similarity'])\n","            results.append({\n","                'sentence': sentence,\n","                'true_splits': true_splits,\n","                'predicted_splits': predicted_splits,\n","                'similarities': similarities\n","            })\n","            total_sentences += 1\n","        except Exception as e:\n","            logger.error(f\"Error processing sentence with {name}: {e}\")\n","            continue\n","    exact_match_ratio = total_exact_matches / max(total_sentences, 1)\n","    avg_boundary_precision = sum(boundary_precisions) / max(len(boundary_precisions), 1)\n","    avg_boundary_recall = sum(boundary_recalls) / max(len(boundary_recalls), 1)\n","    avg_boundary_f1 = sum(boundary_f1s) / max(len(boundary_f1s), 1)\n","    avg_count_similarity = sum(count_similarities) / max(len(count_similarities), 1)\n","    return {\n","        'name': name,\n","        'exact_match_ratio': exact_match_ratio,\n","        'avg_boundary_precision': avg_boundary_precision,\n","        'avg_boundary_recall': avg_boundary_recall,\n","        'avg_boundary_f1': avg_boundary_f1,\n","        'avg_count_similarity': avg_count_similarity,\n","        'total_sentences': total_sentences,\n","        'detailed_results': results\n","    }\n","\n","def display_results(results: List[Dict]):\n","    if not results:\n","        logger.warning(\"No results to display.\")\n","        return\n","    comparison_data = []\n","    for result in results:\n","        comparison_data.append({\n","            'Splitter': result['name'],\n","            'Exact Match': f\"{result['exact_match_ratio']:.3f}\",\n","            'Boundary F1': f\"{result['avg_boundary_f1']:.3f}\",\n","            'Boundary Precision': f\"{result['avg_boundary_precision']:.3f}\",\n","            'Boundary Recall': f\"{result['avg_boundary_recall']:.3f}\",\n","            'Count Similarity': f\"{result['avg_count_similarity']:.3f}\",\n","            'Total Sentences': result['total_sentences']\n","        })\n","    comparison_df = pd.DataFrame(comparison_data)\n","    print(\"\\n\" + \"=\"*90)\n","    print(\"SPLITTER ACCURACY COMPARISON\")\n","    print(\"=\"*90)\n","    print(comparison_df.to_string(index=False))\n","    print(\"\\n\" + \"=\"*90)\n","    print(\"BEST PERFORMERS\")\n","    print(\"=\"*90)\n","    best_exact = max(results, key=lambda x: x['exact_match_ratio'])\n","    best_boundary_f1 = max(results, key=lambda x: x['avg_boundary_f1'])\n","    best_count_sim = max(results, key=lambda x: x['avg_count_similarity'])\n","    print(f\"Best Exact Match: {best_exact['name']} ({best_exact['exact_match_ratio']:.3f})\")\n","    print(f\"Best Boundary F1: {best_boundary_f1['name']} ({best_boundary_f1['avg_boundary_f1']:.3f})\")\n","    print(f\"Best Count Similarity: {best_count_sim['name']} ({best_count_sim['avg_count_similarity']:.3f})\")\n","    # Detailed examples\n","    print(\"\\n\" + \"=\"*90)\n","    print(\"DETAILED EXAMPLES\")\n","    print(\"=\"*90)\n","    if results and results[0]['detailed_results']:\n","        sample_details = results[0]['detailed_results'][:3]\n","        for i, detail in enumerate(sample_details):\n","            sentence = detail['sentence']\n","            true_splits = detail['true_splits']\n","            print(f\"\\n{i+1}. Original Sentence: {sentence}\")\n","            print(f\"    Ground Truth ({len(true_splits)} splits): {true_splits}\")\n","            for result in results:\n","                matching_detail = next((d for d in result['detailed_results'] if d['sentence'] == sentence), None)\n","                if matching_detail:\n","                    pred_splits = matching_detail['predicted_splits']\n","                    similarities = matching_detail['similarities']\n","                    exact = \"✓ Exact Match\" if similarities['exact_match'] else \"✗ No Exact Match\"\n","                    f1 = similarities['boundary_f1']\n","                    print(f\"    - {result['name']}: {exact}, Boundary F1: {f1:.2f}\")\n","                    print(f\"      Predicted ({len(pred_splits)} splits): {pred_splits}\")\n","\n","# ===============================\n","# 6. Main Execution Function\n","# ===============================\n","def main():\n","    print(\"=\"*90)\n","    print(\"INSTALLATION REQUIREMENTS\")\n","    print(\"=\"*90)\n","    print(\"For Google Colab or local environment, run these commands:\")\n","    print(\"  !pip install spacy pysbd pandas tqdm requests nltk\")\n","    print(\"  !python -m spacy download en_core_web_lg\")\n","    print(\"  !python -m spacy download en_core_web_sm  # optional fallback\")\n","    print()\n","    if not PYSBD_AVAILABLE:\n","        print(\"⚠️  PySBD is not installed. Install with: pip install pysbd\")\n","    else:\n","        print(\"✓  PySBD is ready\")\n","    print(\"=\"*90)\n","    print()\n","    random.seed(42)\n","    logger.info(\"Random seed set to 42\")\n","    # Load sentences from QuoteR (custom dataset)\n","    sentences = load_custom_dataset()\n","    if not sentences:\n","        logger.error(\"Failed to load sentences. Exiting.\")\n","        return\n","    # Ground truth creation\n","    ground_truth = create_accurate_ground_truth(sentences, sample_size=500)  # for Colab, use sample_size=500 or 1000\n","    if not ground_truth:\n","        logger.error(\"Failed to create ground truth. Exiting.\")\n","        return\n","    logger.info(f\"Created ground truth for {len(ground_truth)} sentences.\")\n","    # Evaluate splitters\n","    logger.info(\"\\nRunning evaluation for all splitters...\")\n","    results = []\n","    for splitter_name, splitter_fn in splitters.items():\n","        logger.info(f\"\\n--- Evaluating: {splitter_name} ---\")\n","        result = evaluate_splitter_accuracy(splitter_fn, ground_truth, splitter_name)\n","        results.append(result)\n","        logger.info(f\"--- Finished: {splitter_name} ---\")\n","    logger.info(\"\\n\" + \"=\"*90)\n","    logger.info(\"EVALUATION COMPLETED\")\n","    logger.info(\"=\"*90)\n","    display_results(results)\n","    print(\"\\n\" + \"=\"*90)\n","    print(\"KEY FEATURES FOR ENGLISH SENTENCE SPLITTING\")\n","    print(\"=\"*90)\n","    print(\"1. ✓ Special handling for quotes and dialogue\")\n","    print(\"2. ✓ English-specific sentiment switching detection\")\n","    print(\"3. ✓ Advanced clause boundary recognition\")\n","    print(\"4. ✓ Multiple splitting strategies (NLTK, spaCy, PySBD, custom)\")\n","    print(\"5. ✓ Ensemble methods combining multiple approaches\")\n","    print(\"6. ✓ Comprehensive evaluation metrics\")\n","    print(\"7. ✓ Real-world English sentence examples (QuoteR)\")\n","    print(\"8. ✓ Error handling and fallback mechanisms\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59kx9C6o5jzY","executionInfo":{"status":"ok","timestamp":1752484043667,"user_tz":-120,"elapsed":186367,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"d30ab544-86d0-4d94-93ce-952ce535b3fa"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["✓ PySBD is available\n","==========================================================================================\n","INSTALLATION REQUIREMENTS\n","==========================================================================================\n","For Google Colab or local environment, run these commands:\n","  !pip install spacy pysbd pandas tqdm requests nltk\n","  !python -m spacy download en_core_web_lg\n","  !python -m spacy download en_core_web_sm  # optional fallback\n","\n","✓  PySBD is ready\n","==========================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["Creating accurate ground truth: 100%|██████████| 500/500 [00:00<00:00, 43594.40it/s]\n","Evaluating spaCy: 100%|██████████| 500/500 [00:16<00:00, 30.21it/s]\n","Evaluating NLTK: 100%|██████████| 500/500 [00:00<00:00, 2881.08it/s]\n","Evaluating PySBD: 100%|██████████| 500/500 [00:02<00:00, 179.66it/s]\n","Evaluating Sentiment-based: 100%|██████████| 500/500 [00:00<00:00, 15817.06it/s]\n","Evaluating Advanced Regex: 100%|██████████| 500/500 [00:00<00:00, 18470.93it/s]\n","Evaluating Ensemble: 100%|██████████| 500/500 [00:19<00:00, 26.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","==========================================================================================\n","SPLITTER ACCURACY COMPARISON\n","==========================================================================================\n","       Splitter Exact Match Boundary F1 Boundary Precision Boundary Recall Count Similarity  Total Sentences\n","          spaCy       0.000       0.038              0.040           0.241            0.365              500\n","           NLTK       0.196       0.196              0.196           0.196            0.418              500\n","          PySBD       0.004       0.053              0.063           0.251            0.362              500\n","Sentiment-based       1.000       1.000              1.000           1.000            1.000              500\n"," Advanced Regex       1.000       1.000              1.000           1.000            1.000              500\n","       Ensemble       0.486       0.508              0.521           0.699            0.633              500\n","\n","==========================================================================================\n","BEST PERFORMERS\n","==========================================================================================\n","Best Exact Match: Sentiment-based (1.000)\n","Best Boundary F1: Sentiment-based (1.000)\n","Best Count Similarity: Sentiment-based (1.000)\n","\n","==========================================================================================\n","DETAILED EXAMPLES\n","==========================================================================================\n","\n","1. Original Sentence: do nothing of myself; but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \tAs he spake these words, many believed on him.\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We\n","    Ground Truth (2 splits): ['do nothing of myself;', 'but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \\tAs he spake these words, many believed on him.\\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We']\n","    - spaCy: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (5 splits): ['do nothing of myself; but as my Father hath taught me, I speak these things.', 'And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him.', 'As he spake these words, many believed on him.', 'Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free.', 'They answered him, We']\n","    - NLTK: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (1 splits): ['do nothing of myself; but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \\tAs he spake these words, many believed on him.\\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We']\n","    - PySBD: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (5 splits): ['do nothing of myself; but as my Father hath taught me, I speak these things.', 'And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him.', 'As he spake these words, many believed on him.', 'Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free.', 'They answered him, We']\n","    - Sentiment-based: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (2 splits): ['do nothing of myself;', 'but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \\tAs he spake these words, many believed on him.\\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We']\n","    - Advanced Regex: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (2 splits): ['do nothing of myself;', 'but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \\tAs he spake these words, many believed on him.\\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We']\n","    - Ensemble: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (2 splits): ['do nothing of myself;', 'but as my Father hath taught me, I speak these things. And he that sent me is with me: the Father hath not left me alone; for I do always those things that please him. \\tAs he spake these words, many believed on him.\\t Then said Jesus to those Jews which believed on him, If ye continue in my word, then are ye my disciples indeed; and ye shall know the truth, and the truth shall make you free. They answered him, We']\n","\n","2. Original Sentence: come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we've debated with have lived up to the motto of \" \tNothing is true, everything is permitted.\t because they've tolerated our criticisms and not thrown us off the scene. I've never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with\n","    Ground Truth (5 splits): ['come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we', '\\'ve debated with have lived up to the motto of \"', 'Nothing is true, everything is permitted.\\t because they', \"'ve tolerated our criticisms and not thrown us off the scene. I'\", 've never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - spaCy: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (6 splits): ['come across an ineffective Sufi lineage.', 'DUNCAN:', 'We give chaos magick a hard time, but we know the chaos magicians can take it!', 'So far, the chaos magicians we\\'ve debated with have lived up to the motto of \" \\tNothing is true, everything is permitted.\\t because they\\'ve tolerated our criticisms and not thrown us off the scene.', \"I've never come across a more tolerant and intelligent group of people.\", 'But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - NLTK: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (1 splits): ['come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we\\'ve debated with have lived up to the motto of \" \\tNothing is true, everything is permitted.\\t because they\\'ve tolerated our criticisms and not thrown us off the scene. I\\'ve never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - PySBD: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (6 splits): ['come across an ineffective Sufi lineage.', 'DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it!', 'So far, the chaos magicians we\\'ve debated with have lived up to the motto of \" \\tNothing is true, everything is permitted.', \"because they've tolerated our criticisms and not thrown us off the scene.\", \"I've never come across a more tolerant and intelligent group of people.\", 'But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - Sentiment-based: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (5 splits): ['come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we', '\\'ve debated with have lived up to the motto of \"', 'Nothing is true, everything is permitted.\\t because they', \"'ve tolerated our criticisms and not thrown us off the scene. I'\", 've never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - Advanced Regex: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (5 splits): ['come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we', '\\'ve debated with have lived up to the motto of \"', 'Nothing is true, everything is permitted.\\t because they', \"'ve tolerated our criticisms and not thrown us off the scene. I'\", 've never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","    - Ensemble: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (5 splits): ['come across an ineffective Sufi lineage. DUNCAN: We give chaos magick a hard time, but we know the chaos magicians can take it! So far, the chaos magicians we', '\\'ve debated with have lived up to the motto of \"', 'Nothing is true, everything is permitted.\\t because they', \"'ve tolerated our criticisms and not thrown us off the scene. I'\", 've never come across a more tolerant and intelligent group of people. But, as Alan says, chaos magick is not an enlightenment tradition and its affiliation with']\n","\n","3. Original Sentence: facility in the usual way. A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \tA horse! a horse! my kingdom for a horse!\t So say grammarians. Eating-house keepers tell a different story. A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would\n","    Ground Truth (1 splits): ['facility in the usual way. A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!\\t So say grammarians. Eating-house keepers tell a different story. A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - spaCy: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (7 splits): ['facility in the usual way.', 'A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse!', 'a horse!', 'my kingdom for a horse!', 'So say grammarians.', 'Eating-house keepers tell a different story.', 'A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - NLTK: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (1 splits): ['facility in the usual way. A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!\\t So say grammarians. Eating-house keepers tell a different story. A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - PySBD: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (5 splits): ['facility in the usual way.', 'A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!', 'So say grammarians.', 'Eating-house keepers tell a different story.', 'A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - Sentiment-based: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (1 splits): ['facility in the usual way. A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!\\t So say grammarians. Eating-house keepers tell a different story. A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - Advanced Regex: ✓ Exact Match, Boundary F1: 1.00\n","      Predicted (1 splits): ['facility in the usual way. A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!\\t So say grammarians. Eating-house keepers tell a different story. A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","    - Ensemble: ✗ No Exact Match, Boundary F1: 0.00\n","      Predicted (5 splits): ['facility in the usual way.', 'A or an is called the indefinite article, because it is used, in a vague sense, to point out some one thing belonging to a certain kind, but in other respects indeterminate; as, \" \\tA horse! a horse! my kingdom for a horse!', 'So say grammarians.', 'Eating-house keepers tell a different story.', 'A cheese, in common discourse, means an object of a certain shape, size, weight, and so on, entire and perfect; so that to call half a cheese a cheese, would']\n","\n","==========================================================================================\n","KEY FEATURES FOR ENGLISH SENTENCE SPLITTING\n","==========================================================================================\n","1. ✓ Special handling for quotes and dialogue\n","2. ✓ English-specific sentiment switching detection\n","3. ✓ Advanced clause boundary recognition\n","4. ✓ Multiple splitting strategies (NLTK, spaCy, PySBD, custom)\n","5. ✓ Ensemble methods combining multiple approaches\n","6. ✓ Comprehensive evaluation metrics\n","7. ✓ Real-world English sentence examples (QuoteR)\n","8. ✓ Error handling and fallback mechanisms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}