{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GUNhOvAA7gL9pEgjoZdrIJjjMQ__6kWc","timestamp":1752493611407}],"authorship_tag":"ABX9TyMBiLP+9OockZ4DIIOT7u92"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1AYSOnr5WJd","executionInfo":{"status":"ok","timestamp":1752493769810,"user_tz":-120,"elapsed":56847,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"ec26e945-3b83-466b-97e8-2effa8d6de79"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ja-core-news-lg==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_lg-3.8.0/ja_core_news_lg-3.8.0-py3-none-any.whl (555.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m555.3/555.3 MB\u001b[0m \u001b[31m874.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sudachipy!=0.6.1,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from ja-core-news-lg==3.8.0) (0.6.10)\n","Requirement already satisfied: sudachidict-core>=20211220 in /usr/local/lib/python3.11/dist-packages (from ja-core-news-lg==3.8.0) (20250515)\n","Installing collected packages: ja-core-news-lg\n","Successfully installed ja-core-news-lg-3.8.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('ja_core_news_lg')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!pip install spacy ginza sudachipy sudachidict_core pandas tqdm datasets pysbd --quiet\n","!python -m spacy download ja_core_news_lg"]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import random\n","from tqdm import tqdm\n","import spacy\n","from typing import List, Dict, Optional\n","import requests\n","import pysbd\n","\n","# 1. Explicit Indicator Splitter\n","class ExplicitSplitter:\n","    def __init__(self):\n","        self.explicit_switchers = [\n","            'だが', 'ただ', 'ただし', 'とはいえ', 'といっても', 'なのに', 'それなのに', 'にもかかわらず',\n","            'それにもかかわらず', 'ものの', 'ながら', 'ながらも', 'かかわらず', 'とはいうものの', 'そうはいうものの',\n","            'けど', 'けれど', 'けれども', 'しかし', 'でも', 'が', 'ところが', 'そうは言っても', 'とは言え',\n","            'とは言っても', 'にしても', 'にしろ', 'にせよ', 'そうとは言え', 'それでも', 'その反面', 'その一方で',\n","            '反対に', '逆に', 'それが'\n","        ]\n","        # Implicit/sentiment-based splitting: discuss with the team\n","\n","    def find_first_explicit(self, text: str):\n","        positions = [(text.find(sw), sw) for sw in self.explicit_switchers if sw in text]\n","        if positions:\n","            first = min([p for p in positions if p[0] != -1], default=(None, None))\n","            return first[1] if first[0] is not None else None\n","        return None\n","\n","    def split(self, text: str) -> List[str]:\n","        sw = self.find_first_explicit(text)\n","        if sw:\n","            parts = text.split(sw, 1)\n","            if len(parts) == 2 and parts[0].strip() and parts[1].strip():\n","                return [parts[0].strip(), sw + parts[1].strip()]\n","        # No explicit indicator, split on \"。\", \"、、、\", \"。。。\"\n","        # Implicit/sentiment-based splitting: discuss with the team\n","        result = []\n","        # 、、、 or 。。。: try to split at these as well\n","        for delim in [\"、、、\", \"。。。\", \"。\"]:\n","            if delim in text:\n","                return [s+delim if i < len(text.split(delim))-1 else s\n","                        for i, s in enumerate(text.split(delim)) if s]\n","        return [text]\n","\n","explicit_splitter = ExplicitSplitter()\n","\n","# 2. Data Loading (WRIME or fallback)\n","def load_wrime_data(url=\"https://raw.githubusercontent.com/ids-cv/wrime/refs/heads/master/wrime-ver1.tsv\") -> List[str]:\n","    try:\n","        df = pd.read_csv(url, sep=\"\\t\", encoding='utf-8')\n","        sents = df[\"Sentence\"].dropna().astype(str).tolist()\n","        return [s for s in sents if 10 <= len(s) <= 300]\n","    except Exception:\n","        # fallback\n","        return [\n","            \"見た目は素晴らしいです。ただ、値段が高すぎると思います。\",\n","            \"サービスは最高でした。ところが待ち時間が長すぎます。\",\n","            \"料理は美味しいです。そうは言っても、量が少なすぎます。\",\n","            \"音質は素晴らしいです。ただし、重量が重すぎます。\",\n","            \"このアプリは便利。でも、広告が多すぎる。\"\n","        ]\n","\n","# 3. Ground Truth Creation (Explicit & Punct Only)\n","def create_ground_truth(sentences: List[str], splitter: ExplicitSplitter, sample_size=200) -> Dict[str, List[str]]:\n","    sample = random.sample(sentences, min(sample_size, len(sentences)))\n","    ground_truth = {}\n","    for s in tqdm(sample, desc=\"Ground truth\"):\n","        ground_truth[s] = splitter.split(s)\n","    return ground_truth\n","\n","# 4. Splitter Implementations\n","def spacy_split(text: str) -> List[str]:\n","    if not hasattr(spacy_split, \"nlp\"):\n","        try:\n","            spacy_split.nlp = spacy.load(\"ja_core_news_lg\")\n","        except Exception:\n","            return [text]\n","    try:\n","        doc = spacy_split.nlp(text)\n","        return [sent.text.strip() for sent in doc.sents if sent.text.strip()] or [text]\n","    except Exception:\n","        return [text]\n","\n","def punctuation_split(text: str) -> List[str]:\n","    # Split on \"。\", \"、、、\", \"。。。\"\n","    for delim in [\"、、、\", \"。。。\", \"。\"]:\n","        if delim in text:\n","            return [s+delim if i < len(text.split(delim))-1 else s\n","                    for i, s in enumerate(text.split(delim)) if s]\n","    return [text]\n","\n","def advanced_regex_split(text: str) -> List[str]:\n","    # For now, just use punctuation split. (Any advanced logic: discuss with the team)\n","    return punctuation_split(text)\n","\n","def clause_boundary_split(text: str) -> List[str]:\n","    # Only split if conjunction + comma appears\n","    pattern = r'(.*?(?:が|けど|けれど|のに|しかし|そして|または|それで|だから|ところが|ので|から)、)'\n","    matches = re.findall(pattern, text)\n","    result = []\n","    remaining = text\n","    for m in matches:\n","        idx = remaining.find(m)\n","        if idx != -1:\n","            result.append(m.strip())\n","            remaining = remaining[len(m):]\n","    if remaining.strip():\n","        result.append(remaining.strip())\n","    return result if len(result) > 1 else [text]\n","\n","def pysbd_split(text: str) -> List[str]:\n","    seg = pysbd.Segmenter(language=\"ja\", clean=False)\n","    sents = seg.segment(text)\n","    return [s.strip() for s in sents if s.strip()] or [text]\n","\n","# 5. Evaluation\n","def calculate_split_similarity(predicted: List[str], true: List[str]) -> Dict[str, float]:\n","    exact_match = predicted == true\n","    count_diff = abs(len(predicted) - len(true))\n","    count_similarity = 1.0 / (1.0 + count_diff)\n","    # Boundary metrics\n","    def get_boundaries(lst):\n","        pos, bounds = 0, set()\n","        for part in lst[:-1]:\n","            pos += len(part)\n","            bounds.add(pos)\n","        return bounds\n","    pb = get_boundaries(predicted)\n","    tb = get_boundaries(true)\n","    if not pb and not tb:\n","        bp = br = bf1 = 1.0\n","    elif not pb:\n","        bp = br = bf1 = 0.0\n","    elif not tb:\n","        bp = 0.0; br = 1.0; bf1 = 0.0\n","    else:\n","        inter = len(pb & tb)\n","        bp = inter / len(pb) if pb else 0.0\n","        br = inter / len(tb) if tb else 0.0\n","        bf1 = (2 * bp * br) / (bp + br) if (bp + br) > 0 else 0.0\n","    return {\n","        'exact_match': exact_match,\n","        'count_similarity': count_similarity,\n","        'boundary_precision': bp,\n","        'boundary_recall': br,\n","        'boundary_f1': bf1\n","    }\n","\n","def evaluate_splitter(splitter_func, ground_truth: Dict[str, List[str]], name: str) -> Dict:\n","    total_exact = 0\n","    total = 0\n","    boundary_precisions = []\n","    boundary_recalls = []\n","    boundary_f1s = []\n","    count_similarities = []\n","    for sentence, true_splits in tqdm(ground_truth.items(), desc=f\"Evaluating {name}\"):\n","        pred_splits = splitter_func(sentence)\n","        sim = calculate_split_similarity(pred_splits, true_splits)\n","        total_exact += int(sim['exact_match'])\n","        boundary_precisions.append(sim['boundary_precision'])\n","        boundary_recalls.append(sim['boundary_recall'])\n","        boundary_f1s.append(sim['boundary_f1'])\n","        count_similarities.append(sim['count_similarity'])\n","        total += 1\n","    return {\n","        'name': name,\n","        'exact_match_ratio': total_exact / max(total,1),\n","        'avg_boundary_precision': sum(boundary_precisions) / max(len(boundary_precisions),1),\n","        'avg_boundary_recall': sum(boundary_recalls) / max(len(boundary_recalls),1),\n","        'avg_boundary_f1': sum(boundary_f1s) / max(len(boundary_f1s),1),\n","        'avg_count_similarity': sum(count_similarities) / max(len(count_similarities),1),\n","        'total_sentences': total\n","    }\n","\n","# 6. Main Execution\n","def main():\n","    print(\"=\"*60)\n","    print(\"Japanese Sentence Split Evaluation (Explicit Only)\")\n","    print(\"=\"*60)\n","    random.seed(42)\n","    sentences = load_wrime_data()\n","    print(f\"Loaded {len(sentences)} sentences.\")\n","\n","    ground_truth = create_ground_truth(sentences, explicit_splitter, sample_size=10_000)\n","\n","    splitters = {\n","        \"spaCy ja_core_news_lg\": spacy_split,\n","        \"Punctuation Split\": punctuation_split,\n","        \"Advanced Regex Split\": advanced_regex_split,\n","        \"Clause Boundary Split\": clause_boundary_split\n","    }\n","    splitters[\"PySBD Japanese\"] = pysbd_split\n","\n","    results = []\n","    for name, func in splitters.items():\n","        res = evaluate_splitter(func, ground_truth, name)\n","        results.append(res)\n","\n","    # Show results\n","    df = pd.DataFrame(results)\n","    print(df[['name', 'exact_match_ratio', 'avg_boundary_f1', 'avg_boundary_precision', 'avg_boundary_recall', 'avg_count_similarity', 'total_sentences']])\n","\n","    print(\"\\nNOTE: Only explicit indicators and Japanese punctuation (。、、、, etc) used.\")\n","    print(\"      Implicit/sentiment-based splitting will be discussed with the team. May use LLM for future improvement.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59kx9C6o5jzY","executionInfo":{"status":"ok","timestamp":1752493971226,"user_tz":-120,"elapsed":95235,"user":{"displayName":"Ryoji Takahashi","userId":"08099237406056068712"}},"outputId":"cfdcbe52-64e3-4020-ac86-8f79a2f6582e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Japanese Sentence Split Evaluation (Explicit Only)\n","============================================================\n","Loaded 39162 sentences.\n"]},{"output_type":"stream","name":"stderr","text":["Ground truth: 100%|██████████| 10000/10000 [00:00<00:00, 206013.14it/s]\n","Evaluating spaCy ja_core_news_lg: 100%|██████████| 9992/9992 [01:31<00:00, 109.73it/s]\n","Evaluating Punctuation Split: 100%|██████████| 9992/9992 [00:00<00:00, 353472.66it/s]\n","Evaluating Advanced Regex Split: 100%|██████████| 9992/9992 [00:00<00:00, 300784.34it/s]\n","Evaluating Clause Boundary Split: 100%|██████████| 9992/9992 [00:00<00:00, 22267.28it/s]\n","Evaluating PySBD Japanese: 100%|██████████| 9992/9992 [00:03<00:00, 3100.97it/s]"]},{"output_type":"stream","name":"stdout","text":["                    name  exact_match_ratio  avg_boundary_f1  \\\n","0  spaCy ja_core_news_lg           0.412230         0.432917   \n","1      Punctuation Split           0.504604         0.506448   \n","2   Advanced Regex Split           0.504604         0.506448   \n","3  Clause Boundary Split           0.399520         0.401488   \n","4         PySBD Japanese           0.408627         0.431225   \n","\n","   avg_boundary_precision  avg_boundary_recall  avg_count_similarity  \\\n","0                0.429893             0.506388              0.754579   \n","1                0.505903             0.508006              0.799128   \n","2                0.505903             0.508006              0.799128   \n","3                0.401471             0.407926              0.745472   \n","4                0.427978             0.506682              0.752450   \n","\n","   total_sentences  \n","0             9992  \n","1             9992  \n","2             9992  \n","3             9992  \n","4             9992  \n","\n","NOTE: Only explicit indicators and Japanese punctuation (。、、、, etc) used.\n","      Implicit/sentiment-based splitting will be discussed with the team. May use LLM for future improvement.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}