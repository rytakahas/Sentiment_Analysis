{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e26161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERTEX AI XGBOOST PIPELINE WITH ADVANCED EMBEDDINGS\n",
    "# ============================================================================\n",
    "\n",
    "# 1. VERTEX AI SETUP AND IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import CustomJob, CustomTrainingJob\n",
    "from google.cloud.aiplatform.gapic import JobState\n",
    "import google.auth\n",
    "from google.auth import default\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 2. VERTEX AI CONFIGURATION\n",
    "class VertexAIConfig:\n",
    "    def __init__(self, project_id, location=\"us-central1\", staging_bucket=None):\n",
    "        self.project_id = project_id\n",
    "        self.location = location\n",
    "        self.staging_bucket = staging_bucket or f\"gs://{project_id}-ml-staging\"\n",
    "        \n",
    "        # Initialize Vertex AI\n",
    "        aiplatform.init(\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            staging_bucket=self.staging_bucket\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Vertex AI initialized for project: {project_id}\")\n",
    "\n",
    "# 3. DATASET ADAPTER CLASS\n",
    "class DatasetAdapter:\n",
    "    \"\"\"Flexible dataset adapter for different data formats and sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.storage_client = storage.Client(project=config.project_id)\n",
    "    \n",
    "    def load_dataset(self, dataset_config):\n",
    "        \"\"\"\n",
    "        Load dataset based on configuration\n",
    "        \n",
    "        dataset_config example:\n",
    "        {\n",
    "            'type': 'csv',  # 'csv', 'bigquery', 'gcs'\n",
    "            'source': 'path/to/file.csv' or 'project.dataset.table',\n",
    "            'text_column': 'text',\n",
    "            'label_column': 'label',\n",
    "            'additional_features': ['feature1', 'feature2']\n",
    "        }\n",
    "        \"\"\"\n",
    "        if dataset_config['type'] == 'csv':\n",
    "            return self._load_csv(dataset_config)\n",
    "        elif dataset_config['type'] == 'bigquery':\n",
    "            return self._load_bigquery(dataset_config)\n",
    "        elif dataset_config['type'] == 'gcs':\n",
    "            return self._load_gcs(dataset_config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset type: {dataset_config['type']}\")\n",
    "    \n",
    "    def _load_csv(self, config):\n",
    "        \"\"\"Load dataset from local CSV\"\"\"\n",
    "        df = pd.read_csv(config['source'])\n",
    "        return self._process_dataframe(df, config)\n",
    "    \n",
    "    def _load_bigquery(self, config):\n",
    "        \"\"\"Load dataset from BigQuery\"\"\"\n",
    "        from google.cloud import bigquery\n",
    "        client = bigquery.Client(project=self.config.project_id)\n",
    "        \n",
    "        query = f\"SELECT * FROM `{config['source']}`\"\n",
    "        if 'query' in config:\n",
    "            query = config['query']\n",
    "            \n",
    "        df = client.query(query).to_dataframe()\n",
    "        return self._process_dataframe(df, config)\n",
    "    \n",
    "    def _load_gcs(self, config):\n",
    "        \"\"\"Load dataset from Google Cloud Storage\"\"\"\n",
    "        # Download from GCS to local temp file\n",
    "        bucket_name = config['source'].split('/')[2]\n",
    "        blob_path = '/'.join(config['source'].split('/')[3:])\n",
    "        \n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_path)\n",
    "        \n",
    "        local_path = f\"/tmp/{blob_path.split('/')[-1]}\"\n",
    "        blob.download_to_filename(local_path)\n",
    "        \n",
    "        df = pd.read_csv(local_path)\n",
    "        return self._process_dataframe(df, config)\n",
    "    \n",
    "    def _process_dataframe(self, df, config):\n",
    "        \"\"\"Process dataframe to standard format\"\"\"\n",
    "        processed_df = pd.DataFrame()\n",
    "        \n",
    "        # Standard columns\n",
    "        processed_df['text'] = df[config['text_column']]\n",
    "        processed_df['label'] = df[config['label_column']]\n",
    "        \n",
    "        # Additional features if specified\n",
    "        if 'additional_features' in config:\n",
    "            for feature in config['additional_features']:\n",
    "                if feature in df.columns:\n",
    "                    processed_df[feature] = df[feature]\n",
    "        \n",
    "        # Encode labels if they're strings\n",
    "        if processed_df['label'].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            processed_df['label'] = le.fit_transform(processed_df['label'])\n",
    "            processed_df['label_names'] = le.classes_\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "# 4. VERTEX AI EMBEDDING EXTRACTOR\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings using Vertex AI or local models\"\"\"\n",
    "    \n",
    "    def __init__(self, config, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the embedding model\"\"\"\n",
    "        logger.info(f\"Initializing model: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_embeddings(self, texts, batch_size=16, max_length=256):\n",
    "        \"\"\"Extract embeddings from texts\"\"\"\n",
    "        if self.model is None:\n",
    "            self.initialize_model()\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        logger.info(f\"Extracting embeddings for {len(texts)} texts...\")\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = list(texts[i:i+batch_size])\n",
    "            tokens = self.tokenizer(\n",
    "                batch, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=max_length, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    outputs = self.model(**tokens)\n",
    "                    # Use mean pooling of last hidden states\n",
    "                    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                    all_embeddings.append(embeddings)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in batch {i//batch_size}: {e}\")\n",
    "                    # Fallback to zero embeddings\n",
    "                    embeddings = np.zeros((len(batch), 384))  # Adjust dimension as needed\n",
    "                    all_embeddings.append(embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "# 5. VERTEX AI XGBOOST TRAINER\n",
    "class VertexAIXGBoostTrainer:\n",
    "    \"\"\"XGBoost trainer with Vertex AI integration\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "        \n",
    "    def optimize_hyperparameters(self, X_train, y_train, n_trials=50, timeout=600):\n",
    "        \"\"\"Optimize XGBoost hyperparameters using Optuna\"\"\"\n",
    "        \n",
    "        # Create optimization subset for faster tuning\n",
    "        optimization_size = min(2000, len(X_train))\n",
    "        opt_indices = np.random.choice(len(X_train), optimization_size, replace=False)\n",
    "        X_opt = X_train[opt_indices]\n",
    "        y_opt = y_train[opt_indices]\n",
    "        \n",
    "        logger.info(f\"Starting hyperparameter optimization with {optimization_size} samples...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'random_state': 42,\n",
    "                'tree_method': 'hist',  # Use hist for better compatibility\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': len(np.unique(y_train))\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            cv_scores = cross_val_score(\n",
    "                model, X_opt, y_opt, \n",
    "                cv=3, scoring='f1_weighted', n_jobs=1\n",
    "            )\n",
    "            return cv_scores.mean()\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "        )\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        logger.info(f\"Best F1 Score: {self.best_score:.4f}\")\n",
    "        logger.info(f\"Best Parameters: {self.best_params}\")\n",
    "        \n",
    "        return self.best_params, self.best_score\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        \"\"\"Train final model with optimized parameters\"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"Must run optimize_hyperparameters first\")\n",
    "        \n",
    "        final_params = self.best_params.copy()\n",
    "        final_params.update({\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'random_state': 42,\n",
    "            'tree_method': 'hist',\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': len(np.unique(y_train))\n",
    "        })\n",
    "        \n",
    "        self.best_model = xgb.XGBClassifier(**final_params)\n",
    "        \n",
    "        if X_test is not None and y_test is not None:\n",
    "            self.best_model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.best_model.fit(X_train, y_train)\n",
    "        \n",
    "        logger.info(\"Final model training completed\")\n",
    "        return self.best_model\n",
    "\n",
    "# 6. MAIN PIPELINE CLASS\n",
    "class VertexAIMLPipeline:\n",
    "    \"\"\"Main pipeline class that orchestrates the entire process\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id, location=\"us-central1\", staging_bucket=None):\n",
    "        self.config = VertexAIConfig(project_id, location, staging_bucket)\n",
    "        self.dataset_adapter = DatasetAdapter(self.config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(self.config)\n",
    "        self.trainer = VertexAIXGBoostTrainer(self.config)\n",
    "        \n",
    "    def run_pipeline(self, dataset_config, test_size=0.2, embedding_model=None):\n",
    "        \"\"\"Run the complete ML pipeline\"\"\"\n",
    "        \n",
    "        # 1. Load dataset\n",
    "        logger.info(\"Loading dataset...\")\n",
    "        df = self.dataset_adapter.load_dataset(dataset_config)\n",
    "        \n",
    "        # 2. Split data\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=test_size, random_state=42, \n",
    "            stratify=df['label'] if len(df['label'].unique()) > 1 else None\n",
    "        )\n",
    "        \n",
    "        # 3. Extract embeddings\n",
    "        if embedding_model:\n",
    "            self.embedding_extractor.model_name = embedding_model\n",
    "            \n",
    "        X_train_embeddings = self.embedding_extractor.get_embeddings(train_df['text'])\n",
    "        X_test_embeddings = self.embedding_extractor.get_embeddings(test_df['text'])\n",
    "        \n",
    "        # 4. Combine with additional features if available\n",
    "        additional_features = [col for col in train_df.columns \n",
    "                             if col not in ['text', 'label', 'label_names']]\n",
    "        \n",
    "        if additional_features:\n",
    "            logger.info(f\"Adding additional features: {additional_features}\")\n",
    "            X_train_additional = train_df[additional_features].fillna(0).values\n",
    "            X_test_additional = test_df[additional_features].fillna(0).values\n",
    "            \n",
    "            X_train_final = np.hstack([X_train_embeddings, X_train_additional])\n",
    "            X_test_final = np.hstack([X_test_embeddings, X_test_additional])\n",
    "        else:\n",
    "            X_train_final = X_train_embeddings\n",
    "            X_test_final = X_test_embeddings\n",
    "        \n",
    "        y_train = train_df['label'].values\n",
    "        y_test = test_df['label'].values\n",
    "        \n",
    "        logger.info(f\"Final feature shapes: Train {X_train_final.shape}, Test {X_test_final.shape}\")\n",
    "        \n",
    "        # 5. Optimize and train model\n",
    "        self.trainer.optimize_hyperparameters(X_train_final, y_train)\n",
    "        model = self.trainer.train_final_model(X_train_final, y_train, X_test_final, y_test)\n",
    "        \n",
    "        # 6. Evaluate\n",
    "        y_pred = model.predict(X_test_final)\n",
    "        y_proba = model.predict_proba(X_test_final)\n",
    "        \n",
    "        logger.info(\"\\nFinal Model Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        \n",
    "        # 7. Save model to Vertex AI Model Registry (optional)\n",
    "        self.save_model_to_vertex_ai(model, dataset_config.get('model_name', 'xgboost-model'))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba,\n",
    "            'test_labels': y_test,\n",
    "            'feature_importance': model.feature_importances_,\n",
    "            'best_params': self.trainer.best_params,\n",
    "            'best_score': self.trainer.best_score\n",
    "        }\n",
    "    \n",
    "    def save_model_to_vertex_ai(self, model, model_name):\n",
    "        \"\"\"Save model to Vertex AI Model Registry\"\"\"\n",
    "        try:\n",
    "            # Save model locally first\n",
    "            model_path = f\"/tmp/{model_name}.pkl\"\n",
    "            import pickle\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            \n",
    "            # Upload to Vertex AI (simplified - in practice you'd want more robust model serving)\n",
    "            vertex_model = aiplatform.Model.upload(\n",
    "                display_name=model_name,\n",
    "                artifact_uri=self.config.staging_bucket + f\"/models/{model_name}/\",\n",
    "                serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Model uploaded to Vertex AI: {vertex_model.resource_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not upload model to Vertex AI: {e}\")\n",
    "\n",
    "# 7. EXAMPLE USAGE\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the pipeline\"\"\"\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = VertexAIMLPipeline(\n",
    "        project_id=\"your-project-id\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-bucket-name\"\n",
    "    )\n",
    "    \n",
    "    # Define dataset configuration\n",
    "    dataset_config = {\n",
    "        'type': 'csv',  # or 'bigquery', 'gcs'\n",
    "        'source': 'path/to/your/dataset.csv',\n",
    "        'text_column': 'Sentence',  # Adjust to your text column name\n",
    "        'label_column': 'sentiment',  # Adjust to your label column name\n",
    "        'additional_features': [  # Optional additional features\n",
    "            \"Writer_Joy\", \"Writer_Sadness\", \"Writer_Anticipation\", \n",
    "            \"Writer_Surprise\", \"Writer_Anger\", \"Writer_Fear\", \n",
    "            \"Writer_Disgust\", \"Writer_Trust\"\n",
    "        ],\n",
    "        'model_name': 'sentiment-xgboost-model'\n",
    "    }\n",
    "    \n",
    "    # Run pipeline\n",
    "    results = pipeline.run_pipeline(\n",
    "        dataset_config=dataset_config,\n",
    "        test_size=0.2,\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"  # Optional: specify embedding model\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run example\n",
    "# results = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Multi-Task Japanese Sentiment Analysis with Vertex AI\n",
    "# This notebook demonstrates how to use Vertex AI for Japanese sentiment analysis\n",
    "# with multi-task learning capabilities\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, ChatModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "# Authentication and project setup\n",
    "# Make sure you have set up authentication:\n",
    "# gcloud auth application-default login\n",
    "# OR set GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "\n",
    "PROJECT_ID = \"project-id\"  # Replace with your project ID\n",
    "LOCATION = \"us-central1\"  # or your preferred region\n",
    "ENDPOINT_ID = None  # We'll use pre-trained models\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "print(f\"Vertex AI initialized for project: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA PREPARATION AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SentimentData:\n",
    "    sentence: str\n",
    "    sentiment: int  # 0: negative, 1: neutral, 2: positive\n",
    "    joy: float = 0.0\n",
    "    sadness: float = 0.0\n",
    "    anger: float = 0.0\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Data preprocessing utilities for Japanese text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_mapping = {\n",
    "            'negative': 0, 'neutral': 1, 'positive': 2,\n",
    "            0: 'negative', 1: 'neutral', 2: 'positive'\n",
    "        }\n",
    "    \n",
    "    def clean_japanese_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Japanese text\"\"\"\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return \"[EMPTY]\"\n",
    "        \n",
    "        # Basic cleaning\n",
    "        text = str(text).strip()\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def prepare_dataset(self, df: pd.DataFrame) -> List[SentimentData]:\n",
    "        \"\"\"Convert DataFrame to SentimentData objects\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        required_columns = ['Sentence', 'sentiment']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            cleaned_sentence = self.clean_japanese_text(row['Sentence'])\n",
    "            \n",
    "            # Handle emotion scores if available\n",
    "            joy = row.get('Joy', 0.0) if 'Joy' in df.columns else 0.0\n",
    "            sadness = row.get('Sadness', 0.0) if 'Sadness' in df.columns else 0.0\n",
    "            anger = row.get('Anger', 0.0) if 'Anger' in df.columns else 0.0\n",
    "            \n",
    "            data.append(SentimentData(\n",
    "                sentence=cleaned_sentence,\n",
    "                sentiment=int(row['sentiment']),\n",
    "                joy=float(joy),\n",
    "                sadness=float(sadness),\n",
    "                anger=float(anger)\n",
    "            ))\n",
    "        \n",
    "        return data\n",
    "\n",
    "# =============================================================================\n",
    "# 2. VERTEX AI MODEL INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "class VertexAIMultiTaskModel:\n",
    "    \"\"\"\n",
    "    Advanced multi-task sentiment analysis using Vertex AI models\n",
    "    Supports multiple model types including Gemini Pro, PaLM, and custom models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gemini-1.5-pro-002\"):\n",
    "        \"\"\"\n",
    "        Initialize with the best available models for Japanese text analysis\n",
    "        \n",
    "        Recommended models:\n",
    "        - gemini-1.5-pro-002: Latest Gemini Pro with excellent multilingual support\n",
    "        - gemini-1.5-flash-002: Faster version for real-time applications\n",
    "        - text-bison@002: PaLM 2 for Text (good for Japanese)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.generation_config = None\n",
    "        self.safety_settings = None\n",
    "        \n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the appropriate model based on model_name\"\"\"\n",
    "        try:\n",
    "            if \"gemini\" in self.model_name.lower():\n",
    "                self.model = GenerativeModel(self.model_name)\n",
    "                \n",
    "                # Configure generation parameters for consistent results\n",
    "                self.generation_config = generative_models.GenerationConfig(\n",
    "                    max_output_tokens=1024,\n",
    "                    temperature=0.1,  # Low temperature for consistent classification\n",
    "                    top_p=0.8,\n",
    "                    top_k=40,\n",
    "                )\n",
    "                \n",
    "                # Safety settings\n",
    "                self.safety_settings = [\n",
    "                    generative_models.SafetySetting(\n",
    "                        category=generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "                        threshold=generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "                    ),\n",
    "                    generative_models.SafetySetting(\n",
    "                        category=generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "                        threshold=generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "                    ),\n",
    "                ]\n",
    "                \n",
    "            elif \"bison\" in self.model_name.lower():\n",
    "                self.model = TextGenerationModel.from_pretrained(self.model_name)\n",
    "                \n",
    "            print(f\"Successfully initialized {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error initializing model {self.model_name}: {str(e)}\")\n",
    "            # Fallback to Gemini Pro\n",
    "            self.model_name = \"gemini-1.5-pro-002\"\n",
    "            self.model = GenerativeModel(self.model_name)\n",
    "            print(f\"Falling back to {self.model_name}\")\n",
    "    \n",
    "    def create_multi_task_prompt(self, text: str) -> str:\n",
    "        \"\"\"Create a comprehensive prompt for multi-task sentiment analysis\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"ã‚ãªãŸã¯æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã¦ã€æ„Ÿæƒ…æƒ…å ±ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆ: \"{text}\"\n",
    "\n",
    "ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "{{\n",
    "    \"sentiment\": \"positive/neutral/negative\",\n",
    "    \"sentiment_score\": 0-2ã®æ•´æ•°å€¤ (0=negative, 1=neutral, 2=positive),\n",
    "    \"confidence\": 0.0-1.0ã®ä¿¡é ¼åº¦,\n",
    "    \"emotions\": {{\n",
    "        \"joy\": 0.0-1.0ã®ã‚¹ã‚³ã‚¢,\n",
    "        \"sadness\": 0.0-1.0ã®ã‚¹ã‚³ã‚¢,\n",
    "        \"anger\": 0.0-1.0ã®ã‚¹ã‚³ã‚¢,\n",
    "        \"fear\": 0.0-1.0ã®ã‚¹ã‚³ã‚¢,\n",
    "        \"surprise\": 0.0-1.0ã®ã‚¹ã‚³ã‚¢\n",
    "    }},\n",
    "    \"reasoning\": \"åˆ¤æ–­ã®æ ¹æ‹ ã‚’ç°¡æ½”ã«èª¬æ˜\"\n",
    "}}\n",
    "\n",
    "é‡è¦ãªæŒ‡ç¤ºï¼š\n",
    "1. æ—¥æœ¬èªã®æ–‡è„ˆã¨ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ç†è§£ã—ã¦ãã ã•ã„\n",
    "2. æ•¬èªã‚„é–“æ¥çš„ãªè¡¨ç¾ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„\n",
    "3. æ–‡åŒ–çš„ãªèƒŒæ™¯ã‚‚åˆ¤æ–­ã«å«ã‚ã¦ãã ã•ã„\n",
    "4. JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å³å¯†ã«å®ˆã£ã¦ãã ã•ã„\n",
    "5. reasoningä»¥å¤–ã¯è‹±èªã§è¨˜è¿°ã—ã¦ãã ã•ã„\"\"\"\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "    def predict_single(self, text: str, max_retries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Predict sentiment for a single text with retry logic\"\"\"\n",
    "        \n",
    "        prompt = self.create_multi_task_prompt(text)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                if \"gemini\" in self.model_name.lower():\n",
    "                    response = self.model.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config=self.generation_config,\n",
    "                        safety_settings=self.safety_settings\n",
    "                    )\n",
    "                    result_text = response.text\n",
    "                    \n",
    "                elif \"bison\" in self.model_name.lower():\n",
    "                    response = self.model.predict(\n",
    "                        prompt,\n",
    "                        max_output_tokens=1024,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.8,\n",
    "                        top_k=40\n",
    "                    )\n",
    "                    result_text = response.text\n",
    "                \n",
    "                # Parse JSON response\n",
    "                try:\n",
    "                    # Extract JSON from response\n",
    "                    start_idx = result_text.find('{')\n",
    "                    end_idx = result_text.rfind('}') + 1\n",
    "                    \n",
    "                    if start_idx == -1 or end_idx == 0:\n",
    "                        raise ValueError(\"No JSON found in response\")\n",
    "                    \n",
    "                    json_str = result_text[start_idx:end_idx]\n",
    "                    result = json.loads(json_str)\n",
    "                    \n",
    "                    # Validate and normalize result\n",
    "                    result = self._normalize_prediction(result)\n",
    "                    return result\n",
    "                    \n",
    "                except (json.JSONDecodeError, ValueError) as e:\n",
    "                    print(f\"JSON parsing error on attempt {attempt + 1}: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        # Return default prediction\n",
    "                        return self._get_default_prediction()\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"API error on attempt {attempt + 1}: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return self._get_default_prediction()\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return self._get_default_prediction()\n",
    "    \n",
    "    def _normalize_prediction(self, result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Normalize and validate prediction result\"\"\"\n",
    "        \n",
    "        # Ensure required fields exist\n",
    "        normalized = {\n",
    "            'sentiment': result.get('sentiment', 'neutral'),\n",
    "            'sentiment_score': result.get('sentiment_score', 1),\n",
    "            'confidence': result.get('confidence', 0.5),\n",
    "            'emotions': result.get('emotions', {}),\n",
    "            'reasoning': result.get('reasoning', 'No reasoning provided')\n",
    "        }\n",
    "        \n",
    "        # Normalize sentiment\n",
    "        if normalized['sentiment'] not in ['positive', 'neutral', 'negative']:\n",
    "            normalized['sentiment'] = 'neutral'\n",
    "        \n",
    "        # Normalize sentiment_score\n",
    "        if normalized['sentiment'] == 'positive':\n",
    "            normalized['sentiment_score'] = 2\n",
    "        elif normalized['sentiment'] == 'negative':\n",
    "            normalized['sentiment_score'] = 0\n",
    "        else:\n",
    "            normalized['sentiment_score'] = 1\n",
    "        \n",
    "        # Ensure confidence is between 0 and 1\n",
    "        normalized['confidence'] = max(0.0, min(1.0, float(normalized['confidence'])))\n",
    "        \n",
    "        # Normalize emotion scores\n",
    "        emotions = normalized['emotions']\n",
    "        for emotion in ['joy', 'sadness', 'anger', 'fear', 'surprise']:\n",
    "            if emotion in emotions:\n",
    "                emotions[emotion] = max(0.0, min(1.0, float(emotions[emotion])))\n",
    "            else:\n",
    "                emotions[emotion] = 0.0\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _get_default_prediction(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return default prediction when API fails\"\"\"\n",
    "        return {\n",
    "            'sentiment': 'neutral',\n",
    "            'sentiment_score': 1,\n",
    "            'confidence': 0.1,\n",
    "            'emotions': {\n",
    "                'joy': 0.0,\n",
    "                'sadness': 0.0,\n",
    "                'anger': 0.0,\n",
    "                'fear': 0.0,\n",
    "                'surprise': 0.0\n",
    "            },\n",
    "            'reasoning': 'Default prediction due to API failure'\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, texts: List[str], batch_size: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Predict sentiment for a batch of texts with rate limiting\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        print(f\"Starting batch prediction for {total} texts...\")\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_results = []\n",
    "            \n",
    "            for j, text in enumerate(batch):\n",
    "                print(f\"Processing {i + j + 1}/{total}: {text[:50]}...\")\n",
    "                result = self.predict_single(text)\n",
    "                batch_results.append(result)\n",
    "                \n",
    "                # Rate limiting - avoid hitting API limits\n",
    "                if j < len(batch) - 1:  # Don't wait after the last item in batch\n",
    "                    time.sleep(1)  # 1 second between requests\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # Longer pause between batches\n",
    "            if i + batch_size < total:\n",
    "                print(f\"Completed batch {i//batch_size + 1}, pausing...\")\n",
    "                time.sleep(3)\n",
    "        \n",
    "        print(\"Batch prediction completed!\")\n",
    "        return results\n",
    "\n",
    "# =============================================================================\n",
    "# 3. EVALUATION AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive evaluation of multi-task sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_predictions(self, y_true: List[int], predictions: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        \n",
    "        # Extract predictions\n",
    "        y_pred = [pred['sentiment_score'] for pred in predictions]\n",
    "        confidences = [pred['confidence'] for pred in predictions]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        # Classification report\n",
    "        class_report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            target_names=['Negative', 'Neutral', 'Positive'],\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Confidence analysis\n",
    "        avg_confidence = np.mean(confidences)\n",
    "        confidence_by_class = {}\n",
    "        for i in range(3):\n",
    "            class_confidences = [conf for true_label, conf in zip(y_true, confidences) if true_label == i]\n",
    "            confidence_by_class[i] = np.mean(class_confidences) if class_confidences else 0.0\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'classification_report': class_report,\n",
    "            'average_confidence': avg_confidence,\n",
    "            'confidence_by_class': confidence_by_class,\n",
    "            'predictions': y_pred,\n",
    "            'true_labels': y_true,\n",
    "            'confidences': confidences\n",
    "        }\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def analyze_emotions(self, predictions: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze emotion patterns\"\"\"\n",
    "        \n",
    "        emotions_data = {\n",
    "            'joy': [],\n",
    "            'sadness': [],\n",
    "            'anger': [],\n",
    "            'fear': [],\n",
    "            'surprise': []\n",
    "        }\n",
    "        \n",
    "        for pred in predictions:\n",
    "            emotions = pred.get('emotions', {})\n",
    "            for emotion, score in emotions.items():\n",
    "                if emotion in emotions_data:\n",
    "                    emotions_data[emotion].append(score)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        emotion_stats = {}\n",
    "        for emotion, scores in emotions_data.items():\n",
    "            if scores:\n",
    "                emotion_stats[emotion] = {\n",
    "                    'mean': np.mean(scores),\n",
    "                    'std': np.std(scores),\n",
    "                    'max': np.max(scores),\n",
    "                    'min': np.min(scores)\n",
    "                }\n",
    "            else:\n",
    "                emotion_stats[emotion] = {\n",
    "                    'mean': 0.0, 'std': 0.0, 'max': 0.0, 'min': 0.0\n",
    "                }\n",
    "        \n",
    "        return emotion_stats\n",
    "    \n",
    "    def plot_results(self, evaluation_results: Dict[str, Any], save_path: str = None):\n",
    "        \"\"\"Create visualizations of the results\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(evaluation_results['true_labels'], evaluation_results['predictions'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                   ax=axes[0,0])\n",
    "        axes[0,0].set_title('Confusion Matrix')\n",
    "        axes[0,0].set_ylabel('True Label')\n",
    "        axes[0,0].set_xlabel('Predicted Label')\n",
    "        \n",
    "        # Confidence Distribution\n",
    "        axes[0,1].hist(evaluation_results['confidences'], bins=20, alpha=0.7, color='skyblue')\n",
    "        axes[0,1].set_title('Confidence Score Distribution')\n",
    "        axes[0,1].set_xlabel('Confidence Score')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Performance by Class\n",
    "        class_report = evaluation_results['classification_report']\n",
    "        classes = ['Negative', 'Neutral', 'Positive']\n",
    "        f1_scores = [class_report[str(i)]['f1-score'] for i in range(3)]\n",
    "        \n",
    "        axes[1,0].bar(classes, f1_scores, color=['red', 'gray', 'green'], alpha=0.7)\n",
    "        axes[1,0].set_title('F1-Score by Class')\n",
    "        axes[1,0].set_ylabel('F1-Score')\n",
    "        axes[1,0].set_ylim(0, 1)\n",
    "        \n",
    "        # Overall Metrics\n",
    "        metrics = ['Accuracy', 'F1-Macro', 'F1-Weighted']\n",
    "        values = [evaluation_results['accuracy'], \n",
    "                 evaluation_results['f1_macro'], \n",
    "                 evaluation_results['f1_weighted']]\n",
    "        \n",
    "        axes[1,1].bar(metrics, values, color='orange', alpha=0.7)\n",
    "        axes[1,1].set_title('Overall Performance Metrics')\n",
    "        axes[1,1].set_ylabel('Score')\n",
    "        axes[1,1].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main_pipeline(train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                 model_name: str = \"gemini-1.5-pro-002\"):\n",
    "    \"\"\"\n",
    "    Main execution pipeline for Vertex AI multi-task sentiment analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"VERTEX AI MULTI-TASK JAPANESE SENTIMENT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Data Preparation\n",
    "    print(\"\\nStep 1: Data Preparation\")\n",
    "    preprocessor = DataPreprocessor()\n",
    "    \n",
    "    train_data = preprocessor.prepare_dataset(train_df)\n",
    "    test_data = preprocessor.prepare_dataset(test_df)\n",
    "    \n",
    "    print(f\"Training data: {len(train_data)} samples\")\n",
    "    print(f\"Test data: {len(test_data)} samples\")\n",
    "    \n",
    "    # 2. Model Initialization\n",
    "    print(f\"\\n Step 2: Initializing Vertex AI Model ({model_name})\")\n",
    "    model = VertexAIMultiTaskModel(model_name=model_name)\n",
    "    \n",
    "    # 3. Predictions\n",
    "    print(\"\\n Step 3: Generating Predictions\")\n",
    "    test_sentences = [item.sentence for item in test_data]\n",
    "    predictions = model.predict_batch(test_sentences, batch_size=3)  # Smaller batch for stability\n",
    "    \n",
    "    # 4. Evaluation\n",
    "    print(\"\\n Step 4: Model Evaluation\")\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    y_true = [item.sentiment for item in test_data]\n",
    "    evaluation_results = evaluator.evaluate_predictions(y_true, predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n PERFORMANCE RESULTS:\")\n",
    "    print(f\"Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
    "    print(f\"F1-Score (Macro): {evaluation_results['f1_macro']:.4f}\")\n",
    "    print(f\"F1-Score (Weighted): {evaluation_results['f1_weighted']:.4f}\")\n",
    "    print(f\"Average Confidence: {evaluation_results['average_confidence']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n DETAILED CLASSIFICATION REPORT:\")\n",
    "    class_report = evaluation_results['classification_report']\n",
    "    for class_name in ['0', '1', '2']:  # negative, neutral, positive\n",
    "        if class_name in class_report:\n",
    "            metrics = class_report[class_name]\n",
    "            label = ['Negative', 'Neutral', 'Positive'][int(class_name)]\n",
    "            print(f\"{label:>10}: Precision={metrics['precision']:.3f}, \"\n",
    "                  f\"Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
    "    \n",
    "    # 5. Emotion Analysis\n",
    "    print(\"\\n Step 5: Emotion Analysis\")\n",
    "    emotion_stats = evaluator.analyze_emotions(predictions)\n",
    "    \n",
    "    print(\"Average Emotion Scores:\")\n",
    "    for emotion, stats in emotion_stats.items():\n",
    "        print(f\"{emotion.capitalize():>10}: {stats['mean']:.3f} (Â±{stats['std']:.3f})\")\n",
    "    \n",
    "    # 6. Visualization\n",
    "    print(\"\\n Step 6: Creating Visualizations\")\n",
    "    try:\n",
    "        evaluator.plot_results(evaluation_results, save_path='vertex_ai_results.png')\n",
    "    except Exception as e:\n",
    "        print(f\"ï¸ Visualization error: {str(e)}\")\n",
    "    \n",
    "    # 7. Save Results\n",
    "    print(\"\\n Step 7: Saving Results\")\n",
    "    results_summary = {\n",
    "        'model_name': model_name,\n",
    "        'test_samples': len(test_data),\n",
    "        'accuracy': evaluation_results['accuracy'],\n",
    "        'f1_macro': evaluation_results['f1_macro'],\n",
    "        'f1_weighted': evaluation_results['f1_weighted'],\n",
    "        'average_confidence': evaluation_results['average_confidence'],\n",
    "        'emotion_stats': emotion_stats,\n",
    "        'timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame({\n",
    "        'sentence': test_sentences,\n",
    "        'true_sentiment': y_true,\n",
    "        'predicted_sentiment': evaluation_results['predictions'],\n",
    "        'confidence': evaluation_results['confidences'],\n",
    "        'joy': [p['emotions']['joy'] for p in predictions],\n",
    "        'sadness': [p['emotions']['sadness'] for p in predictions],\n",
    "        'anger': [p['emotions']['anger'] for p in predictions],\n",
    "        'reasoning': [p['reasoning'] for p in predictions]\n",
    "    })\n",
    "    \n",
    "    results_df.to_csv('vertex_ai_detailed_results.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    with open('vertex_ai_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\" Results saved to:\")\n",
    "    print(\"  - vertex_ai_detailed_results.csv\")\n",
    "    print(\"  - vertex_ai_summary.json\")\n",
    "    print(\"  - vertex_ai_results.png\")\n",
    "    \n",
    "    return evaluation_results, predictions\n",
    "\n",
    "# =============================================================================\n",
    "# 5. USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage - replace with your actual data loading\n",
    "    print(\"ğŸ“ Loading data...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    # Replace this section with your actual data loading code\n",
    "    sample_data = {\n",
    "        'Sentence': [\n",
    "            'ã“ã®æ˜ ç”»ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã‹ã£ãŸï¼',\n",
    "            'ã¾ã‚ã¾ã‚ã®å‡ºæ¥ã ã¨æ€ã„ã¾ã™ã€‚',\n",
    "            'ã²ã©ã„æ˜ ç”»ã§ã—ãŸã€‚æ™‚é–“ã®ç„¡é§„ã€‚',\n",
    "            'æ™®é€šã®å†…å®¹ã§ã—ãŸã€‚',\n",
    "            'æ„Ÿå‹•çš„ã§æ¶™ãŒå‡ºã¾ã—ãŸã€‚'\n",
    "        ],\n",
    "        'sentiment': [2, 1, 0, 1, 2]  # positive, neutral, negative, neutral, positive\n",
    "    }\n",
    "    \n",
    "    df_sample = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Split into train/test (for demo purposes, using same data)\n",
    "    train_df = df_sample.copy()\n",
    "    test_df = df_sample.copy()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    try:\n",
    "        # Test with different models\n",
    "        models_to_test = [\n",
    "            \"gemini-1.5-pro-002\",      # Best overall performance\n",
    "            \"gemini-1.5-flash-002\",    # Faster inference\n",
    "            # \"text-bison@002\"          # PaLM 2 alternative\n",
    "        ]\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            print(f\"\\n{'='*20} Testing {model_name} {'='*20}\")\n",
    "            \n",
    "            try:\n",
    "                results, predictions = main_pipeline(train_df, test_df, model_name)\n",
    "                all_results[model_name] = results\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error with {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Compare models if multiple were tested\n",
    "        if len(all_results) > 1:\n",
    "            print(f\"\\nğŸ† MODEL COMPARISON:\")\n",
    "            print(f\"{'Model':<25} {'Accuracy':<10} {'F1-Macro':<10} {'F1-Weighted':<12}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for model_name, results in all_results.items():\n",
    "                print(f\"{model_name:<25} {results['accuracy']:<10.4f} \"\n",
    "                      f\"{results['f1_macro']:<10.4f} {results['f1_weighted']:<12.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {str(e)}\")\n",
    "        print(\"Please check your Vertex AI setup and authentication.\")\n",
    "\n",
    "print(\" Vertex AI Multi-Task Sentiment Analysis Setup Complete!\")\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"1. Set your PROJECT_ID variable\")\n",
    "print(\"2. Ensure Vertex AI API is enabled in your GCP project\")\n",
    "print(\"3. Set up authentication (gcloud auth application-default login)\")\n",
    "print(\"4. Replace sample data with your actual dataset\")\n",
    "print(\"5. Run the pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JAPANESE RAG WITH VERTEX AI\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Google Cloud imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
    "from google.cloud.aiplatform.gapic import IndexDatapoint\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# Neo4j for GraphRAG\n",
    "from neo4j import GraphDatabase\n",
    "import neo4j\n",
    "\n",
    "# Text processing for Japanese\n",
    "import MeCab\n",
    "import re\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# LangChain for RAG framework\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import VectorStoreRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VERTEX AI CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    \"\"\"Configuration for Vertex AI services\"\"\"\n",
    "    project_id: str\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = None\n",
    "    embedding_model: str = \"textembedding-gecko\"\n",
    "    generative_model: str = \"gemini-1.5-pro-001\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.staging_bucket:\n",
    "            self.staging_bucket = f\"gs://{self.project_id}-rag-staging\"\n",
    "        \n",
    "        # Initialize Vertex AI\n",
    "        vertexai.init(project=self.project_id, location=self.location)\n",
    "        aiplatform.init(project=self.project_id, location=self.location)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. JAPANESE TEXT PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class JapaneseTextProcessor:\n",
    "    \"\"\"Japanese text preprocessing and tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize MeCab for advanced Japanese processing\n",
    "        try:\n",
    "            self.mecab = MeCab.Tagger('-Owakati')\n",
    "        except:\n",
    "            logger.warning(\"MeCab not available, using Janome fallback\")\n",
    "            self.janome = Tokenizer()\n",
    "            self.mecab = None\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean Japanese text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep Japanese punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF\\u3000-\\u303Fã€‚ã€ï¼ï¼Ÿ]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize Japanese text\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        if self.mecab:\n",
    "            tokens = self.mecab.parse(cleaned_text).strip().split()\n",
    "        else:\n",
    "            tokens = [token.surface for token in self.janome.tokenize(cleaned_text)]\n",
    "        \n",
    "        return [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract named entities from Japanese text\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        if self.mecab:\n",
    "            # Use MeCab with detailed parsing\n",
    "            mecab_detailed = MeCab.Tagger('-Ochasen')\n",
    "            result = mecab_detailed.parse(text)\n",
    "            \n",
    "            for line in result.split('\\n'):\n",
    "                if line and line != 'EOS':\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) >= 4:\n",
    "                        surface = parts[0]\n",
    "                        pos = parts[3] if len(parts) > 3 else \"\"\n",
    "                        \n",
    "                        # Extract proper nouns and other important entities\n",
    "                        if 'å›ºæœ‰åè©' in pos or 'åè©' in pos:\n",
    "                            entities.append({\n",
    "                                'text': surface,\n",
    "                                'type': pos,\n",
    "                                'start': text.find(surface),\n",
    "                                'end': text.find(surface) + len(surface)\n",
    "                            })\n",
    "        \n",
    "        return entities\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VERTEX AI EMBEDDING SERVICE\n",
    "# ============================================================================\n",
    "\n",
    "class VertexAIEmbeddings(Embeddings):\n",
    "    \"\"\"Custom LangChain Embeddings class for Vertex AI\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig):\n",
    "        self.config = config\n",
    "        self.model = TextEmbeddingModel.from_pretrained(config.embedding_model)\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed multiple documents\"\"\"\n",
    "        embeddings = []\n",
    "        batch_size = 5  # Vertex AI rate limits\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                batch_embeddings = self.model.get_embeddings(batch)\n",
    "                embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error embedding batch {i//batch_size}: {e}\")\n",
    "                # Fallback to zero embeddings\n",
    "                embeddings.extend([[0.0] * 768 for _ in batch])\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        try:\n",
    "            embedding = self.model.get_embeddings([text])[0]\n",
    "            return embedding.values\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error embedding query: {e}\")\n",
    "            return [0.0] * 768\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VERTEX AI VECTOR SEARCH INTEGRATION\n",
    "# ============================================================================\n",
    "\n",
    "class VertexAIVectorSearch:\n",
    "    \"\"\"Vertex AI Vector Search integration\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig):\n",
    "        self.config = config\n",
    "        self.index = None\n",
    "        self.endpoint = None\n",
    "        self.embeddings = VertexAIEmbeddings(config)\n",
    "        \n",
    "    def create_index(self, display_name: str, dimensions: int = 768):\n",
    "        \"\"\"Create a new Vector Search index\"\"\"\n",
    "        try:\n",
    "            self.index = MatchingEngineIndex.create_tree_ah_index(\n",
    "                display_name=display_name,\n",
    "                contents_delta_uri=f\"{self.config.staging_bucket}/vector_index/\",\n",
    "                dimensions=dimensions,\n",
    "                approximate_neighbors_count=150,\n",
    "                leaf_node_embedding_count=500,\n",
    "                leaf_nodes_to_search_percent=7,\n",
    "                description=\"Japanese RAG Vector Index\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created index: {self.index.resource_name}\")\n",
    "            return self.index\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating index: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def deploy_index(self, endpoint_display_name: str):\n",
    "        \"\"\"Deploy index to an endpoint\"\"\"\n",
    "        try:\n",
    "            # Create endpoint\n",
    "            self.endpoint = MatchingEngineIndexEndpoint.create(\n",
    "                display_name=endpoint_display_name,\n",
    "                public_endpoint_enabled=True\n",
    "            )\n",
    "            \n",
    "            # Deploy index to endpoint\n",
    "            self.endpoint.deploy_index(\n",
    "                index=self.index,\n",
    "                deployed_index_id=\"deployed_index_id\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Index deployed to endpoint: {self.endpoint.resource_name}\")\n",
    "            return self.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deploying index: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the vector index\"\"\"\n",
    "        datapoints = []\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            embedding = self.embeddings.embed_query(doc.page_content)\n",
    "            \n",
    "            datapoint = IndexDatapoint(\n",
    "                datapoint_id=str(uuid.uuid4()),\n",
    "                feature_vector=embedding,\n",
    "                restricts=[],\n",
    "                crowding_tag=\"\"\n",
    "            )\n",
    "            datapoints.append(datapoint)\n",
    "        \n",
    "        # Batch upload to index\n",
    "        if self.index:\n",
    "            try:\n",
    "                self.index.upsert_datapoints(datapoints)\n",
    "                logger.info(f\"Added {len(datapoints)} documents to index\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error adding documents: {e}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if not self.endpoint:\n",
    "            logger.error(\"Index endpoint not deployed\")\n",
    "            return []\n",
    "        \n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        try:\n",
    "            results = self.endpoint.find_neighbors(\n",
    "                deployed_index_id=\"deployed_index_id\",\n",
    "                queries=[query_embedding],\n",
    "                num_neighbors=top_k\n",
    "            )\n",
    "            \n",
    "            return [{\"id\": neighbor.id, \"distance\": neighbor.distance} \n",
    "                   for neighbor in results[0]]\n",
    "                   \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching: {e}\")\n",
    "            return []\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRADITIONAL RAG IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "class JapaneseRAGSystem:\n",
    "    \"\"\"Traditional RAG system for Japanese documents\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig):\n",
    "        self.config = config\n",
    "        self.text_processor = JapaneseTextProcessor()\n",
    "        self.embeddings = VertexAIEmbeddings(config)\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.llm = self._initialize_llm()\n",
    "        \n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"Initialize Vertex AI LLM\"\"\"\n",
    "        return GenerativeModel(self.config.generative_model)\n",
    "    \n",
    "    def load_documents(self, file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Load and process Japanese documents\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                if file_path.endswith('.pdf'):\n",
    "                    loader = PDFLoader(file_path)\n",
    "                else:\n",
    "                    loader = TextLoader(file_path, encoding='utf-8')\n",
    "                \n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Process each document\n",
    "                for doc in docs:\n",
    "                    # Clean Japanese text\n",
    "                    cleaned_content = self.text_processor.clean_text(doc.page_content)\n",
    "                    doc.page_content = cleaned_content\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks suitable for Japanese text\"\"\"\n",
    "        # Custom splitter for Japanese\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Smaller chunks for Japanese\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ã€\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    def build_vectorstore(self, documents: List[Document]):\n",
    "        \"\"\"Build vector store from documents\"\"\"\n",
    "        logger.info(f\"Building vector store with {len(documents)} documents...\")\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        self.vectorstore = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Create retriever\n",
    "        self.retriever = VectorStoreRetriever(\n",
    "            vectorstore=self.vectorstore,\n",
    "            search_kwargs={\"k\": 5}\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Vector store built successfully\")\n",
    "    \n",
    "    def query(self, question: str, context_window: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not built. Call build_vectorstore first.\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:context_window]])\n",
    "        \n",
    "        # Create prompt for Japanese RAG\n",
    "        prompt = f\"\"\"\n",
    "ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ–‡æ›¸:\n",
    "{context}\n",
    "\n",
    "è³ªå•: {question}\n",
    "\n",
    "å›ç­”:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.llm.generate_content(prompt)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.text,\n",
    "                \"sources\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} \n",
    "                           for doc in relevant_docs],\n",
    "                \"context\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\",\n",
    "                \"sources\": [],\n",
    "                \"context\": \"\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GraphRAG\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, embedding_model=None, llm=None):\n",
    "        self.G = nx.Graph()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = llm\n",
    "    \n",
    "    def add_document(self, doc_id: int, text: str, embedding: np.ndarray = None):\n",
    "        \"\"\"Add a document node to the graph\"\"\"\n",
    "        if embedding is None and self.embedding_model:\n",
    "            # Generate embedding if not provided\n",
    "            embedding = self.embedding_model.get_embeddings([text])[0].values\n",
    "        \n",
    "        self.G.add_node(doc_id, text=text, embedding=embedding)\n",
    "    \n",
    "    def add_relation(self, doc_id1: int, doc_id2: int, relation_type: str = \"reference\"):\n",
    "        \"\"\"Add an edge between two documents\"\"\"\n",
    "        if doc_id1 in self.G.nodes and doc_id2 in self.G.nodes:\n",
    "            self.G.add_edge(doc_id1, doc_id2, type=relation_type)\n",
    "        else:\n",
    "            raise ValueError(f\"One or both document IDs ({doc_id1}, {doc_id2}) not found in graph\")\n",
    "    \n",
    "    def find_similar_nodes(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Find top-k most similar nodes to query embedding\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for node_id in self.G.nodes():\n",
    "            node_embedding = self.G.nodes[node_id]['embedding']\n",
    "            if node_embedding is not None:\n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity(\n",
    "                    query_embedding.reshape(1, -1), \n",
    "                    node_embedding.reshape(1, -1)\n",
    "                )[0][0]\n",
    "                similarities.append((node_id, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending) and return top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_subgraph_context(self, node_ids: List[int], max_hops: int = 1) -> str:\n",
    "        \"\"\"Get context from nodes and their neighbors within max_hops\"\"\"\n",
    "        context_nodes = set(node_ids)\n",
    "        \n",
    "        # Add neighbors within max_hops\n",
    "        for node_id in node_ids:\n",
    "            if node_id in self.G.nodes:\n",
    "                # Get neighbors within max_hops using BFS\n",
    "                neighbors = nx.single_source_shortest_path_length(\n",
    "                    self.G, node_id, cutoff=max_hops\n",
    "                )\n",
    "                context_nodes.update(neighbors.keys())\n",
    "        \n",
    "        # Collect text from all context nodes\n",
    "        context_texts = []\n",
    "        for node_id in context_nodes:\n",
    "            if node_id in self.G.nodes and 'text' in self.G.nodes[node_id]:\n",
    "                text = self.G.nodes[node_id]['text']\n",
    "                context_texts.append(f\"æ–‡æ›¸{node_id}: {text}\")\n",
    "        \n",
    "        return \"\\n\".join(context_texts)\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 3, max_hops: int = 1) -> str:\n",
    "        \"\"\"Main query method for GraphRAG\"\"\"\n",
    "        if not self.embedding_model or not self.llm:\n",
    "            raise ValueError(\"Both embedding_model and llm must be provided\")\n",
    "        \n",
    "        # 1. Get query embedding\n",
    "        try:\n",
    "            query_embedding = self.embedding_model.get_embeddings([query])[0].values\n",
    "        except AttributeError:\n",
    "            # Handle different embedding model interfaces\n",
    "            query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # 2. Find similar nodes\n",
    "        similar_nodes = self.find_similar_nodes(query_embedding, top_k)\n",
    "        \n",
    "        if not similar_nodes:\n",
    "            return \"é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    "        \n",
    "        # 3. Get subgraph context\n",
    "        top_node_ids = [node_id for node_id, _ in similar_nodes]\n",
    "        context = self.get_subgraph_context(top_node_ids, max_hops)\n",
    "        \n",
    "        # 4. Generate response using LLM\n",
    "        prompt = f\"\"\"å‚è€ƒæ–‡æ›¸:\n",
    "{context}\n",
    "\n",
    "è³ªå•: {query}\n",
    "\n",
    "ä¸Šè¨˜ã®å‚è€ƒæ–‡æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ç­”ãˆ:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.predict(prompt)\n",
    "            return response.text if hasattr(response, 'text') else str(response)\n",
    "        except Exception as e:\n",
    "            return f\"LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the GraphRAG class\"\"\"\n",
    "    \n",
    "    # Initialize GraphRAG\n",
    "    graph_rag = GraphRAG()  # You would pass your actual embedding_model and llm here\n",
    "    \n",
    "    # Sample embeddings (in practice, these would come from your embedding model)\n",
    "    doc_embeddings = [\n",
    "        np.random.rand(384),  # Example embedding dimension\n",
    "        np.random.rand(384),\n",
    "        np.random.rand(384)\n",
    "    ]\n",
    "    \n",
    "    # Add documents\n",
    "    graph_rag.add_document(0, \"ã“ã‚Œã¯æœ€åˆã®æ–‡æ›¸ã§ã™ã€‚äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\", doc_embeddings[0])\n",
    "    graph_rag.add_document(1, \"ã“ã‚Œã¯äºŒç•ªç›®ã®æ–‡æ›¸ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦è©³ã—ãè¿°ã¹ã¦ã„ã¾ã™ã€‚\", doc_embeddings[1])\n",
    "    graph_rag.add_document(2, \"ã“ã‚Œã¯ä¸‰ç•ªç›®ã®æ–‡æ›¸ã§ã™ã€‚æ·±å±¤å­¦ç¿’ã®å¿œç”¨ã«ã¤ã„ã¦æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚\", doc_embeddings[2])\n",
    "    \n",
    "    # Add relations\n",
    "    graph_rag.add_relation(0, 1, \"reference\")\n",
    "    graph_rag.add_relation(1, 2, \"related\")\n",
    "    \n",
    "    # Example query (you would need actual embedding_model and llm)\n",
    "    query = \"æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„\"\n",
    "    \n",
    "    # For demonstration without actual models:\n",
    "    print(\"GraphRAG setup completed!\")\n",
    "    print(f\"Graph has {graph_rag.G.number_of_nodes()} nodes and {graph_rag.G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Show graph structure\n",
    "    print(\"\\nGraph structure:\")\n",
    "    for node in graph_rag.G.nodes(data=True):\n",
    "        print(f\"Node {node[0]}: {node[1]['text'][:50]}...\")\n",
    "    \n",
    "    for edge in graph_rag.G.edges(data=True):\n",
    "        print(f\"Edge {edge[0]}-{edge[1]}: {edge[2]['type']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8254e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. GRAPHRAG IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "class JapaneseGraphRAG:\n",
    "    \"\"\"GraphRAG system for Japanese documents using Neo4j\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, neo4j_uri: str, neo4j_user: str, neo4j_password: str):\n",
    "        self.config = config\n",
    "        self.text_processor = JapaneseTextProcessor()\n",
    "        self.embeddings = VertexAIEmbeddings(config)\n",
    "        self.llm = GenerativeModel(config.generative_model)\n",
    "        \n",
    "        # Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.close()\n",
    "    \n",
    "    def create_graph_from_documents(self, documents: List[Document]):\n",
    "        \"\"\"Create knowledge graph from Japanese documents\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Clear existing graph\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "            for i, doc in enumerate(documents):\n",
    "                logger.info(f\"Processing document {i+1}/{len(documents)}\")\n",
    "                \n",
    "                # Extract entities\n",
    "                entities = self.text_processor.extract_entities(doc.page_content)\n",
    "                \n",
    "                # Create document node\n",
    "                doc_embedding = self.embeddings.embed_query(doc.page_content)\n",
    "                session.run(\"\"\"\n",
    "                    CREATE (d:Document {\n",
    "                        id: $doc_id,\n",
    "                        content: $content,\n",
    "                        embedding: $embedding,\n",
    "                        metadata: $metadata\n",
    "                    })\n",
    "                \"\"\", doc_id=f\"doc_{i}\", content=doc.page_content, \n",
    "                    embedding=doc_embedding, metadata=json.dumps(doc.metadata))\n",
    "                \n",
    "                # Create entity nodes and relationships\n",
    "                for entity in entities:\n",
    "                    entity_embedding = self.embeddings.embed_query(entity['text'])\n",
    "                    \n",
    "                    # Create entity node\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (e:Entity {text: $text})\n",
    "                        SET e.type = $type,\n",
    "                            e.embedding = $embedding\n",
    "                    \"\"\", text=entity['text'], type=entity['type'], embedding=entity_embedding)\n",
    "                    \n",
    "                    # Create relationship between document and entity\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (d:Document {id: $doc_id})\n",
    "                        MATCH (e:Entity {text: $entity_text})\n",
    "                        CREATE (d)-[:CONTAINS]->(e)\n",
    "                    \"\"\", doc_id=f\"doc_{i}\", entity_text=entity['text'])\n",
    "                \n",
    "                # Extract relationships between entities using LLM\n",
    "                relationships = self._extract_relationships(doc.page_content, entities)\n",
    "                \n",
    "                for rel in relationships:\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (e1:Entity {text: $entity1})\n",
    "                        MATCH (e2:Entity {text: $entity2})\n",
    "                        CREATE (e1)-[:RELATED {type: $rel_type, confidence: $confidence}]->(e2)\n",
    "                    \"\"\", entity1=rel['entity1'], entity2=rel['entity2'], \n",
    "                        rel_type=rel['type'], confidence=rel['confidence'])\n",
    "    \n",
    "    def _extract_relationships(self, text: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract relationships between entities using LLM\"\"\"\n",
    "        if len(entities) < 2:\n",
    "            return []\n",
    "        \n",
    "        entity_list = [e['text'] for e in entities[:10]]  # Limit to avoid token limits\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆ: {text}\n",
    "\n",
    "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {', '.join(entity_list)}\n",
    "\n",
    "é–¢ä¿‚ã‚’ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\n",
    "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£1|é–¢ä¿‚ã®ç¨®é¡|ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£2|ä¿¡é ¼åº¦(0-1)\n",
    "\n",
    "ä¾‹ï¼š\n",
    "æ±äº¬|ä½ç½®|æ—¥æœ¬|0.9\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.generate_content(prompt)\n",
    "            relationships = []\n",
    "            \n",
    "            for line in response.text.split('\\n'):\n",
    "                if '|' in line:\n",
    "                    parts = line.strip().split('|')\n",
    "                    if len(parts) >= 4:\n",
    "                        relationships.append({\n",
    "                            'entity1': parts[0],\n",
    "                            'type': parts[1],\n",
    "                            'entity2': parts[2],\n",
    "                            'confidence': float(parts[3]) if parts[3].replace('.', '').isdigit() else 0.5\n",
    "                        })\n",
    "            \n",
    "            return relationships\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting relationships: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Hybrid search combining vector similarity and graph traversal\"\"\"\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Vector similarity search on documents\n",
    "            vector_results = session.run(\"\"\"\n",
    "                MATCH (d:Document)\n",
    "                WITH d, gds.similarity.cosine(d.embedding, $query_embedding) AS similarity\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT $top_k\n",
    "                RETURN d.id as doc_id, d.content as content, similarity, d.metadata as metadata\n",
    "            \"\"\", query_embedding=query_embedding, top_k=top_k)\n",
    "            \n",
    "            # Graph-based search for entities\n",
    "            entity_results = session.run(\"\"\"\n",
    "                MATCH (e:Entity)\n",
    "                WITH e, gds.similarity.cosine(e.embedding, $query_embedding) AS similarity\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT $top_k\n",
    "                MATCH (d:Document)-[:CONTAINS]->(e)\n",
    "                RETURN DISTINCT d.id as doc_id, d.content as content, \n",
    "                       e.text as entity, e.type as entity_type, similarity\n",
    "            \"\"\", query_embedding=query_embedding, top_k=top_k)\n",
    "            \n",
    "            # Combine results\n",
    "            results = []\n",
    "            \n",
    "            for record in vector_results:\n",
    "                results.append({\n",
    "                    'doc_id': record['doc_id'],\n",
    "                    'content': record['content'],\n",
    "                    'similarity': record['similarity'],\n",
    "                    'metadata': json.loads(record['metadata']),\n",
    "                    'source': 'vector'\n",
    "                })\n",
    "            \n",
    "            for record in entity_results:\n",
    "                results.append({\n",
    "                    'doc_id': record['doc_id'],\n",
    "                    'content': record['content'],\n",
    "                    'entity': record['entity'],\n",
    "                    'entity_type': record['entity_type'],\n",
    "                    'similarity': record['similarity'],\n",
    "                    'source': 'graph'\n",
    "                })\n",
    "            \n",
    "            # Sort by similarity and remove duplicates\n",
    "            unique_results = {}\n",
    "            for result in results:\n",
    "                doc_id = result['doc_id']\n",
    "                if doc_id not in unique_results or result['similarity'] > unique_results[doc_id]['similarity']:\n",
    "                    unique_results[doc_id] = result\n",
    "            \n",
    "            return sorted(unique_results.values(), key=lambda x: x['similarity'], reverse=True)[:top_k]\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Query the GraphRAG system\"\"\"\n",
    "        # Get hybrid search results\n",
    "        search_results = self.hybrid_search(question, top_k)\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_parts = []\n",
    "        for result in search_results:\n",
    "            context_parts.append(f\"æ–‡æ›¸ID: {result['doc_id']}\")\n",
    "            context_parts.append(f\"å†…å®¹: {result['content'][:500]}...\")\n",
    "            if 'entity' in result:\n",
    "                context_parts.append(f\"é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {result['entity']} ({result['entity_type']})\")\n",
    "            context_parts.append(\"---\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Create prompt for GraphRAG\n",
    "        prompt = f\"\"\"\n",
    "ä»¥ä¸‹ã®æ–‡æ›¸ã¨ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n",
    "\n",
    "å‚è€ƒæƒ…å ±:\n",
    "{context}\n",
    "\n",
    "è³ªå•: {question}\n",
    "\n",
    "å›ç­”:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.generate_content(prompt)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.text,\n",
    "                \"sources\": search_results,\n",
    "                \"context\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating GraphRAG response: {e}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\",\n",
    "                \"sources\": [],\n",
    "                \"context\": \"\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. MAIN PIPELINE CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class JapaneseRAGPipeline:\n",
    "    \"\"\"Main pipeline for Japanese RAG and GraphRAG\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, neo4j_config: Dict = None):\n",
    "        self.config = config\n",
    "        self.rag_system = JapaneseRAGSystem(config)\n",
    "        \n",
    "        if neo4j_config:\n",
    "            self.graph_rag = JapaneseGraphRAG(\n",
    "                config, \n",
    "                neo4j_config['uri'], \n",
    "                neo4j_config['user'], \n",
    "                neo4j_config['password']\n",
    "            )\n",
    "        else:\n",
    "            self.graph_rag = None\n",
    "    \n",
    "    def setup_rag(self, document_paths: List[str]):\n",
    "        \"\"\"Setup traditional RAG system\"\"\"\n",
    "        logger.info(\"Setting up RAG system...\")\n",
    "        \n",
    "        # Load and process documents\n",
    "        documents = self.rag_system.load_documents(document_paths)\n",
    "        split_docs = self.rag_system.split_documents(documents)\n",
    "        \n",
    "        # Build vector store\n",
    "        self.rag_system.build_vectorstore(split_docs)\n",
    "        \n",
    "        logger.info(\"RAG system setup complete\")\n",
    "    \n",
    "    def setup_graph_rag(self, document_paths: List[str]):\n",
    "        \"\"\"Setup GraphRAG system\"\"\"\n",
    "        if not self.graph_rag:\n",
    "            raise ValueError(\"Neo4j configuration not provided\")\n",
    "        \n",
    "        logger.info(\"Setting up GraphRAG system...\")\n",
    "        \n",
    "        # Load documents\n",
    "        documents = self.rag_system.load_documents(document_paths)\n",
    "        split_docs = self.rag_system.split_documents(documents)\n",
    "        \n",
    "        # Create knowledge graph\n",
    "        self.graph_rag.create_graph_from_documents(split_docs)\n",
    "        \n",
    "        logger.info(\"GraphRAG system setup complete\")\n",
    "    \n",
    "    def query_rag(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Query traditional RAG system\"\"\"\n",
    "        return self.rag_system.query(question)\n",
    "    \n",
    "    def query_graph_rag(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Query GraphRAG system\"\"\"\n",
    "        if not self.graph_rag:\n",
    "            raise ValueError(\"GraphRAG not initialized\")\n",
    "        return self.graph_rag.query(question)\n",
    "    \n",
    "    def compare_systems(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Compare RAG and GraphRAG responses\"\"\"\n",
    "        rag_result = self.query_rag(question)\n",
    "        graph_rag_result = self.query_graph_rag(question) if self.graph_rag else None\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"rag_response\": rag_result,\n",
    "            \"graph_rag_response\": graph_rag_result,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the Japanese RAG/GraphRAG systems\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = VertexAIConfig(\n",
    "        project_id=\"your-project-id\",\n",
    "        location=\"us-central1\",\n",
    "        embedding_model=\"textembedding-gecko\",\n",
    "        generative_model=\"gemini-1.5-pro-001\"\n",
    "    )\n",
    "    \n",
    "    # Neo4j configuration (optional for GraphRAG)\n",
    "    neo4j_config = {\n",
    "        \"uri\": \"bolt://localhost:7687\",  # or Neo4j Aura URI\n",
    "        \"user\": \"neo4j\",\n",
    "        \"password\": \"your-password\"\n",
    "    }\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = JapaneseRAGPipeline(config, neo4j_config)\n",
    "    \n",
    "    # Document paths (support .txt, .pdf files)\n",
    "    document_paths = [\n",
    "        \"japanese_document1.txt\",\n",
    "        \"japanese_document2.pdf\",\n",
    "        # Add your Japanese document paths here\n",
    "    ]\n",
    "    \n",
    "    # Setup systems\n",
    "    pipeline.setup_rag(document_paths)\n",
    "    pipeline.setup_graph_rag(document_paths)  # Optional\n",
    "    \n",
    "    # Query examples\n",
    "    questions = [\n",
    "        \"ã“ã®æ–‡æ›¸ã®ä¸»è¦ãªãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "        \"ç™»å ´äººç‰©ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„\",\n",
    "        \"é‡è¦ãªå‡ºæ¥äº‹ã¯ã„ã¤èµ·ã“ã‚Šã¾ã—ãŸã‹ï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nè³ªå•: {question}\")\n",
    "        \n",
    "        # Traditional RAG\n",
    "        rag_result = pipeline.query_rag(question)\n",
    "        print(f\"RAGå›ç­”: {rag_result['answer']}\")\n",
    "        \n",
    "        # GraphRAG (if available)\n",
    "        if pipeline.graph_rag:\n",
    "            graph_result = pipeline.query_graph_rag(question)\n",
    "            print(f\"GraphRAGå›ç­”: {graph_result['answer']}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Uncomment to run example\n",
    "# example_usage()\n",
    "\n",
    "# ============================================================================\n",
    "# 9. DEPLOYMENT UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class RAGDeployment:\n",
    "    \"\"\"Utilities for deploying RAG systems to production\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vertex_ai_endpoint(config: VertexAIConfig, model_name: str):\n",
    "        \"\"\"Create Vertex AI endpoint for RAG service\"\"\"\n",
    "        # This would typically involve containerizing the RAG service\n",
    "        # and deploying it to Vertex AI Endpoints\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_monitoring(config: VertexAIConfig):\n",
    "        \"\"\"Setup monitoring for RAG system\"\"\"\n",
    "        # Setup Cloud Monitoring for the RAG system\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_api_gateway(config: VertexAIConfig):\n",
    "        \"\"\"Create API Gateway for RAG service\"\"\"\n",
    "        # Setup API Gateway to expose RAG endpoints\n",
    "        pass\n",
    "\n",
    "# Example production setup\n",
    "def production_setup():\n",
    "    \"\"\"Example production setup\"\"\"\n",
    "    config = VertexAIConfig(\n",
    "        project_id=\"your-production-project\",\n",
    "        location=\"asia-northeast1\",  # Tokyo region for Japanese content\n",
    "        staging_bucket=\"gs://your-production-bucket\"\n",
    "    )\n",
    "    \n",
    "    # Setup production RAG pipeline\n",
    "    pipeline = JapaneseRAGPipeline(config)\n",
    "    \n",
    "    # Additional production configurations would go here\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. DEPLOYMENT UTILITIES (COMPLETE)\n",
    "# ============================================================================\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import monitoring_v3\n",
    "from google.cloud import apigateway_v1\n",
    "\n",
    "class RAGDeployment:\n",
    "    \"\"\"Utilities for deploying RAG systems to production on Vertex AI.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_vertex_ai_endpoint(config: VertexAIConfig, model_display_name: str, serving_image_uri: str):\n",
    "        \"\"\"\n",
    "        Deploy a custom model (container) to Vertex AI Endpoint.\n",
    "        serving_image_uri: Path to container image in GCR/AR (Artifact Registry)\n",
    "        \"\"\"\n",
    "        aiplatform.init(project=config.project_id, location=config.location, staging_bucket=config.staging_bucket)\n",
    "\n",
    "        # Model upload (Container)\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=model_display_name,\n",
    "            serving_container_image_uri=serving_image_uri,\n",
    "            artifact_uri=None,\n",
    "            sync=True\n",
    "        )\n",
    "        logger.info(f\"Model uploaded: {model.resource_name}\")\n",
    "\n",
    "        # Endpoint creation\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_display_name}-endpoint\",\n",
    "            sync=True\n",
    "        )\n",
    "        logger.info(f\"Endpoint created: {endpoint.resource_name}\")\n",
    "\n",
    "        # Deploy model to endpoint\n",
    "        endpoint.deploy(\n",
    "            model=model,\n",
    "            deployed_model_display_name=f\"{model_display_name}-deployed\",\n",
    "            machine_type=\"n1-standard-4\",  # or \"a2-highgpu-1g\" for GPU\n",
    "            traffic_split={\"0\": 100}\n",
    "        )\n",
    "        logger.info(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "        return endpoint\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_monitoring(config: VertexAIConfig):\n",
    "        \"\"\"\n",
    "        Setup Cloud Monitoring: Example is a placeholder for custom metric setup.\n",
    "        \"\"\"\n",
    "        client = monitoring_v3.MetricServiceClient()\n",
    "        project_name = f\"projects/{config.project_id}\"\n",
    "        logger.info(f\"Monitoring client set up for project {project_name}\")\n",
    "        # You can set up custom metrics, alerting policies, etc.\n",
    "        # See https://cloud.google.com/monitoring/docs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_api_gateway(config: VertexAIConfig, api_name: str):\n",
    "        \"\"\"\n",
    "        Create API Gateway endpoint (stub; setup via Cloud Console or IaC tools).\n",
    "        \"\"\"\n",
    "        client = apigateway_v1.ApiGatewayServiceClient()\n",
    "        parent = f\"projects/{config.project_id}/locations/global\"\n",
    "        api = apigateway_v1.Api(\n",
    "            display_name=api_name\n",
    "        )\n",
    "        operation = client.create_api(parent=parent, api_id=api_name, api=api)\n",
    "        logger.info(f\"API Gateway creation initiated: {api_name}\")\n",
    "        # Complete this with OpenAPI spec for routing REST to Vertex AI endpoint.\n",
    "\n",
    "# ============================================================================\n",
    "# 10. EXAMPLE FASTAPI ENDPOINT FOR ONLINE INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# Instantiate your pipeline globally for API\n",
    "pipeline = None  # Will be assigned in startup event\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    global pipeline\n",
    "    # Adjust config as needed for prod\n",
    "    config = VertexAIConfig(\n",
    "        project_id=\"your-production-project\",\n",
    "        location=\"asia-northeast1\",\n",
    "        staging_bucket=\"gs://your-production-bucket\"\n",
    "    )\n",
    "    pipeline = JapaneseRAGPipeline(config)\n",
    "\n",
    "@app.post(\"/rag\")\n",
    "async def rag_endpoint(query: QueryRequest):\n",
    "    \"\"\"REST API endpoint for traditional RAG\"\"\"\n",
    "    result = pipeline.query_rag(query.question)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/graph_rag\")\n",
    "async def graph_rag_endpoint(query: QueryRequest):\n",
    "    \"\"\"REST API endpoint for GraphRAG\"\"\"\n",
    "    result = pipeline.query_graph_rag(query.question)\n",
    "    return result\n",
    "\n",
    "# Example: run the API locally for testing\n",
    "# if __name__ == \"__main__\":\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# ============================================================================\n",
    "# 11. PRODUCTION SETUP & CHECKLIST (TIPS)\n",
    "# ============================================================================\n",
    "\n",
    "def production_setup():\n",
    "    \"\"\"\n",
    "    Example production setup for Japanese RAG/GraphRAG pipeline.\n",
    "    \"\"\"\n",
    "    config = VertexAIConfig(\n",
    "        project_id=\"your-production-project\",\n",
    "        location=\"asia-northeast1\",  # Tokyo region for Japanese content\n",
    "        staging_bucket=\"gs://your-production-bucket\"\n",
    "    )\n",
    "    # You may load Neo4j config from env/secret manager\n",
    "    neo4j_config = {\n",
    "        \"uri\": \"bolt://<YOUR_NEO4J_HOST>:7687\",\n",
    "        \"user\": \"neo4j\",\n",
    "        \"password\": \"<your-password>\"\n",
    "    }\n",
    "    pipeline = JapaneseRAGPipeline(config, neo4j_config)\n",
    "    return pipeline\n",
    "\n",
    "# ============================================================================\n",
    "# 12. INFERENCE FROM JUPYTER OR OTHER CLIENTS\n",
    "# ============================================================================\n",
    "\n",
    "import requests\n",
    "\n",
    "def query_api(question: str, endpoint_url: str = \"http://localhost:8000/rag\"):\n",
    "    \"\"\"Call REST API for RAG pipeline\"\"\"\n",
    "    resp = requests.post(endpoint_url, json={\"question\": question})\n",
    "    return resp.json()\n",
    "\n",
    "# ============================================================================\n",
    "# 13. NOTES FOR PRODUCTIONIZATION\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "- For **Vertex AI Endpoints**, youâ€™ll usually build a Docker image serving your FastAPI app (or Flask, etc) and deploy it as a custom model endpoint.  \n",
    "- For **RAG pipelines**, you might precompute and store vector indices, and load at startup for best performance.\n",
    "- Use **Vertex AI Vector Search** or **FAISS** (in-memory, for small scale) as vector backend.\n",
    "- Use **Cloud Scheduler**, **Pub/Sub**, or **Workflows** for retraining, batch jobs, or updating vector/graph indices.\n",
    "- For **API Gateway**, use OpenAPI spec to proxy public requests to Vertex AI endpoints.\n",
    "- Consider **GCP Secret Manager** for sensitive credentials (Neo4j, API keys).\n",
    "- Use **Google Cloud Monitoring** for log/metric aggregation, alerting.\n",
    "- For scale: use \"a2-highgpu-1g\" machines for LLM endpoints, \"n2-standard-16\" for vector/embedding-heavy workloads.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# END OF PIPELINE\n",
    "# ============================================================================\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
