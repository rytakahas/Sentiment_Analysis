{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Improved LoRA Fine-tuning Script for Causal Language Models\n",
    "Supports both instruction-following and conversation formats\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, List, Optional, Union\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import wandb\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Fine-tune language models with LoRA\")\n",
    "    \n",
    "    # Data arguments\n",
    "    parser.add_argument(\"--train_data\", type=str, required=True,\n",
    "                       help=\"Path to training data (JSON/JSONL)\")\n",
    "    parser.add_argument(\"--valid_data\", type=str, required=True,\n",
    "                       help=\"Path to validation data (JSON/JSONL)\")\n",
    "    \n",
    "    # Model arguments\n",
    "    parser.add_argument(\"--model\", type=str, required=True,\n",
    "                       help=\"Model name or path (e.g., microsoft/DialoGPT-medium)\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True,\n",
    "                       help=\"Output directory for model and checkpoints\")\n",
    "    \n",
    "    # Training arguments\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3,\n",
    "                       help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4,\n",
    "                       help=\"Training batch size per device\")\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=4,\n",
    "                       help=\"Evaluation batch size per device\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4,\n",
    "                       help=\"Learning rate\")\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100,\n",
    "                       help=\"Number of warmup steps\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512,\n",
    "                       help=\"Maximum sequence length\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1,\n",
    "                       help=\"Gradient accumulation steps\")\n",
    "    \n",
    "    # LoRA arguments\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=16,\n",
    "                       help=\"LoRA rank\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32,\n",
    "                       help=\"LoRA alpha parameter\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.1,\n",
    "                       help=\"LoRA dropout\")\n",
    "    parser.add_argument(\"--target_modules\", type=str, nargs=\"+\", \n",
    "                       default=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "                       help=\"Target modules for LoRA\")\n",
    "    \n",
    "    # Advanced arguments\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None,\n",
    "                       help=\"Resume training from checkpoint\")\n",
    "    parser.add_argument(\"--use_fp16\", action=\"store_true\", default=True,\n",
    "                       help=\"Use FP16 training\")\n",
    "    parser.add_argument(\"--use_8bit\", action=\"store_true\",\n",
    "                       help=\"Use 8-bit quantization\")\n",
    "    parser.add_argument(\"--gradient_checkpointing\", action=\"store_true\",\n",
    "                       help=\"Enable gradient checkpointing\")\n",
    "    parser.add_argument(\"--early_stopping_patience\", type=int, default=3,\n",
    "                       help=\"Early stopping patience\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=None,\n",
    "                       help=\"Weights & Biases project name\")\n",
    "    parser.add_argument(\"--data_format\", type=str, choices=[\"instruction\", \"conversation\"], \n",
    "                       default=\"instruction\",\n",
    "                       help=\"Data format: instruction (input/output) or conversation\")\n",
    "    parser.add_argument(\"--instruction_template\", type=str, \n",
    "                       default=\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\",\n",
    "                       help=\"Template for instruction format\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def setup_model_and_tokenizer(args):\n",
    "    \"\"\"Load and setup model and tokenizer\"\"\"\n",
    "    logger.info(f\"Loading model and tokenizer: {args.model}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model, \n",
    "        use_fast=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Add padding token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # Load model with quantization if specified\n",
    "    model_kwargs = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"torch_dtype\": torch.float16 if args.use_fp16 else torch.float32,\n",
    "    }\n",
    "    \n",
    "    if args.use_8bit:\n",
    "        model_kwargs.update({\n",
    "            \"load_in_8bit\": True,\n",
    "            \"device_map\": \"auto\",\n",
    "        })\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model, **model_kwargs)\n",
    "    \n",
    "    # Resize token embeddings if we added new tokens\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Prepare model for training\n",
    "    if args.use_8bit:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    if args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Setup LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=args.target_modules,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_and_preprocess_data(args, tokenizer):\n",
    "    \"\"\"Load and preprocess training data\"\"\"\n",
    "    logger.info(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Determine file format\n",
    "    file_format = \"json\" if args.train_data.endswith('.json') else \"text\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_ds = load_dataset(file_format, data_files=args.train_data, split=\"train\")\n",
    "    valid_ds = load_dataset(file_format, data_files=args.valid_data, split=\"train\")\n",
    "    \n",
    "    logger.info(f\"Training samples: {len(train_ds)}\")\n",
    "    logger.info(f\"Validation samples: {len(valid_ds)}\")\n",
    "    \n",
    "    def preprocess_instruction_format(examples):\n",
    "        \"\"\"Preprocess data in instruction format (input/output)\"\"\"\n",
    "        inputs = []\n",
    "        \n",
    "        for i in range(len(examples[\"input\"])):\n",
    "            instruction = examples[\"input\"][i]\n",
    "            response = examples[\"output\"][i]\n",
    "            \n",
    "            # Format using template\n",
    "            text = args.instruction_template.format(\n",
    "                instruction=instruction,\n",
    "                response=response\n",
    "            )\n",
    "            inputs.append(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=args.max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    def preprocess_conversation_format(examples):\n",
    "        \"\"\"Preprocess data in conversation format\"\"\"\n",
    "        inputs = []\n",
    "        \n",
    "        for conversation in examples[\"conversation\"]:\n",
    "            # Convert conversation to text\n",
    "            text = \"\"\n",
    "            for turn in conversation:\n",
    "                role = turn.get(\"role\", \"user\")\n",
    "                content = turn.get(\"content\", \"\")\n",
    "                text += f\"{role}: {content}\\n\"\n",
    "            \n",
    "            inputs.append(text.strip())\n",
    "        \n",
    "        # Tokenize\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=args.max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    # Choose preprocessing function based on data format\n",
    "    if args.data_format == \"instruction\":\n",
    "        preprocess_fn = preprocess_instruction_format\n",
    "    else:\n",
    "        preprocess_fn = preprocess_conversation_format\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    train_ds = train_ds.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_ds.column_names,\n",
    "        desc=\"Preprocessing training data\"\n",
    "    )\n",
    "    \n",
    "    valid_ds = valid_ds.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=valid_ds.column_names,\n",
    "        desc=\"Preprocessing validation data\"\n",
    "    )\n",
    "    \n",
    "    return train_ds, valid_ds\n",
    "\n",
    "def setup_training_arguments(args):\n",
    "    \"\"\"Setup training arguments\"\"\"\n",
    "    \n",
    "    # Initialize wandb if specified\n",
    "    if args.wandb_project:\n",
    "        wandb.init(project=args.wandb_project)\n",
    "        report_to = \"wandb\"\n",
    "    else:\n",
    "        report_to = None\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        \n",
    "        # Training parameters\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        \n",
    "        # Optimization\n",
    "        fp16=args.use_fp16,\n",
    "        optim=\"adamw_torch\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # Evaluation and saving\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        \n",
    "        # Logging\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        report_to=report_to,\n",
    "        \n",
    "        # Early stopping\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        \n",
    "        # Memory optimization\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with additional features\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"Custom loss computation\"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Shift labels for causal LM\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Calculate loss only on non-padded tokens\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    args = parse_arguments()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save arguments\n",
    "    with open(os.path.join(args.output_dir, \"training_args.json\"), \"w\") as f:\n",
    "        json.dump(vars(args), f, indent=2)\n",
    "    \n",
    "    # Setup model and tokenizer\n",
    "    model, tokenizer = setup_model_and_tokenizer(args)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_ds, valid_ds = load_and_preprocess_data(args, tokenizer)\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_arguments(args)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "        pad_to_multiple_of=8 if args.use_fp16 else None,\n",
    "    )\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.early_stopping_patience)]\n",
    "    )\n",
    "    \n",
    "    # Resume from checkpoint if specified\n",
    "    checkpoint = None\n",
    "    if args.resume_from_checkpoint:\n",
    "        checkpoint = args.resume_from_checkpoint\n",
    "    elif os.path.isdir(args.output_dir):\n",
    "        checkpoint = get_last_checkpoint(args.output_dir)\n",
    "    \n",
    "    if checkpoint:\n",
    "        logger.info(f\"Resuming training from {checkpoint}\")\n",
    "    \n",
    "    # Train the model\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    \n",
    "    # Save the final model\n",
    "    logger.info(\"Saving final model...\")\n",
    "    trainer.save_model()\n",
    "    trainer.save_state()\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "    # Save training metrics\n",
    "    if trainer.state.log_history:\n",
    "        with open(os.path.join(args.output_dir, \"training_log.json\"), \"w\") as f:\n",
    "            json.dump(trainer.state.log_history, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Training completed! Model saved to {args.output_dir}\")\n",
    "    \n",
    "    # Test the model with a sample\n",
    "    test_generation(model, tokenizer, args)\n",
    "\n",
    "def test_generation(model, tokenizer, args):\n",
    "    \"\"\"Test the trained model with sample generation\"\"\"\n",
    "    logger.info(\"Testing model generation...\")\n",
    "    \n",
    "    # Sample prompt\n",
    "    if args.data_format == \"instruction\":\n",
    "        prompt = \"### Instruction:\\nWhat is machine learning?\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = \"user: Hello, how are you?\\nassistant:\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    logger.info(f\"Generated text:\\n{generated_text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_lora.py \\\n",
    "  --train_data train.json \\\n",
    "  --valid_data valid.json \\\n",
    "  --model meta-llama/Llama-2-7b-hf \\\n",
    "  --output_dir ./llama-finetuned \\\n",
    "  --epochs 3 \\\n",
    "  --batch_size 2 \\\n",
    "  --use_8bit \\\n",
    "  --gradient_checkpointing \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 64 \\\n",
    "  --wandb_project my-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG\n",
    "\n",
    "# Install dependencies (run in Colab)\n",
    "!pip install -U google-cloud-aiplatform chromadb vertexai\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import chromadb\n",
    "\n",
    "# Initialize Vertex AI (replace with your project details)\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your actual project ID\n",
    "LOCATION = \"us-central1\"  # or your preferred location\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# 1. Prepare corpus and get embeddings\n",
    "texts = [\"これは最初の文書です。\", \"これは二番目の文書です。\"]\n",
    "EMBED_MODEL = \"text-multilingual-embedding-002\"\n",
    "\n",
    "try:\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(EMBED_MODEL)\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        embedding_result = embedding_model.get_embeddings([text])\n",
    "        doc_embeddings.append(embedding_result[0].values)\n",
    "    \n",
    "    print(f\"Generated {len(doc_embeddings)} document embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating embeddings: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Build vector database (Chroma)\n",
    "try:\n",
    "    chroma_client = chromadb.Client()\n",
    "    \n",
    "    # Delete collection if it exists (for testing)\n",
    "    try:\n",
    "        chroma_client.delete_collection(\"docs\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = chroma_client.create_collection(\"docs\")\n",
    "    \n",
    "    # Add documents with embeddings\n",
    "    for i, (text, emb) in enumerate(zip(texts, doc_embeddings)):\n",
    "        collection.add(\n",
    "            documents=[text], \n",
    "            embeddings=[emb], \n",
    "            ids=[str(i)]\n",
    "        )\n",
    "    \n",
    "    print(\"Vector database created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating vector database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 3. Process user query\n",
    "query = \"二番目の文書について教えて\"\n",
    "\n",
    "try:\n",
    "    # Get query embedding\n",
    "    query_embedding_result = embedding_model.get_embeddings([query])\n",
    "    query_emb = query_embedding_result[0].values\n",
    "    \n",
    "    # Search for similar documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_emb], \n",
    "        n_results=2\n",
    "    )\n",
    "    \n",
    "    if results['documents'] and len(results['documents'][0]) > 0:\n",
    "        top_context = results['documents'][0][0]  # Top document text\n",
    "        print(f\"Retrieved context: {top_context}\")\n",
    "    else:\n",
    "        print(\"No relevant documents found\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing query: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Generate response with Gemini\n",
    "try:\n",
    "    # Use the newer GenerativeModel class\n",
    "    model = GenerativeModel(\"gemini-1.5-pro\")\n",
    "    \n",
    "    prompt = f\"\"\"参考文書: {top_context}\n",
    "\n",
    "質問: {query}\n",
    "\n",
    "上記の参考文書に基づいて質問に答えてください。\n",
    "\n",
    "答え:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    print(f\"\\nFinal Response: {response.text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating response: {e}\")\n",
    "    \n",
    "    # Fallback: try with different model name\n",
    "    try:\n",
    "        model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        print(f\"\\nFinal Response (fallback): {response.text}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback also failed: {e2}\")\n",
    "        print(\"Please check your model access permissions and available models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GraphRAG\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, embedding_model=None, llm=None):\n",
    "        self.G = nx.Graph()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = llm\n",
    "    \n",
    "    def add_document(self, doc_id: int, text: str, embedding: np.ndarray = None):\n",
    "        \"\"\"Add a document node to the graph\"\"\"\n",
    "        if embedding is None and self.embedding_model:\n",
    "            # Generate embedding if not provided\n",
    "            embedding = self.embedding_model.get_embeddings([text])[0].values\n",
    "        \n",
    "        self.G.add_node(doc_id, text=text, embedding=embedding)\n",
    "    \n",
    "    def add_relation(self, doc_id1: int, doc_id2: int, relation_type: str = \"reference\"):\n",
    "        \"\"\"Add an edge between two documents\"\"\"\n",
    "        if doc_id1 in self.G.nodes and doc_id2 in self.G.nodes:\n",
    "            self.G.add_edge(doc_id1, doc_id2, type=relation_type)\n",
    "        else:\n",
    "            raise ValueError(f\"One or both document IDs ({doc_id1}, {doc_id2}) not found in graph\")\n",
    "    \n",
    "    def find_similar_nodes(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Find top-k most similar nodes to query embedding\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for node_id in self.G.nodes():\n",
    "            node_embedding = self.G.nodes[node_id]['embedding']\n",
    "            if node_embedding is not None:\n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity(\n",
    "                    query_embedding.reshape(1, -1), \n",
    "                    node_embedding.reshape(1, -1)\n",
    "                )[0][0]\n",
    "                similarities.append((node_id, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending) and return top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_subgraph_context(self, node_ids: List[int], max_hops: int = 1) -> str:\n",
    "        \"\"\"Get context from nodes and their neighbors within max_hops\"\"\"\n",
    "        context_nodes = set(node_ids)\n",
    "        \n",
    "        # Add neighbors within max_hops\n",
    "        for node_id in node_ids:\n",
    "            if node_id in self.G.nodes:\n",
    "                # Get neighbors within max_hops using BFS\n",
    "                neighbors = nx.single_source_shortest_path_length(\n",
    "                    self.G, node_id, cutoff=max_hops\n",
    "                )\n",
    "                context_nodes.update(neighbors.keys())\n",
    "        \n",
    "        # Collect text from all context nodes\n",
    "        context_texts = []\n",
    "        for node_id in context_nodes:\n",
    "            if node_id in self.G.nodes and 'text' in self.G.nodes[node_id]:\n",
    "                text = self.G.nodes[node_id]['text']\n",
    "                context_texts.append(f\"文書{node_id}: {text}\")\n",
    "        \n",
    "        return \"\\n\".join(context_texts)\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 3, max_hops: int = 1) -> str:\n",
    "        \"\"\"Main query method for GraphRAG\"\"\"\n",
    "        if not self.embedding_model or not self.llm:\n",
    "            raise ValueError(\"Both embedding_model and llm must be provided\")\n",
    "        \n",
    "        # 1. Get query embedding\n",
    "        try:\n",
    "            query_embedding = self.embedding_model.get_embeddings([query])[0].values\n",
    "        except AttributeError:\n",
    "            # Handle different embedding model interfaces\n",
    "            query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # 2. Find similar nodes\n",
    "        similar_nodes = self.find_similar_nodes(query_embedding, top_k)\n",
    "        \n",
    "        if not similar_nodes:\n",
    "            return \"関連する文書が見つかりませんでした。\"\n",
    "        \n",
    "        # 3. Get subgraph context\n",
    "        top_node_ids = [node_id for node_id, _ in similar_nodes]\n",
    "        context = self.get_subgraph_context(top_node_ids, max_hops)\n",
    "        \n",
    "        # 4. Generate response using LLM\n",
    "        prompt = f\"\"\"参考文書:\n",
    "{context}\n",
    "\n",
    "質問: {query}\n",
    "\n",
    "上記の参考文書の内容に基づいて、質問に答えてください。\n",
    "\n",
    "答え:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.predict(prompt)\n",
    "            return response.text if hasattr(response, 'text') else str(response)\n",
    "        except Exception as e:\n",
    "            return f\"LLMによる回答生成中にエラーが発生しました: {str(e)}\"\n",
    "\n",
    "# Usage example\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the GraphRAG class\"\"\"\n",
    "    \n",
    "    # Initialize GraphRAG\n",
    "    graph_rag = GraphRAG()  # You would pass your actual embedding_model and llm here\n",
    "    \n",
    "    # Sample embeddings (in practice, these would come from your embedding model)\n",
    "    doc_embeddings = [\n",
    "        np.random.rand(384),  # Example embedding dimension\n",
    "        np.random.rand(384),\n",
    "        np.random.rand(384)\n",
    "    ]\n",
    "    \n",
    "    # Add documents\n",
    "    graph_rag.add_document(0, \"これは最初の文書です。人工知能について説明しています。\", doc_embeddings[0])\n",
    "    graph_rag.add_document(1, \"これは二番目の文書です。機械学習について詳しく述べています。\", doc_embeddings[1])\n",
    "    graph_rag.add_document(2, \"これは三番目の文書です。深層学習の応用について書かれています。\", doc_embeddings[2])\n",
    "    \n",
    "    # Add relations\n",
    "    graph_rag.add_relation(0, 1, \"reference\")\n",
    "    graph_rag.add_relation(1, 2, \"related\")\n",
    "    \n",
    "    # Example query (you would need actual embedding_model and llm)\n",
    "    query = \"機械学習について教えてください\"\n",
    "    \n",
    "    # For demonstration without actual models:\n",
    "    print(\"GraphRAG setup completed!\")\n",
    "    print(f\"Graph has {graph_rag.G.number_of_nodes()} nodes and {graph_rag.G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Show graph structure\n",
    "    print(\"\\nGraph structure:\")\n",
    "    for node in graph_rag.G.nodes(data=True):\n",
    "        print(f\"Node {node[0]}: {node[1]['text'][:50]}...\")\n",
    "    \n",
    "    for edge in graph_rag.G.edges(data=True):\n",
    "        print(f\"Edge {edge[0]}-{edge[1]}: {edge[2]['type']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run in Colab)\n",
    "!pip install -U google-cloud-aiplatform chromadb vertexai\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import chromadb\n",
    "\n",
    "# Initialize Vertex AI (replace with your project details)\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your actual project ID\n",
    "LOCATION = \"us-central1\"  # or your preferred location\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# 1. Prepare corpus and get embeddings\n",
    "texts = [\"これは最初の文書です。\", \"これは二番目の文書です。\"]\n",
    "EMBED_MODEL = \"text-multilingual-embedding-002\"\n",
    "\n",
    "try:\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(EMBED_MODEL)\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        embedding_result = embedding_model.get_embeddings([text])\n",
    "        doc_embeddings.append(embedding_result[0].values)\n",
    "    \n",
    "    print(f\"Generated {len(doc_embeddings)} document embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating embeddings: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Build vector database (Chroma)\n",
    "try:\n",
    "    chroma_client = chromadb.Client()\n",
    "    \n",
    "    # Delete collection if it exists (for testing)\n",
    "    try:\n",
    "        chroma_client.delete_collection(\"docs\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = chroma_client.create_collection(\"docs\")\n",
    "    \n",
    "    # Add documents with embeddings\n",
    "    for i, (text, emb) in enumerate(zip(texts, doc_embeddings)):\n",
    "        collection.add(\n",
    "            documents=[text], \n",
    "            embeddings=[emb], \n",
    "            ids=[str(i)]\n",
    "        )\n",
    "    \n",
    "    print(\"Vector database created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating vector database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 3. Process user query\n",
    "query = \"二番目の文書について教えて\"\n",
    "\n",
    "try:\n",
    "    # Get query embedding\n",
    "    query_embedding_result = embedding_model.get_embeddings([query])\n",
    "    query_emb = query_embedding_result[0].values\n",
    "    \n",
    "    # Search for similar documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_emb], \n",
    "        n_results=2\n",
    "    )\n",
    "    \n",
    "    if results['documents'] and len(results['documents'][0]) > 0:\n",
    "        top_context = results['documents'][0][0]  # Top document text\n",
    "        print(f\"Retrieved context: {top_context}\")\n",
    "    else:\n",
    "        print(\"No relevant documents found\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing query: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Generate response with Gemini\n",
    "try:\n",
    "    # Use the newer GenerativeModel class\n",
    "    model = GenerativeModel(\"gemini-1.5-pro\")\n",
    "    \n",
    "    prompt = f\"\"\"参考文書: {top_context}\n",
    "\n",
    "質問: {query}\n",
    "\n",
    "上記の参考文書に基づいて質問に答えてください。\n",
    "\n",
    "答え:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    print(f\"\\nFinal Response: {response.text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating response: {e}\")\n",
    "    \n",
    "    # Fallback: try with different model name\n",
    "    try:\n",
    "        model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        print(f\"\\nFinal Response (fallback): {response.text}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback also failed: {e2}\")\n",
    "        print(\"Please check your model access permissions and available models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
