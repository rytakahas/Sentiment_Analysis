{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese ABSA Pipeline with MultiLLM + LoRA (no embedding extractor, LLM-only)\n",
    "# Requirements: torch, transformers, peft, datasets, matplotlib, seaborn, pandas, numpy, google-cloud-aiplatform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to 'pip install datasets'\")\n",
    "\n",
    "# Vertex AI imports (only for data generation)\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    project_id: str = \"your-project-id\"\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_splits: int = 5\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 256\n",
    "    model_candidates: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.model_candidates is None:\n",
    "            self.model_candidates = [\n",
    "                \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "                \"rinna/japanese-roberta-base\",\n",
    "                \"studio-ousia/luke-japanese-base-lite\",\n",
    "                \"xlm-roberta-base\"\n",
    "            ]\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    def _initialize_model(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        prompts = self._create_generation_prompts()\n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            try:\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt, generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        prompts = []\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        return prompts\n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        return samples\n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        manual_samples = [\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        return manual_samples\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {'negative': '#FF6B6B','neutral': '#FFD93D','positive': '#6BCF7F'}\n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        metrics = {}\n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "        return metrics\n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "class AdvancedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels=3, aux_tasks=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.main_head = nn.Linear(hidden_size, num_labels)\n",
    "        self.aux_heads = nn.ModuleDict()\n",
    "        if aux_tasks:\n",
    "            for name, classes in aux_tasks.items():\n",
    "                self.aux_heads[name] = nn.Linear(hidden_size, classes)\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        out = {'logits': self.main_head(pooled)}\n",
    "        for name, head in self.aux_heads.items():\n",
    "            out[name + '_logits'] = head(pooled)\n",
    "        return out\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(\"Generating training data...\")\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "\n",
    "    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(f\"Extracting aspect flags for {len(df)} samples...\")\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(pattern, na=False, regex=True).astype(int)\n",
    "        df['text_length'] = df['review_text'].str.len()\n",
    "        return df\n",
    "\n",
    "    def prepare_aux_labels(self, df: pd.DataFrame):\n",
    "        # (For demonstration: create aux labels. Replace with real data if available)\n",
    "        for emo in ['joy', 'sadness', 'anger']:\n",
    "            df[f\"high_{emo}\"] = np.random.randint(0, 2, size=len(df))\n",
    "        return df\n",
    "\n",
    "    def prepare_datasets(self, df, tokenizer):\n",
    "        def tokenize_function(examples):\n",
    "            tok = tokenizer(\n",
    "                examples['review_text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            )\n",
    "            tok['labels'] = examples['sentiment']\n",
    "            tok['joy_labels'] = examples['high_joy']\n",
    "            tok['sadness_labels'] = examples['high_sadness']\n",
    "            tok['anger_labels'] = examples['high_anger']\n",
    "            return tok\n",
    "        dataset = Dataset.from_pandas(df[['review_text', 'sentiment', 'high_joy', 'high_sadness', 'high_anger']])\n",
    "        dataset = dataset.map(tokenize_function, batched=True)\n",
    "        columns = ['input_ids', 'attention_mask', 'labels', 'joy_labels', 'sadness_labels', 'anger_labels']\n",
    "        dataset.set_format(type='torch', columns=columns)\n",
    "        return dataset\n",
    "\n",
    "    def setup_model_and_tokenizer(self, model_name, num_labels, aux_tasks=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        model = AdvancedMultiTaskModel(base_model, num_labels=num_labels, aux_tasks=aux_tasks)\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"]\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return tokenizer, model\n",
    "\n",
    "    def train_evaluate_multillm_lora(self, df):\n",
    "        # Prepare data and split\n",
    "        df = self.prepare_aux_labels(df)\n",
    "        train_df, test_df = train_test_split(df, test_size=self.model_config.test_size, stratify=df['sentiment'], random_state=42)\n",
    "        # Features/plots\n",
    "        self.insight_extractor.executive_summary(train_df)\n",
    "        self.insight_extractor.generate_business_recommendations(train_df)\n",
    "        num_labels = 3\n",
    "        aux_tasks = {'joy': 2, 'sadness': 2, 'anger': 2}\n",
    "        best_f1 = -1\n",
    "        best_result = None\n",
    "        for model_name in self.model_config.model_candidates:\n",
    "            print(f\"\\n===== Training {model_name} =====\")\n",
    "            tokenizer, model = self.setup_model_and_tokenizer(model_name, num_labels, aux_tasks)\n",
    "            train_dataset = self.prepare_datasets(train_df, tokenizer)\n",
    "            test_dataset = self.prepare_datasets(test_df, tokenizer)\n",
    "            class MultiTaskTrainer(Trainer):\n",
    "                def compute_loss(self, model, inputs, *args, **kwargs):\n",
    "                    labels = inputs.pop(\"labels\")\n",
    "                    joy_labels = inputs.pop(\"joy_labels\")\n",
    "                    sadness_labels = inputs.pop(\"sadness_labels\")\n",
    "                    anger_labels = inputs.pop(\"anger_labels\")\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = F.cross_entropy(outputs['logits'], labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['joy_logits'], joy_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['sadness_logits'], sadness_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['anger_logits'], anger_labels)\n",
    "                    return (loss, outputs) if kwargs.get('return_outputs', False) else loss\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                num_train_epochs=2,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                eval_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                learning_rate=1e-4,\n",
    "                logging_dir=\"./logs\",\n",
    "                logging_steps=100,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                report_to=[],\n",
    "                dataloader_num_workers=0,\n",
    "                max_grad_norm=1.0,\n",
    "                remove_unused_columns=False,\n",
    "                label_names=[\"labels\", \"joy_labels\", \"sadness_labels\", \"anger_labels\"],\n",
    "            )\n",
    "            data_collator = DataCollatorWithPadding(tokenizer, padding=True, max_length=128)\n",
    "            trainer = MultiTaskTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            trainer.train()\n",
    "            preds = trainer.predict(test_dataset)\n",
    "            y_pred = np.argmax(preds.predictions[0], axis=1)\n",
    "            y_true = test_df['sentiment'].values\n",
    "            report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "            if macro_f1 > best_f1:\n",
    "                best_f1 = macro_f1\n",
    "                best_result = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"trainer\": trainer,\n",
    "                    \"tokenizer\": tokenizer,\n",
    "                    \"model\": model,\n",
    "                    \"test_df\": test_df,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_true\": y_true\n",
    "                }\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"\\n===== Best model: {best_result['model_name']} (macro F1={best_f1:.4f}) =====\")\n",
    "        cm = confusion_matrix(best_result[\"y_true\"], best_result[\"y_pred\"])\n",
    "        ConfusionMatrixDisplay(cm, display_labels=[\"neg\", \"neu\", \"pos\"]).plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix ({best_result['model_name']})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        self.model = best_result['model']\n",
    "        self.tokenizer = best_result['tokenizer']\n",
    "        self.trainer = best_result['trainer']\n",
    "        return best_result\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df)\n",
    "        best_result = self.train_evaluate_multillm_lora(df)\n",
    "        print(\"\\nPipeline finished! Best LLM+LoRA saved in memory.\")\n",
    "\n",
    "# --------------- Main Entrypoint ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"able-balm-454718-n8\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        use_gpu=True\n",
    "    )\n",
    "    model_config = ModelConfig()\n",
    "    aspect_config = AspectConfig()\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    pipeline.pipeline(num_samples=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese ABSA Pipeline with MultiLLM + LoRA + RAG (with all business logic, plots, and analytics preserved)\n",
    "# Requirements: torch, transformers, peft, datasets, matplotlib, seaborn, pandas, numpy, google-cloud-aiplatform, sentence-transformers, faiss-cpu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to 'pip install datasets'\")\n",
    "\n",
    "# RAG imports\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to 'pip install sentence-transformers faiss-cpu'\")\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    project_id: str = \"your-project-id\"\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "    model_candidates: list = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_candidates is None:\n",
    "            self.model_candidates = [\n",
    "                \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "                \"rinna/japanese-roberta-base\",\n",
    "                \"studio-ousia/luke-japanese-base-lite\",\n",
    "                \"xlm-roberta-base\"\n",
    "            ]\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "# ==========================\n",
    "# Embedding & Data Generation\n",
    "# ==========================\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(self.model_config.vertex_embedding_model)\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                embeddings = []\n",
    "                batch_size = 5\n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                return self.embedding_model.encode(\n",
    "                    texts, show_progress_bar=True, batch_size=self.model_config.batch_size, normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    def _initialize_model(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        prompts = self._create_generation_prompts()\n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            try:\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt, generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        prompts = []\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        return prompts\n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        return samples\n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        manual_samples = [\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        return manual_samples\n",
    "\n",
    "# =============================\n",
    "# Business Insight & Analytics\n",
    "# =============================\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {'negative': '#FF6B6B','neutral': '#FFD93D','positive': '#6BCF7F'}\n",
    "\n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        metrics = {}\n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "\n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "# ===================================\n",
    "# LLM+LoRA advanced model and pipeline\n",
    "# ===================================\n",
    "\n",
    "class AdvancedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels=3, aux_tasks=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.main_head = nn.Linear(hidden_size, num_labels)\n",
    "        self.aux_heads = nn.ModuleDict()\n",
    "        if aux_tasks:\n",
    "            for name, classes in aux_tasks.items():\n",
    "                self.aux_heads[name] = nn.Linear(hidden_size, classes)\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        out = {'logits': self.main_head(pooled)}\n",
    "        for name, head in self.aux_heads.items():\n",
    "            out[name + '_logits'] = head(pooled)\n",
    "        return out\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(vertex_config, model_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(\"Generating training data...\")\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "\n",
    "    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(f\"Extracting features for {len(df)} samples...\")\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(pattern, na=False, regex=True).astype(int)\n",
    "        df['text_length'] = df['review_text'].str.len()\n",
    "        return df\n",
    "\n",
    "    def prepare_aux_labels(self, df: pd.DataFrame):\n",
    "        # (For demonstration: create aux labels. Replace with real data if available)\n",
    "        for emo in ['joy', 'sadness', 'anger']:\n",
    "            df[f\"high_{emo}\"] = np.random.randint(0, 2, size=len(df))\n",
    "        return df\n",
    "\n",
    "    def prepare_datasets(self, df, tokenizer):\n",
    "        def tokenize_function(examples):\n",
    "            tok = tokenizer(\n",
    "                examples['review_text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            )\n",
    "            tok['labels'] = examples['sentiment']\n",
    "            tok['joy_labels'] = examples['high_joy']\n",
    "            tok['sadness_labels'] = examples['high_sadness']\n",
    "            tok['anger_labels'] = examples['high_anger']\n",
    "            return tok\n",
    "        dataset = Dataset.from_pandas(df[['review_text', 'sentiment', 'high_joy', 'high_sadness', 'high_anger']])\n",
    "        dataset = dataset.map(tokenize_function, batched=True)\n",
    "        columns = ['input_ids', 'attention_mask', 'labels', 'joy_labels', 'sadness_labels', 'anger_labels']\n",
    "        dataset.set_format(type='torch', columns=columns)\n",
    "        return dataset\n",
    "\n",
    "    def setup_model_and_tokenizer(self, model_name, num_labels, aux_tasks=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        model = AdvancedMultiTaskModel(base_model, num_labels=num_labels, aux_tasks=aux_tasks)\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"]\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return tokenizer, model\n",
    "\n",
    "    def train_evaluate_multillm_lora(self, df):\n",
    "        df = self.prepare_aux_labels(df)\n",
    "        train_df, test_df = train_test_split(df, test_size=self.model_config.test_size, stratify=df['sentiment'], random_state=42)\n",
    "        self.insight_extractor.executive_summary(train_df)\n",
    "        self.insight_extractor.generate_business_recommendations(train_df)\n",
    "        num_labels = 3\n",
    "        aux_tasks = {'joy': 2, 'sadness': 2, 'anger': 2}\n",
    "        best_f1 = -1\n",
    "        best_result = None\n",
    "        for model_name in self.model_config.model_candidates:\n",
    "            print(f\"\\n===== Training {model_name} =====\")\n",
    "            tokenizer, model = self.setup_model_and_tokenizer(model_name, num_labels, aux_tasks)\n",
    "            train_dataset = self.prepare_datasets(train_df, tokenizer)\n",
    "            test_dataset = self.prepare_datasets(test_df, tokenizer)\n",
    "            class MultiTaskTrainer(Trainer):\n",
    "                def compute_loss(self, model, inputs, *args, **kwargs):\n",
    "                    labels = inputs.pop(\"labels\")\n",
    "                    joy_labels = inputs.pop(\"joy_labels\")\n",
    "                    sadness_labels = inputs.pop(\"sadness_labels\")\n",
    "                    anger_labels = inputs.pop(\"anger_labels\")\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = F.cross_entropy(outputs['logits'], labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['joy_logits'], joy_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['sadness_logits'], sadness_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['anger_logits'], anger_labels)\n",
    "                    return (loss, outputs) if kwargs.get('return_outputs', False) else loss\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                num_train_epochs=2,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                eval_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                learning_rate=1e-4,\n",
    "                logging_dir=\"./logs\",\n",
    "                logging_steps=100,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                report_to=[],\n",
    "                dataloader_num_workers=0,\n",
    "                max_grad_norm=1.0,\n",
    "                remove_unused_columns=False,\n",
    "                label_names=[\"labels\", \"joy_labels\", \"sadness_labels\", \"anger_labels\"],\n",
    "            )\n",
    "            data_collator = DataCollatorWithPadding(tokenizer, padding=True, max_length=128)\n",
    "            trainer = MultiTaskTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            trainer.train()\n",
    "            preds = trainer.predict(test_dataset)\n",
    "            y_pred = np.argmax(preds.predictions[0], axis=1)\n",
    "            y_true = test_df['sentiment'].values\n",
    "            report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "            if macro_f1 > best_f1:\n",
    "                best_f1 = macro_f1\n",
    "                best_result = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"trainer\": trainer,\n",
    "                    \"tokenizer\": tokenizer,\n",
    "                    \"model\": model,\n",
    "                    \"test_df\": test_df,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_true\": y_true\n",
    "                }\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"\\n===== Best model: {best_result['model_name']} (macro F1={best_f1:.4f}) =====\")\n",
    "        cm = confusion_matrix(best_result[\"y_true\"], best_result[\"y_pred\"])\n",
    "        ConfusionMatrixDisplay(cm, display_labels=[\"neg\", \"neu\", \"pos\"]).plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix ({best_result['model_name']})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        self.model = best_result['model']\n",
    "        self.tokenizer = best_result['tokenizer']\n",
    "        self.trainer = best_result['trainer']\n",
    "        self.df_train = train_df # <- for RAG\n",
    "        return best_result\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df)\n",
    "        best_result = self.train_evaluate_multillm_lora(df)\n",
    "        print(\"\\nPipeline finished! Best LLM+LoRA saved in memory.\")\n",
    "        return best_result\n",
    "\n",
    "# ----------------- RAG Inference -----------------\n",
    "class RAGInference:\n",
    "    \"\"\" Retrieval-Augmented Generation for ABSA sentiment inference \"\"\"\n",
    "    def __init__(\n",
    "        self, model, tokenizer, df_corpus, embedding_extractor, top_k=3\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_corpus = df_corpus.reset_index(drop=True)\n",
    "        self.top_k = top_k\n",
    "        self.embedding_extractor = embedding_extractor\n",
    "        print(\"Building RAG vector index from corpus...\")\n",
    "        corpus_embeddings = self.embedding_extractor.get_embeddings(list(self.df_corpus['review_text']))\n",
    "        self.index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
    "        self.index.add(corpus_embeddings.astype(np.float32))\n",
    "        self.corpus_embeddings = corpus_embeddings\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        q_emb = self.embedding_extractor.get_embeddings([query]).astype(np.float32)\n",
    "        D, I = self.index.search(q_emb, self.top_k)\n",
    "        return [self.df_corpus.iloc[i]['review_text'] for i in I[0]]\n",
    "\n",
    "    def predict(self, text):\n",
    "        retrieved = self.retrieve(text)\n",
    "        context = \" \".join(retrieved)\n",
    "        input_text = f\"{text} [CONTEXT] {context}\"\n",
    "        tokens = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens = {k: v.to(next(self.model.parameters()).device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokens)['logits']\n",
    "        pred = logits.argmax(dim=-1).item()\n",
    "        return pred, retrieved\n",
    "\n",
    "# --------------- Main Entrypoint ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"able-balm-454718-n8\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        use_gpu=True\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    pipeline.pipeline(num_samples=1200)\n",
    "\n",
    "    # ---- RAG inference (example) ----\n",
    "    print(\"\\n--- Initializing RAG inference wrapper with vector DB embeddings ---\")\n",
    "    rag = RAGInference(\n",
    "        pipeline.model, pipeline.tokenizer,\n",
    "        pipeline.df_train, pipeline.embedding_extractor, top_k=3\n",
    "    )\n",
    "\n",
    "    example_text = \"ã“ã®å•†å“ã¯ã¨ã¦ã‚‚ä¾¿åˆ©ã§ã™ãŒã€å“è³ªãŒå°‘ã—å¿ƒé…ã§ã™ã€‚\"\n",
    "    pred, retrieved = rag.predict(example_text)\n",
    "    print(f\"\\nRAG Sentiment prediction: {['negative','neutral','positive'][pred]}\")\n",
    "    print(f\"Retrieved context:\\n- \" + \"\\n- \".join(retrieved))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese ABSA Pipeline with Vertex AI + MultiLLM + LoRA + RAG (Cloud-Ready)\n",
    "# Requirements: torch, transformers, peft, datasets, matplotlib, seaborn, pandas, numpy,\n",
    "# google-cloud-aiplatform, sentence-transformers, faiss-cpu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    project_id: str = \"your-project-id\"\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "    model_candidates: list = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_candidates is None:\n",
    "            self.model_candidates = [\n",
    "                \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "                \"rinna/japanese-roberta-base\",\n",
    "                \"studio-ousia/luke-japanese-base-lite\",\n",
    "                \"xlm-roberta-base\"\n",
    "            ]\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "# ==========================\n",
    "# Embedding & Data Generation\n",
    "# ==========================\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(self.model_config.vertex_embedding_model)\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                embeddings = []\n",
    "                batch_size = 5\n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                return self.embedding_model.encode(\n",
    "                    texts, show_progress_bar=True, batch_size=self.model_config.batch_size, normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    def _initialize_model(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        prompts = self._create_generation_prompts()\n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            try:\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt, generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        prompts = []\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        return prompts\n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        return samples\n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        manual_samples = [\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        return manual_samples\n",
    "\n",
    "# =============================\n",
    "# Business Insight & Analytics\n",
    "# =============================\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {'negative': '#FF6B6B','neutral': '#FFD93D','positive': '#6BCF7F'}\n",
    "\n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        metrics = {}\n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "\n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "# ===================================\n",
    "# LLM+LoRA advanced model and pipeline\n",
    "# ===================================\n",
    "\n",
    "class AdvancedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels=3, aux_tasks=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.main_head = nn.Linear(hidden_size, num_labels)\n",
    "        self.aux_heads = nn.ModuleDict()\n",
    "        if aux_tasks:\n",
    "            for name, classes in aux_tasks.items():\n",
    "                self.aux_heads[name] = nn.Linear(hidden_size, classes)\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        out = {'logits': self.main_head(pooled)}\n",
    "        for name, head in self.aux_heads.items():\n",
    "            out[name + '_logits'] = head(pooled)\n",
    "        return out\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(vertex_config, model_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(\"Generating training data...\")\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "\n",
    "    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(f\"Extracting features for {len(df)} samples...\")\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(pattern, na=False, regex=True).astype(int)\n",
    "        df['text_length'] = df['review_text'].str.len()\n",
    "        return df\n",
    "\n",
    "    def prepare_aux_labels(self, df: pd.DataFrame):\n",
    "        for emo in ['joy', 'sadness', 'anger']:\n",
    "            df[f\"high_{emo}\"] = np.random.randint(0, 2, size=len(df))\n",
    "        return df\n",
    "\n",
    "    def prepare_datasets(self, df, tokenizer):\n",
    "        def tokenize_function(examples):\n",
    "            tok = tokenizer(\n",
    "                examples['review_text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            )\n",
    "            tok['labels'] = examples['sentiment']\n",
    "            tok['joy_labels'] = examples['high_joy']\n",
    "            tok['sadness_labels'] = examples['high_sadness']\n",
    "            tok['anger_labels'] = examples['high_anger']\n",
    "            return tok\n",
    "        dataset = Dataset.from_pandas(df[['review_text', 'sentiment', 'high_joy', 'high_sadness', 'high_anger']])\n",
    "        dataset = dataset.map(tokenize_function, batched=True)\n",
    "        columns = ['input_ids', 'attention_mask', 'labels', 'joy_labels', 'sadness_labels', 'anger_labels']\n",
    "        dataset.set_format(type='torch', columns=columns)\n",
    "        return dataset\n",
    "\n",
    "    def setup_model_and_tokenizer(self, model_name, num_labels, aux_tasks=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        model = AdvancedMultiTaskModel(base_model, num_labels=num_labels, aux_tasks=aux_tasks)\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"]\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return tokenizer, model\n",
    "\n",
    "    def train_evaluate_multillm_lora(self, df):\n",
    "        df = self.prepare_aux_labels(df)\n",
    "        train_df, test_df = train_test_split(df, test_size=self.model_config.test_size, stratify=df['sentiment'], random_state=42)\n",
    "        self.insight_extractor.executive_summary(train_df)\n",
    "        self.insight_extractor.generate_business_recommendations(train_df)\n",
    "        num_labels = 3\n",
    "        aux_tasks = {'joy': 2, 'sadness': 2, 'anger': 2}\n",
    "        best_f1 = -1\n",
    "        best_result = None\n",
    "        for model_name in self.model_config.model_candidates:\n",
    "            print(f\"\\n===== Training {model_name} =====\")\n",
    "            tokenizer, model = self.setup_model_and_tokenizer(model_name, num_labels, aux_tasks)\n",
    "            train_dataset = self.prepare_datasets(train_df, tokenizer)\n",
    "            test_dataset = self.prepare_datasets(test_df, tokenizer)\n",
    "            class MultiTaskTrainer(Trainer):\n",
    "                def compute_loss(self, model, inputs, *args, **kwargs):\n",
    "                    labels = inputs.pop(\"labels\")\n",
    "                    joy_labels = inputs.pop(\"joy_labels\")\n",
    "                    sadness_labels = inputs.pop(\"sadness_labels\")\n",
    "                    anger_labels = inputs.pop(\"anger_labels\")\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = F.cross_entropy(outputs['logits'], labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['joy_logits'], joy_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['sadness_logits'], sadness_labels)\n",
    "                    loss += 0.2 * F.cross_entropy(outputs['anger_logits'], anger_labels)\n",
    "                    return (loss, outputs) if kwargs.get('return_outputs', False) else loss\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                num_train_epochs=2,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                learning_rate=1e-4,\n",
    "                logging_dir=\"./logs\",\n",
    "                logging_steps=100,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                report_to=[],\n",
    "                dataloader_num_workers=0,\n",
    "                max_grad_norm=1.0,\n",
    "                remove_unused_columns=False,\n",
    "                label_names=[\"labels\", \"joy_labels\", \"sadness_labels\", \"anger_labels\"],\n",
    "            )\n",
    "            data_collator = DataCollatorWithPadding(tokenizer, padding=True, max_length=128)\n",
    "            trainer = MultiTaskTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            trainer.train()\n",
    "            preds = trainer.predict(test_dataset)\n",
    "            y_pred = np.argmax(preds.predictions[0], axis=1)\n",
    "            y_true = test_df['sentiment'].values\n",
    "            report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "            if macro_f1 > best_f1:\n",
    "                best_f1 = macro_f1\n",
    "                best_result = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"trainer\": trainer,\n",
    "                    \"tokenizer\": tokenizer,\n",
    "                    \"model\": model,\n",
    "                    \"test_df\": test_df,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"y_true\": y_true\n",
    "                }\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"\\n===== Best model: {best_result['model_name']} (macro F1={best_f1:.4f}) =====\")\n",
    "        cm = confusion_matrix(best_result[\"y_true\"], best_result[\"y_pred\"])\n",
    "        ConfusionMatrixDisplay(cm, display_labels=[\"neg\", \"neu\", \"pos\"]).plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix ({best_result['model_name']})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        self.model = best_result['model']\n",
    "        self.tokenizer = best_result['tokenizer']\n",
    "        self.trainer = best_result['trainer']\n",
    "        self.df_train = train_df # <- for RAG\n",
    "        return best_result\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df)\n",
    "        best_result = self.train_evaluate_multillm_lora(df)\n",
    "        print(\"\\nPipeline finished! Best LLM+LoRA saved in memory.\")\n",
    "        return best_result\n",
    "\n",
    "# ----------------- RAG Inference -----------------\n",
    "class RAGInference:\n",
    "    \"\"\" Retrieval-Augmented Generation for ABSA sentiment inference \"\"\"\n",
    "    def __init__(\n",
    "        self, model, tokenizer, df_corpus, embedding_extractor, top_k=3\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_corpus = df_corpus.reset_index(drop=True)\n",
    "        self.top_k = top_k\n",
    "        self.embedding_extractor = embedding_extractor\n",
    "        print(\"Building RAG vector index from corpus...\")\n",
    "        corpus_embeddings = self.embedding_extractor.get_embeddings(list(self.df_corpus['review_text']))\n",
    "        self.index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
    "        self.index.add(corpus_embeddings.astype(np.float32))\n",
    "        self.corpus_embeddings = corpus_embeddings\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        q_emb = self.embedding_extractor.get_embeddings([query]).astype(np.float32)\n",
    "        D, I = self.index.search(q_emb, self.top_k)\n",
    "        return [self.df_corpus.iloc[i]['review_text'] for i in I[0]]\n",
    "\n",
    "    def predict(self, text):\n",
    "        retrieved = self.retrieve(text)\n",
    "        context = \" \".join(retrieved)\n",
    "        input_text = f\"{text} [CONTEXT] {context}\"\n",
    "        tokens = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens = {k: v.to(next(self.model.parameters()).device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokens)['logits']\n",
    "        pred = logits.argmax(dim=-1).item()\n",
    "        return pred, retrieved\n",
    "\n",
    "# --------------- Main Entrypoint ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Hardware: Vertex AI/Cloud/Local (config controls everything)\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"your-gcp-project\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        use_gpu=True,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "        accelerator_count=1,\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    pipeline.pipeline(num_samples=1200)\n",
    "\n",
    "    # ---- RAG inference (example) ----\n",
    "    print(\"\\n--- Initializing RAG inference wrapper with vector DB embeddings ---\")\n",
    "    rag = RAGInference(\n",
    "        pipeline.model, pipeline.tokenizer,\n",
    "        pipeline.df_train, pipeline.embedding_extractor, top_k=3\n",
    "    )\n",
    "\n",
    "    example_text = \"ã“ã®å•†å“ã¯ã¨ã¦ã‚‚ä¾¿åˆ©ã§ã™ãŒã€å“è³ªãŒå°‘ã—å¿ƒé…ã§ã™ã€‚\"\n",
    "    pred, retrieved = rag.predict(example_text)\n",
    "    print(f\"\\nRAG Sentiment prediction: {['negative','neutral','positive'][pred]}\")\n",
    "    print(f\"Retrieved context:\\n- \" + \"\\n- \".join(retrieved))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ffc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# All imports & dataclasses\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Vertex AI\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# ========== Logging & Styles ==========\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    project_id: str = \"your-project-id\"\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "    model_candidates: list = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_candidates is None:\n",
    "            self.model_candidates = [\n",
    "                \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "                \"rinna/japanese-roberta-base\",\n",
    "                \"studio-ousia/luke-japanese-base-lite\",\n",
    "                \"xlm-roberta-base\"\n",
    "            ]\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "# =============================================\n",
    "# Embedding & Data Generation (same as before)\n",
    "# =============================================\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(self.model_config.vertex_embedding_model)\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                embeddings = []\n",
    "                batch_size = 5\n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                return self.embedding_model.encode(\n",
    "                    texts, show_progress_bar=True, batch_size=self.model_config.batch_size, normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    def _initialize_model(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        prompts = self._create_generation_prompts()\n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            try:\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt, generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        prompts = []\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        return prompts\n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        return samples\n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        manual_samples = [\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        return manual_samples\n",
    "\n",
    "# =======================\n",
    "# Analytics/Plotting/etc.\n",
    "# (Same as before, not pasted for brevity)\n",
    "# ...Keep your EnhancedBusinessInsightExtractor here\n",
    "# =======================\n",
    "\n",
    "# =======================\n",
    "# LoRA MultiLLM pipeline\n",
    "# (Same as before, not pasted for brevity)\n",
    "# ...Keep AdvancedMultiTaskModel & VertexAIJapaneseABSAPipeline here\n",
    "# =======================\n",
    "\n",
    "# =======================\n",
    "# RAG Retriever\n",
    "# (Same as before, not pasted for brevity)\n",
    "# ...Keep your RAGInference here\n",
    "# =======================\n",
    "\n",
    "# ======================================\n",
    "# Gemini/PaLM2 RAG Pipeline Class\n",
    "# ======================================\n",
    "\n",
    "class GeminiRAGReranker:\n",
    "    def __init__(self, retriever, gcp_project, gcp_location, model_name=\"gemini-pro\"):\n",
    "        vertexai.init(project=gcp_project, location=gcp_location)\n",
    "        self.model = GenerativeModel(model_name)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def prompt(self, user_text, context_texts):\n",
    "        ctx = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(context_texts)])\n",
    "        prompt = f\"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼: {user_text}\n",
    "å‚è€ƒãƒ¬ãƒ“ãƒ¥ãƒ¼:\n",
    "{ctx}\n",
    "-----\n",
    "ã“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã‹ã‚‰åˆ†é¡žã—ã€ãã®ç†ç”±ã‚‚æ—¥æœ¬èªžã§ç°¡æ½”ã«èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚\n",
    "å‡ºåŠ›ä¾‹: \n",
    "Sentiment: ãƒã‚¸ãƒ†ã‚£ãƒ–\n",
    "Reason: ...ç†ç”±...\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def predict(self, user_text, top_k=3):\n",
    "        retrieved = self.retriever.retrieve(user_text, top_k=top_k)\n",
    "        prompt = self.prompt(user_text, retrieved)\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text, retrieved\n",
    "\n",
    "# ================\n",
    "# Main Entrypoint\n",
    "# ================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train local LoRA model as before\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"your-gcp-project\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        use_gpu=True,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "        accelerator_count=1,\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    pipeline.pipeline(num_samples=1200)\n",
    "\n",
    "    print(\"\\n--- Initializing RAG inference with vector DB embeddings (local LoRA) ---\")\n",
    "    rag = RAGInference(\n",
    "        pipeline.model, pipeline.tokenizer,\n",
    "        pipeline.df_train, pipeline.embedding_extractor, top_k=3\n",
    "    )\n",
    "\n",
    "    example_text = \"ã“ã®å•†å“ã¯ã¨ã¦ã‚‚ä¾¿åˆ©ã§ã™ãŒã€å“è³ªãŒå°‘ã—å¿ƒé…ã§ã™ã€‚\"\n",
    "    pred, retrieved = rag.predict(example_text)\n",
    "    print(f\"\\n[Local LoRA RAG] Sentiment prediction: {['negative','neutral','positive'][pred]}\")\n",
    "    print(f\"Retrieved context:\\n- \" + \"\\n- \".join(retrieved))\n",
    "\n",
    "    # 2. RAG+Gemini pipeline (NO local model required)\n",
    "    print(\"\\n--- Initializing Gemini/VertexAI LLM RAG inference ---\")\n",
    "    # Use the same retriever as above (just needs .retrieve)\n",
    "    gemini_rag = GeminiRAGReranker(\n",
    "        retriever=rag,\n",
    "        gcp_project=vertex_config.project_id,\n",
    "        gcp_location=vertex_config.location,\n",
    "        model_name=\"gemini-pro\"  # or \"chat-bison\"\n",
    "    )\n",
    "    gemini_output, gemini_retrieved = gemini_rag.predict(example_text, top_k=3)\n",
    "    print(\"\\n[Gemini RAG] Response:\")\n",
    "    print(gemini_output)\n",
    "    print(\"Retrieved context:\")\n",
    "    for c in gemini_retrieved:\n",
    "        print(\"-\", c)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
