{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese Aspect-Based Sentiment Analysis Pipeline with Vertex AI\n",
    "# Requirements: google-cloud-aiplatform, sentence-transformers, xgboost, optuna, scikit-learn, matplotlib, seaborn, pandas, numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Traditional ML imports\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    \"\"\"Configuration for Vertex AI\"\"\"\n",
    "    project_id: str = \"your-project-id\"  # Replace with your GCP project ID\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"  # Replace with your bucket\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None  # Optional service account\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the ABSA model\"\"\"\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    optuna_trials: int = 30\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    \"\"\"Configuration for aspect categories\"\"\"\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    \"\"\"Vertex AI embedding extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    \n",
    "    def _initialize_vertex_ai(self):\n",
    "        \"\"\"Initialize Vertex AI\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "                self.model_config.vertex_embedding_model\n",
    "            )\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            logger.info(\"Falling back to SentenceTransformer\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    \n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings from Vertex AI or fallback model\"\"\"\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                # Use Vertex AI embeddings\n",
    "                embeddings = []\n",
    "                batch_size = 5  # Vertex AI has rate limits\n",
    "                \n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                \n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                # Use SentenceTransformer as fallback\n",
    "                return self.embedding_model.encode(\n",
    "                    texts,\n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=self.model_config.batch_size,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    \"\"\"Generate training data using Vertex AI's generative models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize generative model\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Generate training data using Vertex AI\"\"\"\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        \n",
    "        # Create prompts for different aspects and sentiments\n",
    "        prompts = self._create_generation_prompts()\n",
    "        \n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate text using Vertex AI\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Parse response and create samples\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        \n",
    "        # Add some manual examples to ensure quality\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        \n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    \n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        \"\"\"Create prompts for data generation\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        # Create prompts for each aspect and sentiment combination\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        \"\"\"Parse generated response into structured data\"\"\"\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        \n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                \n",
    "                # If we have all three components, create a sample\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    # Map sentiment to numeric\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    \n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    \n",
    "                    # Reset for next sample\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    \n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        \"\"\"Create high-quality manual samples\"\"\"\n",
    "        manual_samples = [\n",
    "            # Quality - Positive\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            \n",
    "            # Quality - Negative\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            \n",
    "            # Service - Positive\n",
    "            {'review_text': 'ã‚¹ã‚¿ãƒƒãƒ•ã®å¯¾å¿œãŒç´ æ™´ã‚‰ã—ã‹ã£ãŸã§ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'è¦ªåˆ‡ã§ä¸å¯§ãªæŽ¥å®¢ã«æ„Ÿè¬ã—ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ãŒè‰¯ãã¦æ°—æŒã¡ã‚ˆãåˆ©ç”¨ã§ãã¾ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            \n",
    "            # Service - Negative\n",
    "            {'review_text': 'åº—å“¡ã®æ…‹åº¦ãŒæ‚ªãã¦ä¸å¿«ã§ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ã®è³ªãŒä½Žãã¦æ®‹å¿µã§ã™ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'æŽ¥å®¢ãŒé›‘ã§äºŒåº¦ã¨æ¥ãŸãã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            \n",
    "            # Price - Positive\n",
    "            {'review_text': 'ä¾¡æ ¼ãŒå®‰ãã¦ãŠå¾—æ„ŸãŒã‚ã‚Šã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãŒè‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'é©æ­£ä¾¡æ ¼ã§æº€è¶³ã—ã¦ã„ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            \n",
    "            # Price - Negative\n",
    "            {'review_text': 'å€¤æ®µãŒé«˜ã™ãŽã¦æ‰‹ãŒå‡ºã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ä¾¡æ ¼è¨­å®šãŒä¸é©åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãŒé«˜ãã¦ç¶šã‘ã‚‰ã‚Œã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            \n",
    "            # Neutral samples\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        \n",
    "        # Add text_length and generated flag\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        \n",
    "        return manual_samples\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    \"\"\"Enhanced business insight extractor with better analytics\"\"\"\n",
    "    \n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {\n",
    "            'negative': '#FF6B6B',\n",
    "            'neutral': '#FFD93D', \n",
    "            'positive': '#6BCF7F'\n",
    "        }\n",
    "        \n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate detailed metrics for each aspect\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            \n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate actionable business recommendations\"\"\"\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Sort aspects by priority (negative rate * mentions)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "            \n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            \n",
    "            # Generate specific recommendations\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "            \n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "    \n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate executive summary with key metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        \n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)  # Scale for display\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        \n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    \"\"\"Vertex AI Japanese ABSA Pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(vertex_config, model_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "        \n",
    "        # Model components\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.endpoint = None\n",
    "        \n",
    "        # Initialize Vertex AI\n",
    "        self._initialize_vertex_ai()\n",
    "    \n",
    "    def _initialize_vertex_ai(self):\n",
    "        \"\"\"Initialize Vertex AI platform\"\"\"\n",
    "        try:\n",
    "            aiplatform.init(\n",
    "                project=self.vertex_config.project_id,\n",
    "                location=self.vertex_config.location,\n",
    "                staging_bucket=self.vertex_config.staging_bucket\n",
    "            )\n",
    "            logger.info(\"Vertex AI initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Vertex AI: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Generate training data\"\"\"\n",
    "        logger.info(\"Generating training data...\")\n",
    "        \n",
    "        # Generate data using Vertex AI\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        \n",
    "        # Clean and validate data\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        \n",
    "        # Add additional features\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        \n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    \n",
    "    def extract_features(self, df: pd.DataFrame, fit_scaler: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Extract features from text data\"\"\"\n",
    "        logger.info(f\"Extracting features for {len(df)} samples...\")\n",
    "        \n",
    "        # Extract aspect features\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(\n",
    "                pattern, na=False, regex=True\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Extract embeddings using Vertex AI\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        embeddings = self.embedding_extractor.get_embeddings(df['review_text'].tolist())\n",
    "        \n",
    "        # Add embedding features\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        for i in range(embedding_dim):\n",
    "            df[f'emb_{i}'] = embeddings[:, i]\n",
    "        \n",
    "        # Scale embeddings\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.fit_transform(df[emb_cols])\n",
    "        elif self.scaler is not None:\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.transform(df[emb_cols])\n",
    "        \n",
    "        logger.info(\"Feature extraction completed\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"Prepare features for training\"\"\"\n",
    "        feature_cols = [\n",
    "            col for col in df.columns \n",
    "            if col.startswith(('aspect_', 'emb_')) or col in ['text_length', 'word_count', 'exclamation_count', 'question_count']\n",
    "        ]\n",
    "        \n",
    "        X = df[feature_cols].values\n",
    "        y = df['sentiment'].values\n",
    "        \n",
    "        logger.info(f\"Feature matrix shape: {X.shape}\")\n",
    "        logger.info(f\"Target distribution: {np.bincount(y)}\")\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            # XGBoost parameters\n",
    "            params = {\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": 3,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "                \"use_label_encoder\": False,\n",
    "                \"verbosity\": 0,\n",
    "                \"random_state\": self.model_config.random_state\n",
    "            }\n",
    "            \n",
    "            # Cross-validation\n",
    "            skf = StratifiedKFold(n_splits=self.model_config.n_splits, shuffle=True, \n",
    "                                 random_state=self.model_config.random_state)\n",
    "            \n",
    "            scores = []\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Train model\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predict and evaluate\n",
    "                y_pred = model.predict(X_val)\n",
    "                f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                scores.append(f1)\n",
    "            \n",
    "            return np.mean(scores)\n",
    "        \n",
    "        # Run optimization\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=self.model_config.optuna_trials)\n",
    "        \n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "        logger.info(f\"Best params: {study.best_params}\")\n",
    "        \n",
    "        return study.best_params, study\n",
    "    \n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray, params: Dict[str, Any]) -> xgb.XGBClassifier:\n",
    "        \"\"\"Train XGBoost model\"\"\"\n",
    "        logger.info(\"Training XGBoost model...\")\n",
    "        \n",
    "        # Update parameters\n",
    "        model_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"use_label_encoder\": False,\n",
    "            \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "            \"random_state\": self.model_config.random_state,\n",
    "            **params\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        self.model = xgb.XGBClassifier(**model_params)\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        logger.info(\"Model training completed\")\n",
    "        return self.model\n",
    "    \n",
    "    def save_model_to_gcs(self, model_path: str = None) -> str:\n",
    "        \"\"\"Save model to Google Cloud Storage\"\"\"\n",
    "        if model_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_path = f\"absa_model_{timestamp}.pkl\"\n",
    "        \n",
    "        # Create model artifact\n",
    "        model_artifact = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'vertex_config': self.vertex_config,\n",
    "            'model_config': self.model_config,\n",
    "            'aspect_config': self.aspect_config\n",
    "        }\n",
    "        \n",
    "        # Save to local file first\n",
    "        local_path = f\"/tmp/{model_path}\"\n",
    "        with open(local_path, 'wb') as f:\n",
    "            pickle.dump(model_artifact, f)\n",
    "        \n",
    "        # Upload to GCS\n",
    "        bucket_name = self.vertex_config.staging_bucket.replace('gs://', '')\n",
    "        storage_client = storage.Client(project=self.vertex_config.project_id)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(f\"absa_models/{model_path}\")\n",
    "        blob.upload_from_filename(local_path)\n",
    "        logger.info(f\"Model saved to GCS: gs://{bucket_name}/absa_models/{model_path}\")\n",
    "        return f\"gs://{bucket_name}/absa_models/{model_path}\"\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray, feature_columns: List[str]):\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        cr = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "        logger.info(\"Classification report:\\n\" + classification_report(y_test, y_pred, digits=3))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/tmp/absa_cm.png\")\n",
    "        plt.close()\n",
    "        logger.info(\"Confusion matrix saved as /tmp/absa_cm.png\")\n",
    "\n",
    "        # Feature importance\n",
    "        importances = self.model.feature_importances_\n",
    "        fi = pd.Series(importances, index=feature_columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=fi.values[:20], y=fi.index[:20], palette='husl')\n",
    "        plt.title(\"Top 20 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/tmp/absa_feature_importance.png\")\n",
    "        plt.close()\n",
    "        logger.info(\"Feature importance plot saved as /tmp/absa_feature_importance.png\")\n",
    "\n",
    "        # SHAP\n",
    "        try:\n",
    "            import shap\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test, feature_names=feature_columns, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"/tmp/absa_shap_summary.png\")\n",
    "            plt.close()\n",
    "            logger.info(\"SHAP summary plot saved as /tmp/absa_shap_summary.png\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SHAP failed: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"classification_report\": cr,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"feature_importance\": fi,\n",
    "            \"plots\": {\n",
    "                \"cm\": \"/tmp/absa_cm.png\",\n",
    "                \"feature_importance\": \"/tmp/absa_feature_importance.png\",\n",
    "                \"shap_summary\": \"/tmp/absa_shap_summary.png\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        # 1. Data Generation\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df, fit_scaler=True)\n",
    "        X, y, feature_columns = self.prepare_features(df)\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "        # 2. Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.model_config.test_size, stratify=y, random_state=self.model_config.random_state\n",
    "        )\n",
    "\n",
    "        # 3. Hyperparameter tuning\n",
    "        best_params, study = self.optimize_hyperparameters(X_train, y_train)\n",
    "\n",
    "        # 4. Final training\n",
    "        self.train_model(X_train, y_train, best_params)\n",
    "\n",
    "        # 5. Evaluation\n",
    "        results = self.evaluate(X_test, y_test, feature_columns)\n",
    "\n",
    "        # 6. Business Insights\n",
    "        df_test = df.iloc[y_test.index] if hasattr(y_test, \"index\") else df.iloc[:len(y_test)]\n",
    "        df_test = df_test.copy()\n",
    "        df_test['predicted_sentiment'] = self.model.predict(X_test)\n",
    "        self.insight_extractor.executive_summary(df_test)\n",
    "        self.insight_extractor.generate_business_recommendations(df_test)\n",
    "\n",
    "        # 7. Save model\n",
    "        model_uri = self.save_model_to_gcs()\n",
    "\n",
    "        return {\n",
    "            \"model_uri\": model_uri,\n",
    "            \"results\": results,\n",
    "            \"optuna_study\": study\n",
    "        }\n",
    "\n",
    "\n",
    "# --------------- Main Entrypoint ---------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ==== Configure (customize these before running!) ====\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"able-balm-454718-n8\",  # YOUR GCP PROJECT\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",  # YOUR BUCKET\n",
    "        use_gpu=True\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        optuna_trials=30,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    result = pipeline.pipeline(num_samples=10_000)\n",
    "\n",
    "    print(\"\\n\\n=========================\")\n",
    "    print(\"Pipeline finished!\")\n",
    "    print(f\"Model saved at: {result['model_uri']}\")\n",
    "    print(\"=========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese Aspect-Based Sentiment Analysis Pipeline with Vertex AI\n",
    "# Requirements: google-cloud-aiplatform, sentence-transformers, xgboost, optuna, scikit-learn, matplotlib, seaborn, pandas, numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Traditional ML imports\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    \"\"\"Configuration for Vertex AI\"\"\"\n",
    "    project_id: str = \"your-project-id\"  # Replace with your GCP project ID\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"  # Replace with your bucket\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None  # Optional service account\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the ABSA model\"\"\"\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    optuna_trials: int = 30\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    \"\"\"Configuration for aspect categories\"\"\"\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    \"\"\"Vertex AI embedding extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    \n",
    "    def _initialize_vertex_ai(self):\n",
    "        \"\"\"Initialize Vertex AI\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "                self.model_config.vertex_embedding_model\n",
    "            )\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            logger.info(\"Falling back to SentenceTransformer\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    \n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings from Vertex AI or fallback model\"\"\"\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                # Use Vertex AI embeddings\n",
    "                embeddings = []\n",
    "                batch_size = 5  # Vertex AI has rate limits\n",
    "                \n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                \n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                # Use SentenceTransformer as fallback\n",
    "                return self.embedding_model.encode(\n",
    "                    texts,\n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=self.model_config.batch_size,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    \"\"\"Generate training data using Vertex AI's generative models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize generative model\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Generate training data using Vertex AI\"\"\"\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        \n",
    "        # Create prompts for different aspects and sentiments\n",
    "        prompts = self._create_generation_prompts()\n",
    "        \n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate text using Vertex AI\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Parse response and create samples\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        \n",
    "        # Add some manual examples to ensure quality\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        \n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    \n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        \"\"\"Create prompts for data generation\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        # Create prompts for each aspect and sentiment combination\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        \"\"\"Parse generated response into structured data\"\"\"\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        \n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                \n",
    "                # If we have all three components, create a sample\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    # Map sentiment to numeric\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    \n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    \n",
    "                    # Reset for next sample\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    \n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        \"\"\"Create high-quality manual samples\"\"\"\n",
    "        manual_samples = [\n",
    "            # Quality - Positive\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            \n",
    "            # Quality - Negative\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            \n",
    "            # Service - Positive\n",
    "            {'review_text': 'ã‚¹ã‚¿ãƒƒãƒ•ã®å¯¾å¿œãŒç´ æ™´ã‚‰ã—ã‹ã£ãŸã§ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'è¦ªåˆ‡ã§ä¸å¯§ãªæŽ¥å®¢ã«æ„Ÿè¬ã—ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ãŒè‰¯ãã¦æ°—æŒã¡ã‚ˆãåˆ©ç”¨ã§ãã¾ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            \n",
    "            # Service - Negative\n",
    "            {'review_text': 'åº—å“¡ã®æ…‹åº¦ãŒæ‚ªãã¦ä¸å¿«ã§ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ã®è³ªãŒä½Žãã¦æ®‹å¿µã§ã™ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'æŽ¥å®¢ãŒé›‘ã§äºŒåº¦ã¨æ¥ãŸãã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            \n",
    "            # Price - Positive\n",
    "            {'review_text': 'ä¾¡æ ¼ãŒå®‰ãã¦ãŠå¾—æ„ŸãŒã‚ã‚Šã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãŒè‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'é©æ­£ä¾¡æ ¼ã§æº€è¶³ã—ã¦ã„ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            \n",
    "            # Price - Negative\n",
    "            {'review_text': 'å€¤æ®µãŒé«˜ã™ãŽã¦æ‰‹ãŒå‡ºã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ä¾¡æ ¼è¨­å®šãŒä¸é©åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãŒé«˜ãã¦ç¶šã‘ã‚‰ã‚Œã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            \n",
    "            # Neutral samples\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        \n",
    "        # Add text_length and generated flag\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        \n",
    "        return manual_samples\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    \"\"\"Enhanced business insight extractor with better analytics\"\"\"\n",
    "    \n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {\n",
    "            'negative': '#FF6B6B',\n",
    "            'neutral': '#FFD93D', \n",
    "            'positive': '#6BCF7F'\n",
    "        }\n",
    "        \n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate detailed metrics for each aspect\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            \n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate actionable business recommendations\"\"\"\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Sort aspects by priority (negative rate * mentions)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "            \n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            \n",
    "            # Generate specific recommendations\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "            \n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "    \n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate executive summary with key metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        \n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)  # Scale for display\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        \n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    \"\"\"Vertex AI Japanese ABSA Pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "\n",
    "        # Initialize components\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(vertex_config, model_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "\n",
    "        # Model components\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.endpoint = None\n",
    "\n",
    "        # Initialize Vertex AI\n",
    "        self._initialize_vertex_ai()\n",
    "\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            aiplatform.init(\n",
    "                project=self.vertex_config.project_id,\n",
    "                location=self.vertex_config.location,\n",
    "                staging_bucket=self.vertex_config.staging_bucket\n",
    "            )\n",
    "            logger.info(\"Vertex AI initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Vertex AI: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(\"Generating training data...\")\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "\n",
    "    def extract_features(self, df: pd.DataFrame, fit_scaler: bool = False) -> pd.DataFrame:\n",
    "        logger.info(f\"Extracting features for {len(df)} samples...\")\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(pattern, na=False, regex=True).astype(int)\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        embeddings = self.embedding_extractor.get_embeddings(df['review_text'].tolist())\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        for i in range(embedding_dim):\n",
    "            df[f'emb_{i}'] = embeddings[:, i]\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.fit_transform(df[emb_cols])\n",
    "        elif self.scaler is not None:\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.transform(df[emb_cols])\n",
    "        logger.info(\"Feature extraction completed\")\n",
    "        return df\n",
    "\n",
    "    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        feature_cols = [\n",
    "            col for col in df.columns\n",
    "            if col.startswith(('aspect_', 'emb_')) or col in ['text_length', 'word_count', 'exclamation_count', 'question_count']\n",
    "        ]\n",
    "        X = df[feature_cols].values\n",
    "        y = df['sentiment'].values\n",
    "        logger.info(f\"Feature matrix shape: {X.shape}\")\n",
    "        logger.info(f\"Target distribution: {np.bincount(y)}\")\n",
    "        return X, y, feature_cols\n",
    "\n",
    "    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": 3,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "                \"use_label_encoder\": False,\n",
    "                \"verbosity\": 0,\n",
    "                \"random_state\": self.model_config.random_state\n",
    "            }\n",
    "            skf = StratifiedKFold(n_splits=self.model_config.n_splits, shuffle=True,\n",
    "                                  random_state=self.model_config.random_state)\n",
    "            scores = []\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                scores.append(f1)\n",
    "            return np.mean(scores)\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=self.model_config.optuna_trials)\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "        logger.info(f\"Best params: {study.best_params}\")\n",
    "        return study.best_params, study\n",
    "\n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray, params: Dict[str, Any]) -> xgb.XGBClassifier:\n",
    "        logger.info(\"Training XGBoost model...\")\n",
    "        model_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"use_label_encoder\": False,\n",
    "            \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "            \"random_state\": self.model_config.random_state,\n",
    "            **params\n",
    "        }\n",
    "        self.model = xgb.XGBClassifier(**model_params)\n",
    "        self.model.fit(X, y)\n",
    "        logger.info(\"Model training completed\")\n",
    "        return self.model\n",
    "\n",
    "    def save_model_to_gcs(self, model_path: str = None) -> str:\n",
    "        if model_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_path = f\"absa_model_{timestamp}.pkl\"\n",
    "        model_artifact = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'vertex_config': self.vertex_config,\n",
    "            'model_config': self.model_config,\n",
    "            'aspect_config': self.aspect_config\n",
    "        }\n",
    "        local_path = f\"/tmp/{model_path}\"\n",
    "        with open(local_path, 'wb') as f:\n",
    "            pickle.dump(model_artifact, f)\n",
    "        bucket_name = self.vertex_config.staging_bucket.replace('gs://', '')\n",
    "        storage_client = storage.Client(project=self.vertex_config.project_id)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(f\"absa_models/{model_path}\")\n",
    "        blob.upload_from_filename(local_path)\n",
    "        logger.info(f\"Model saved to GCS: gs://{bucket_name}/absa_models/{model_path}\")\n",
    "        return f\"gs://{bucket_name}/absa_models/{model_path}\"\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray, feature_columns: List[str]):\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)\n",
    "        cr = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "        logger.info(\"Classification report:\\n\" + classification_report(y_test, y_pred, digits=3))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/tmp/absa_cm.png\")\n",
    "        plt.close()\n",
    "        logger.info(\"Confusion matrix saved as /tmp/absa_cm.png\")\n",
    "        importances = self.model.feature_importances_\n",
    "        fi = pd.Series(importances, index=feature_columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=fi.values[:20], y=fi.index[:20], palette='husl')\n",
    "        plt.title(\"Top 20 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/tmp/absa_feature_importance.png\")\n",
    "        plt.close()\n",
    "        logger.info(\"Feature importance plot saved as /tmp/absa_feature_importance.png\")\n",
    "        try:\n",
    "            import shap\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test, feature_names=feature_columns, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"/tmp/absa_shap_summary.png\")\n",
    "            plt.close()\n",
    "            logger.info(\"SHAP summary plot saved as /tmp/absa_shap_summary.png\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SHAP failed: {e}\")\n",
    "        return {\n",
    "            \"classification_report\": cr,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"feature_importance\": fi,\n",
    "            \"plots\": {\n",
    "                \"cm\": \"/tmp/absa_cm.png\",\n",
    "                \"feature_importance\": \"/tmp/absa_feature_importance.png\",\n",
    "                \"shap_summary\": \"/tmp/absa_shap_summary.png\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def cross_validate(self, X, y, feature_columns, n_splits=5):\n",
    "        print(\"\\n==========================\")\n",
    "        print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
    "        print(\"==========================\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=self.model_config.random_state)\n",
    "        all_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                num_class=3,\n",
    "                use_label_encoder=False,\n",
    "                tree_method='gpu_hist' if self.vertex_config.use_gpu else 'auto',\n",
    "                random_state=self.model_config.random_state\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            report = classification_report(y_val, y_pred, digits=3)\n",
    "            print(f\"\\n--- Fold {fold} ---\")\n",
    "            print(f\"Weighted F1: {f1:.4f}  |  Accuracy: {acc:.4f}\")\n",
    "            print(report)\n",
    "            all_scores.append((acc, f1))\n",
    "        mean_acc = np.mean([x[0] for x in all_scores])\n",
    "        mean_f1 = np.mean([x[1] for x in all_scores])\n",
    "        print(\"\\n==========================\")\n",
    "        print(f\"CV Mean Accuracy: {mean_acc:.4f}\")\n",
    "        print(f\"CV Mean Weighted F1: {mean_f1:.4f}\")\n",
    "        print(\"==========================\\n\")\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        # 1. Data Generation\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df, fit_scaler=True)\n",
    "        X, y, feature_columns = self.prepare_features(df)\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "        # 2. Cross Validation (CV=5)\n",
    "        self.cross_validate(X, y, feature_columns, n_splits=self.model_config.n_splits)\n",
    "\n",
    "        # 3. Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.model_config.test_size, stratify=y, random_state=self.model_config.random_state\n",
    "        )\n",
    "\n",
    "        # 4. Hyperparameter tuning\n",
    "        best_params, study = self.optimize_hyperparameters(X_train, y_train)\n",
    "\n",
    "        # 5. Final training\n",
    "        self.train_model(X_train, y_train, best_params)\n",
    "\n",
    "        # 6. Evaluation\n",
    "        results = self.evaluate(X_test, y_test, feature_columns)\n",
    "\n",
    "        # 7. Business Insights\n",
    "        df_test = df.iloc[y_test.index] if hasattr(y_test, \"index\") else df.iloc[:len(y_test)]\n",
    "        df_test = df_test.copy()\n",
    "        df_test['predicted_sentiment'] = self.model.predict(X_test)\n",
    "        self.insight_extractor.executive_summary(df_test)\n",
    "        self.insight_extractor.generate_business_recommendations(df_test)\n",
    "\n",
    "        # 8. Save model\n",
    "        model_uri = self.save_model_to_gcs()\n",
    "\n",
    "        return {\n",
    "            \"model_uri\": model_uri,\n",
    "            \"results\": results,\n",
    "            \"optuna_study\": study\n",
    "        }\n",
    "\n",
    "# --------------- Main Entrypoint ---------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ==== Configure (customize these before running!) ====\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"able-balm-454718-n8\",  # YOUR GCP PROJECT\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",  # YOUR BUCKET\n",
    "        use_gpu=True\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        optuna_trials=30,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    result = pipeline.pipeline(num_samples=1200)\n",
    "\n",
    "    print(\"\\n\\n=========================\")\n",
    "    print(\"Pipeline finished!\")\n",
    "    print(f\"Model saved at: {result['model_uri']}\")\n",
    "    print(\"=========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese Aspect-Based Sentiment Analysis Pipeline with Vertex AI Endpoints\n",
    "# Requirements: google-cloud-aiplatform, xgboost, optuna, sentence-transformers, scikit-learn, matplotlib, seaborn, pandas, numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic import EndpointServiceClient\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Plotting and logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    project_id: str = \"your-project-id\"\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    optuna_trials: int = 30\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "                self.model_config.vertex_embedding_model\n",
    "            )\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            logger.info(\"Falling back to SentenceTransformer\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    def get_embeddings(self, texts: list) -> np.ndarray:\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                embeddings = []\n",
    "                batch_size = 5\n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                return self.embedding_model.encode(\n",
    "                    texts,\n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=self.model_config.batch_size,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    def _initialize_model(self):\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        prompts = self._create_generation_prompts()\n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            try:\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    def _create_generation_prompts(self) -> list:\n",
    "        prompts = []\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        return prompts\n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> list:\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        return samples\n",
    "    def _create_manual_samples(self) -> list:\n",
    "        manual_samples = [\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        return manual_samples\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {\n",
    "            'negative': '#FF6B6B',\n",
    "            'neutral': '#FFD93D', \n",
    "            'positive': '#6BCF7F'\n",
    "        }\n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        metrics = {}\n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "        return metrics\n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "class VertexAIJapaneseABSAPipeline:\n",
    "    def __init__(self, vertex_config: VertexAIConfig, model_config: ModelConfig, aspect_config: AspectConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model_config = model_config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.data_generator = VertexAIDataGenerator(vertex_config, aspect_config)\n",
    "        self.embedding_extractor = VertexAIEmbeddingExtractor(vertex_config, model_config)\n",
    "        self.insight_extractor = EnhancedBusinessInsightExtractor(aspect_config)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.endpoint = None\n",
    "        self._initialize_vertex_ai()\n",
    "\n",
    "    def _initialize_vertex_ai(self):\n",
    "        try:\n",
    "            aiplatform.init(\n",
    "                project=self.vertex_config.project_id,\n",
    "                location=self.vertex_config.location,\n",
    "                staging_bucket=self.vertex_config.staging_bucket\n",
    "            )\n",
    "            logger.info(\"Vertex AI initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Vertex AI: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        logger.info(\"Generating training data...\")\n",
    "        df = self.data_generator.generate_training_data(num_samples)\n",
    "        df = df.dropna(subset=['review_text'])\n",
    "        df = df[df['review_text'].str.len() > 5]\n",
    "        df = df[df['review_text'].str.len() <= self.model_config.max_text_length]\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "\n",
    "    def extract_features(self, df: pd.DataFrame, fit_scaler: bool = False) -> pd.DataFrame:\n",
    "        logger.info(f\"Extracting features for {len(df)} samples...\")\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(pattern, na=False, regex=True).astype(int)\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        embeddings = self.embedding_extractor.get_embeddings(df['review_text'].tolist())\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        for i in range(embedding_dim):\n",
    "            df[f'emb_{i}'] = embeddings[:, i]\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.fit_transform(df[emb_cols])\n",
    "        elif self.scaler is not None:\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.transform(df[emb_cols])\n",
    "        logger.info(\"Feature extraction completed\")\n",
    "        return df\n",
    "\n",
    "    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, list]:\n",
    "        feature_cols = [\n",
    "            col for col in df.columns\n",
    "            if col.startswith(('aspect_', 'emb_')) or col in ['text_length', 'word_count', 'exclamation_count', 'question_count']\n",
    "        ]\n",
    "        X = df[feature_cols].values\n",
    "        y = df['sentiment'].values\n",
    "        logger.info(f\"Feature matrix shape: {X.shape}\")\n",
    "        logger.info(f\"Target distribution: {np.bincount(y)}\")\n",
    "        return X, y, feature_cols\n",
    "\n",
    "    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray) -> Tuple[dict, Any]:\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": 3,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "                \"use_label_encoder\": False,\n",
    "                \"verbosity\": 0,\n",
    "                \"random_state\": self.model_config.random_state\n",
    "            }\n",
    "            skf = StratifiedKFold(n_splits=self.model_config.n_splits, shuffle=True,\n",
    "                                  random_state=self.model_config.random_state)\n",
    "            scores = []\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                scores.append(f1)\n",
    "            return np.mean(scores)\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=self.model_config.optuna_trials)\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "        logger.info(f\"Best params: {study.best_params}\")\n",
    "        return study.best_params, study\n",
    "\n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray, params: dict) -> xgb.XGBClassifier:\n",
    "        logger.info(\"Training XGBoost model...\")\n",
    "        model_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"use_label_encoder\": False,\n",
    "            \"tree_method\": \"gpu_hist\" if self.vertex_config.use_gpu else \"auto\",\n",
    "            \"random_state\": self.model_config.random_state,\n",
    "            **params\n",
    "        }\n",
    "        self.model = xgb.XGBClassifier(**model_params)\n",
    "        self.model.fit(X, y)\n",
    "        logger.info(\"Model training completed\")\n",
    "        return self.model\n",
    "\n",
    "    def save_model_to_gcs(self, model_path: str = None) -> str:\n",
    "        if model_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_path = f\"absa_model_{timestamp}.pkl\"\n",
    "        model_artifact = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'vertex_config': self.vertex_config,\n",
    "            'model_config': self.model_config,\n",
    "            'aspect_config': self.aspect_config\n",
    "        }\n",
    "        local_path = f\"/tmp/{model_path}\"\n",
    "        with open(local_path, 'wb') as f:\n",
    "            pickle.dump(model_artifact, f)\n",
    "        bucket_name = self.vertex_config.staging_bucket.replace('gs://', '')\n",
    "        storage_client = storage.Client(project=self.vertex_config.project_id)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(f\"absa_models/{model_path}\")\n",
    "        blob.upload_from_filename(local_path)\n",
    "        logger.info(f\"Model saved to GCS: gs://{bucket_name}/absa_models/{model_path}\")\n",
    "        return f\"gs://{bucket_name}/absa_models/{model_path}\"\n",
    "\n",
    "    def deploy_model_to_vertex_ai(self, model_uri: str) -> str:\n",
    "        display_name = self.vertex_config.model_display_name + \"-\" + datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "        # Container for XGBoost 1.7+ (prebuilt container)\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=display_name,\n",
    "            artifact_uri=model_uri,\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\",\n",
    "            project=self.vertex_config.project_id,\n",
    "            location=self.vertex_config.location,\n",
    "            sync=True\n",
    "        )\n",
    "        logger.info(f\"Model registered in Vertex AI: {model.resource_name}\")\n",
    "        endpoint = model.deploy(\n",
    "            machine_type=self.vertex_config.machine_type,\n",
    "            accelerator_type=self.vertex_config.accelerator_type if self.vertex_config.use_gpu else None,\n",
    "            accelerator_count=self.vertex_config.accelerator_count if self.vertex_config.use_gpu else None,\n",
    "            traffic_split={\"0\": 100}\n",
    "        )\n",
    "        logger.info(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "        self.endpoint = endpoint\n",
    "        return endpoint.resource_name\n",
    "\n",
    "    def predict_with_endpoint(self, endpoint_name: str, instances: np.ndarray, feature_columns: list) -> np.ndarray:\n",
    "        endpoint = aiplatform.Endpoint(endpoint_name)\n",
    "        instance_dicts = [\n",
    "            {col: float(val) for col, val in zip(feature_columns, row)}\n",
    "            for row in instances\n",
    "        ]\n",
    "        predictions = endpoint.predict(instances=instance_dicts)\n",
    "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray, feature_columns: list, use_endpoint: bool = False, endpoint_name: str = None):\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        if use_endpoint and endpoint_name is not None:\n",
    "            y_pred = self.predict_with_endpoint(endpoint_name, X_test, feature_columns)\n",
    "        else:\n",
    "            y_pred = self.model.predict(X_test)\n",
    "        cr = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "        logger.info(\"Classification report:\\n\" + classification_report(y_test, y_pred, digits=3))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive']).plot(cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/tmp/absa_cm.png\")\n",
    "        plt.close()\n",
    "        return {\n",
    "            \"classification_report\": cr,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"plots\": {\n",
    "                \"cm\": \"/tmp/absa_cm.png\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def pipeline(self, num_samples: int = 1000):\n",
    "        df = self.generate_training_data(num_samples)\n",
    "        df = self.extract_features(df, fit_scaler=True)\n",
    "        X, y, feature_columns = self.prepare_features(df)\n",
    "        self.feature_columns = feature_columns\n",
    "        self.insight_extractor.executive_summary(df)\n",
    "        self.insight_extractor.generate_business_recommendations(df)\n",
    "\n",
    "        # 2. Cross-validation\n",
    "        self.cross_validate(X, y, feature_columns, n_splits=self.model_config.n_splits)\n",
    "\n",
    "        # 3. Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.model_config.test_size, stratify=y, random_state=self.model_config.random_state\n",
    "        )\n",
    "\n",
    "        # 4. Hyperparameter tuning\n",
    "        best_params, study = self.optimize_hyperparameters(X_train, y_train)\n",
    "\n",
    "        # 5. Final training\n",
    "        self.train_model(X_train, y_train, best_params)\n",
    "\n",
    "        # 6. Save model\n",
    "        model_uri = self.save_model_to_gcs()\n",
    "\n",
    "        # 7. Deploy to Vertex AI endpoint\n",
    "        endpoint_name = self.deploy_model_to_vertex_ai(model_uri)\n",
    "\n",
    "        # 8. Evaluate via endpoint\n",
    "        results = self.evaluate(X_test, y_test, feature_columns, use_endpoint=True, endpoint_name=endpoint_name)\n",
    "\n",
    "        return {\n",
    "            \"model_uri\": model_uri,\n",
    "            \"endpoint_name\": endpoint_name,\n",
    "            \"results\": results,\n",
    "            \"optuna_study\": study\n",
    "        }\n",
    "\n",
    "    def cross_validate(self, X, y, feature_columns, n_splits=5):\n",
    "        print(\"\\n==========================\")\n",
    "        print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
    "        print(\"==========================\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=self.model_config.random_state)\n",
    "        all_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                num_class=3,\n",
    "                use_label_encoder=False,\n",
    "                tree_method='gpu_hist' if self.vertex_config.use_gpu else 'auto',\n",
    "                random_state=self.model_config.random_state\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            report = classification_report(y_val, y_pred, digits=3)\n",
    "            print(f\"\\n--- Fold {fold} ---\")\n",
    "            print(f\"Weighted F1: {f1:.4f}  |  Accuracy: {acc:.4f}\")\n",
    "            print(report)\n",
    "            all_scores.append((acc, f1))\n",
    "        mean_acc = np.mean([x[0] for x in all_scores])\n",
    "        mean_f1 = np.mean([x[1] for x in all_scores])\n",
    "        print(\"\\n==========================\")\n",
    "        print(f\"CV Mean Accuracy: {mean_acc:.4f}\")\n",
    "        print(f\"CV Mean Weighted F1: {mean_f1:.4f}\")\n",
    "        print(\"==========================\\n\")\n",
    "\n",
    "# --------- Entrypoint ---------\n",
    "if __name__ == \"__main__\":\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"able-balm-454718-n8\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        use_gpu=True\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        n_splits=5,\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "        optuna_trials=30,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        batch_size=64,\n",
    "        max_text_length=256,\n",
    "        vertex_embedding_model=\"textembedding-gecko-multilingual@001\"\n",
    "    )\n",
    "    aspect_config = AspectConfig()\n",
    "    pipeline = VertexAIJapaneseABSAPipeline(vertex_config, model_config, aspect_config)\n",
    "    result = pipeline.pipeline(num_samples=1200)\n",
    "    print(\"\\n\\n=========================\")\n",
    "    print(\"Pipeline finished!\")\n",
    "    print(f\"Model saved at: {result['model_uri']}\")\n",
    "    print(f\"Model endpoint deployed at: {result['endpoint_name']}\")\n",
    "    print(\"=========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87241b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Japanese Aspect-Based Sentiment Analysis Pipeline with Vertex AI Deployment\n",
    "# Requirements: google-cloud-aiplatform, sentence-transformers, xgboost, optuna, scikit-learn, matplotlib, seaborn, pandas, numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import pickle\n",
    "import tempfile\n",
    "import joblib\n",
    "\n",
    "# Vertex AI imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Traditional ML imports\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "@dataclass\n",
    "class VertexAIConfig:\n",
    "    \"\"\"Configuration for Vertex AI\"\"\"\n",
    "    project_id: str = \"your-project-id\"  # Replace with your GCP project ID\n",
    "    location: str = \"us-central1\"\n",
    "    staging_bucket: str = \"gs://your-bucket-name\"  # Replace with your bucket\n",
    "    model_display_name: str = \"japanese-absa-model\"\n",
    "    endpoint_display_name: str = \"japanese-absa-endpoint\"\n",
    "    service_account: str = None  # Optional service account\n",
    "    machine_type: str = \"n1-standard-4\"\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\"\n",
    "    accelerator_count: int = 1\n",
    "    use_gpu: bool = True\n",
    "    min_replica_count: int = 1\n",
    "    max_replica_count: int = 3\n",
    "    explanation_config: bool = True\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the ABSA model\"\"\"\n",
    "    n_splits: int = 5\n",
    "    embedding_model_name: str = \"intfloat/multilingual-e5-base\"\n",
    "    optuna_trials: int = 30\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 64\n",
    "    max_text_length: int = 512\n",
    "    vertex_embedding_model: str = \"textembedding-gecko-multilingual@001\"\n",
    "\n",
    "@dataclass\n",
    "class AspectConfig:\n",
    "    \"\"\"Configuration for aspect categories\"\"\"\n",
    "    aspects: Dict[str, List[str]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.aspects is None:\n",
    "            self.aspects = {\n",
    "                'quality': ['å“è³ª', 'è³ª', 'è‰¯ã„', 'æ‚ªã„', 'é«˜å“è³ª', 'ä½Žå“è³ª', 'ã‚¯ã‚ªãƒªãƒ†ã‚£', 'å“è³ªç®¡ç†'],\n",
    "                'service': ['ã‚µãƒ¼ãƒ“ã‚¹', 'å¯¾å¿œ', 'æŽ¥å®¢', 'è¦ªåˆ‡', 'ä¸å¯§', 'æ…‹åº¦', 'ã‚¹ã‚¿ãƒƒãƒ•', 'åº—å“¡'],\n",
    "                'price': ['ä¾¡æ ¼', 'å€¤æ®µ', 'æ–™é‡‘', 'å®‰ã„', 'é«˜ã„', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼è¨­å®š'],\n",
    "                'convenience': ['ä¾¿åˆ©', 'ä¸ä¾¿', 'ç°¡å˜', 'é›£ã—ã„', 'ä½¿ã„ã‚„ã™ã„', 'ä½¿ã„ã«ãã„', 'ã‚¢ã‚¯ã‚»ã‚¹'],\n",
    "                'speed': ['é€Ÿã„', 'é…ã„', 'æ—©ã„', 'ã‚¹ãƒ”ãƒ¼ãƒ‰', 'è¿…é€Ÿ', 'æ™‚é–“', 'å¾…ã¡æ™‚é–“'],\n",
    "                'atmosphere': ['é›°å›²æ°—', 'ç’°å¢ƒ', 'ç©ºé–“', 'å±…å¿ƒåœ°', 'å¿«é©', 'ä¸å¿«', 'æ¸…æ½”'],\n",
    "                'taste': ['å‘³', 'ç¾Žå‘³ã—ã„', 'ã¾ãšã„', 'ç¾Žå‘³', 'é¢¨å‘³', 'é£Ÿæ„Ÿ', 'æ–°é®®'],\n",
    "                'design': ['ãƒ‡ã‚¶ã‚¤ãƒ³', 'è¦‹ãŸç›®', 'å¤–è¦³', 'ãŠã—ã‚ƒã‚Œ', 'ã‹ã£ã“ã„ã„', 'ç¾Žã—ã„']\n",
    "            }\n",
    "\n",
    "class VertexAICustomPredictor:\n",
    "    \"\"\"Custom predictor for Vertex AI endpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, model_artifact_path: str):\n",
    "        self.model_artifact_path = model_artifact_path\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.vertex_config = None\n",
    "        self.model_config = None\n",
    "        self.aspect_config = None\n",
    "        self.embedding_extractor = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model artifacts\"\"\"\n",
    "        try:\n",
    "            with open(self.model_artifact_path, 'rb') as f:\n",
    "                artifacts = pickle.load(f)\n",
    "                \n",
    "            self.model = artifacts['model']\n",
    "            self.scaler = artifacts['scaler']\n",
    "            self.feature_columns = artifacts['feature_columns']\n",
    "            self.vertex_config = artifacts['vertex_config']\n",
    "            self.model_config = artifacts['model_config']\n",
    "            self.aspect_config = artifacts['aspect_config']\n",
    "            \n",
    "            # Initialize embedding extractor\n",
    "            self.embedding_extractor = VertexAIEmbeddingExtractor(\n",
    "                self.vertex_config, self.model_config\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Model artifacts loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model artifacts: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, instances: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Predict sentiment and aspects for input instances\"\"\"\n",
    "        try:\n",
    "            # Extract texts from instances\n",
    "            texts = [instance.get('text', '') for instance in instances]\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame({'review_text': texts})\n",
    "            \n",
    "            # Extract features\n",
    "            df = self._extract_features(df)\n",
    "            \n",
    "            # Prepare features\n",
    "            X = df[self.feature_columns].values\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.model.predict(X)\n",
    "            probabilities = self.model.predict_proba(X)\n",
    "            \n",
    "            # Format results\n",
    "            results = []\n",
    "            label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "            \n",
    "            for i, (pred, proba) in enumerate(zip(predictions, probabilities)):\n",
    "                # Detect aspects\n",
    "                aspects = self._detect_aspects(texts[i])\n",
    "                \n",
    "                result = {\n",
    "                    'predicted_sentiment': label_map[pred],\n",
    "                    'sentiment_score': int(pred),\n",
    "                    'confidence': float(np.max(proba)),\n",
    "                    'probabilities': {\n",
    "                        'negative': float(proba[0]),\n",
    "                        'neutral': float(proba[1]),\n",
    "                        'positive': float(proba[2])\n",
    "                    },\n",
    "                    'detected_aspects': aspects,\n",
    "                    'text': texts[i]\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract features from text data\"\"\"\n",
    "        # Add text statistics\n",
    "        df['text_length'] = df['review_text'].str.len()\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "        df['question_count'] = df['review_text'].str.count('?')\n",
    "        \n",
    "        # Extract aspect features\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            df[f'aspect_{aspect}'] = df['review_text'].str.contains(\n",
    "                pattern, na=False, regex=True\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_extractor.get_embeddings(df['review_text'].tolist())\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        \n",
    "        for i in range(embedding_dim):\n",
    "            df[f'emb_{i}'] = embeddings[:, i]\n",
    "        \n",
    "        # Scale embeddings\n",
    "        if self.scaler is not None:\n",
    "            emb_cols = [f'emb_{i}' for i in range(embedding_dim)]\n",
    "            df[emb_cols] = self.scaler.transform(df[emb_cols])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _detect_aspects(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect aspects mentioned in text\"\"\"\n",
    "        detected_aspects = []\n",
    "        \n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "            if re.search(pattern, text):\n",
    "                detected_aspects.append(aspect)\n",
    "        \n",
    "        return detected_aspects\n",
    "\n",
    "class VertexAIEmbeddingExtractor:\n",
    "    \"\"\"Vertex AI embedding extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, model_config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.model_config = model_config\n",
    "        self.embedding_model = None\n",
    "        self._initialize_vertex_ai()\n",
    "    \n",
    "    def _initialize_vertex_ai(self):\n",
    "        \"\"\"Initialize Vertex AI\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "                self.model_config.vertex_embedding_model\n",
    "            )\n",
    "            logger.info(f\"Initialized Vertex AI embedding model: {self.model_config.vertex_embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize Vertex AI embeddings: {e}\")\n",
    "            logger.info(\"Falling back to SentenceTransformer\")\n",
    "            self.embedding_model = SentenceTransformer(self.model_config.embedding_model_name)\n",
    "    \n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings from Vertex AI or fallback model\"\"\"\n",
    "        try:\n",
    "            if isinstance(self.embedding_model, TextEmbeddingModel):\n",
    "                # Use Vertex AI embeddings\n",
    "                embeddings = []\n",
    "                batch_size = 5  # Vertex AI has rate limits\n",
    "                \n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
    "                    embeddings.extend([emb.values for emb in batch_embeddings])\n",
    "                \n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                # Use SentenceTransformer as fallback\n",
    "                return self.embedding_model.encode(\n",
    "                    texts,\n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=self.model_config.batch_size,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VertexAIDataGenerator:\n",
    "    \"\"\"Generate training data using Vertex AI's generative models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VertexAIConfig, aspect_config: AspectConfig):\n",
    "        self.config = config\n",
    "        self.aspect_config = aspect_config\n",
    "        self.generative_model = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize generative model\"\"\"\n",
    "        try:\n",
    "            vertexai.init(project=self.config.project_id, location=self.config.location)\n",
    "            self.generative_model = GenerativeModel(\"gemini-pro\")\n",
    "            logger.info(\"Initialized Vertex AI generative model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize generative model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Generate training data using Vertex AI\"\"\"\n",
    "        logger.info(f\"Generating {num_samples} training samples using Vertex AI...\")\n",
    "        \n",
    "        # Create prompts for different aspects and sentiments\n",
    "        prompts = self._create_generation_prompts()\n",
    "        \n",
    "        generated_data = []\n",
    "        samples_per_prompt = num_samples // len(prompts)\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"Generating data for prompt {i+1}/{len(prompts)}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate text using Vertex AI\n",
    "                response = self.generative_model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config={\n",
    "                        \"max_output_tokens\": 2048,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.8,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Parse response and create samples\n",
    "                samples = self._parse_generated_response(response.text, samples_per_prompt)\n",
    "                generated_data.extend(samples)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating data for prompt {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(generated_data)\n",
    "        \n",
    "        # Add some manual examples to ensure quality\n",
    "        manual_samples = self._create_manual_samples()\n",
    "        manual_df = pd.DataFrame(manual_samples)\n",
    "        \n",
    "        df = pd.concat([df, manual_df], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Generated {len(df)} training samples\")\n",
    "        return df\n",
    "    \n",
    "    def _create_generation_prompts(self) -> List[str]:\n",
    "        \"\"\"Create prompts for data generation\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        # Create prompts for each aspect and sentiment combination\n",
    "        for aspect, keywords in self.aspect_config.aspects.items():\n",
    "            for sentiment in ['positive', 'negative', 'neutral']:\n",
    "                prompt = f\"\"\"\n",
    "Generate 20 realistic Japanese customer reviews about {aspect} ({', '.join(keywords[:3])}) \n",
    "with {sentiment} sentiment. Each review should be 20-100 characters long.\n",
    "\n",
    "Format each review as:\n",
    "Review: [Japanese text]\n",
    "Sentiment: {sentiment}\n",
    "Aspect: {aspect}\n",
    "\n",
    "Example:\n",
    "Review: ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚\n",
    "Sentiment: positive\n",
    "Aspect: quality\n",
    "\n",
    "Generate 20 similar reviews:\n",
    "\"\"\"\n",
    "                prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def _parse_generated_response(self, response_text: str, max_samples: int) -> List[Dict]:\n",
    "        \"\"\"Parse generated response into structured data\"\"\"\n",
    "        samples = []\n",
    "        lines = response_text.split('\\n')\n",
    "        \n",
    "        current_review = None\n",
    "        current_sentiment = None\n",
    "        current_aspect = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('Review:'):\n",
    "                current_review = line.replace('Review:', '').strip()\n",
    "            elif line.startswith('Sentiment:'):\n",
    "                current_sentiment = line.replace('Sentiment:', '').strip()\n",
    "            elif line.startswith('Aspect:'):\n",
    "                current_aspect = line.replace('Aspect:', '').strip()\n",
    "                \n",
    "                # If we have all three components, create a sample\n",
    "                if current_review and current_sentiment and current_aspect:\n",
    "                    # Map sentiment to numeric\n",
    "                    sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "                    \n",
    "                    samples.append({\n",
    "                        'review_text': current_review,\n",
    "                        'sentiment': sentiment_map.get(current_sentiment, 1),\n",
    "                        'aspect': current_aspect,\n",
    "                        'text_length': len(current_review),\n",
    "                        'generated': True\n",
    "                    })\n",
    "                    \n",
    "                    # Reset for next sample\n",
    "                    current_review = None\n",
    "                    current_sentiment = None\n",
    "                    current_aspect = None\n",
    "                    \n",
    "                    if len(samples) >= max_samples:\n",
    "                        break\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _create_manual_samples(self) -> List[Dict]:\n",
    "        \"\"\"Create high-quality manual samples\"\"\"\n",
    "        manual_samples = [\n",
    "            # Quality - Positive\n",
    "            {'review_text': 'ã“ã®å•†å“ã®å“è³ªã¯æœŸå¾…ä»¥ä¸Šã§ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'é«˜å“è³ªãªææ–™ã‚’ä½¿ç”¨ã—ã¦ã„ã¦æº€è¶³ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            {'review_text': 'ä½œã‚ŠãŒã—ã£ã‹ã‚Šã—ã¦ã„ã¦è‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'quality'},\n",
    "            \n",
    "            # Quality - Negative\n",
    "            {'review_text': 'å“è³ªãŒæ‚ªãã¦ãŒã£ã‹ã‚Šã—ã¾ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'å®‰ã£ã½ã„ææ–™ã§ä½œã‚‰ã‚Œã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            {'review_text': 'ã‚¯ã‚ªãƒªãƒ†ã‚£ãŒä½Žã™ãŽã¦ä½¿ã„ç‰©ã«ãªã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'quality'},\n",
    "            \n",
    "            # Service - Positive\n",
    "            {'review_text': 'ã‚¹ã‚¿ãƒƒãƒ•ã®å¯¾å¿œãŒç´ æ™´ã‚‰ã—ã‹ã£ãŸã§ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'è¦ªåˆ‡ã§ä¸å¯§ãªæŽ¥å®¢ã«æ„Ÿè¬ã—ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ãŒè‰¯ãã¦æ°—æŒã¡ã‚ˆãåˆ©ç”¨ã§ãã¾ã—ãŸã€‚', 'sentiment': 2, 'aspect': 'service'},\n",
    "            \n",
    "            # Service - Negative\n",
    "            {'review_text': 'åº—å“¡ã®æ…‹åº¦ãŒæ‚ªãã¦ä¸å¿«ã§ã—ãŸã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'ã‚µãƒ¼ãƒ“ã‚¹ã®è³ªãŒä½Žãã¦æ®‹å¿µã§ã™ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            {'review_text': 'æŽ¥å®¢ãŒé›‘ã§äºŒåº¦ã¨æ¥ãŸãã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'service'},\n",
    "            \n",
    "            # Price - Positive\n",
    "            {'review_text': 'ä¾¡æ ¼ãŒå®‰ãã¦ãŠå¾—æ„ŸãŒã‚ã‚Šã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãŒè‰¯ã„å•†å“ã§ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            {'review_text': 'é©æ­£ä¾¡æ ¼ã§æº€è¶³ã—ã¦ã„ã¾ã™ã€‚', 'sentiment': 2, 'aspect': 'price'},\n",
    "            \n",
    "            # Price - Negative\n",
    "            {'review_text': 'å€¤æ®µãŒé«˜ã™ãŽã¦æ‰‹ãŒå‡ºã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ä¾¡æ ¼è¨­å®šãŒä¸é©åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            {'review_text': 'ã‚³ã‚¹ãƒˆãŒé«˜ãã¦ç¶šã‘ã‚‰ã‚Œã¾ã›ã‚“ã€‚', 'sentiment': 0, 'aspect': 'price'},\n",
    "            \n",
    "            # Neutral samples\n",
    "            {'review_text': 'æ™®é€šã®å•†å“ã ã¨æ€ã„ã¾ã™ã€‚', 'sentiment': 1, 'aspect': 'quality'},\n",
    "            {'review_text': 'ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚', 'sentiment': 1, 'aspect': 'service'},\n",
    "            {'review_text': 'æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã®å•†å“ã§ã™ã€‚', 'sentiment': 1, 'aspect': 'price'},\n",
    "        ]\n",
    "        \n",
    "        # Add text_length and generated flag\n",
    "        for sample in manual_samples:\n",
    "            sample['text_length'] = len(sample['review_text'])\n",
    "            sample['generated'] = False\n",
    "        \n",
    "        return manual_samples\n",
    "\n",
    "class EnhancedBusinessInsightExtractor:\n",
    "    \"\"\"Enhanced business insight extractor with better analytics\"\"\"\n",
    "    \n",
    "    def __init__(self, aspect_config: AspectConfig = None, label_map: Dict[int, str] = None):\n",
    "        self.aspect_config = aspect_config or AspectConfig()\n",
    "        self.label_map = label_map or {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.colors = {\n",
    "            'negative': '#FF6B6B',\n",
    "            'neutral': '#FFD93D', \n",
    "            'positive': '#6BCF7F'\n",
    "        }\n",
    "        \n",
    "    def calculate_aspect_metrics(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate detailed metrics for each aspect\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for aspect in self.aspect_config.aspects.keys():\n",
    "            aspect_col = f'aspect_{aspect}'\n",
    "            if aspect_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            aspect_data = df[df[aspect_col] == 1]\n",
    "            if len(aspect_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            total_mentions = len(aspect_data)\n",
    "            sentiment_counts = aspect_data['sentiment'].value_counts()\n",
    "            \n",
    "            metrics[aspect] = {\n",
    "                'total_mentions': total_mentions,\n",
    "                'negative_count': sentiment_counts.get(0, 0),\n",
    "                'neutral_count': sentiment_counts.get(1, 0),\n",
    "                'positive_count': sentiment_counts.get(2, 0),\n",
    "                'negative_rate': sentiment_counts.get(0, 0) / total_mentions * 100,\n",
    "                'neutral_rate': sentiment_counts.get(1, 0) / total_mentions * 100,\n",
    "                'positive_rate': sentiment_counts.get(2, 0) / total_mentions * 100,\n",
    "                'sentiment_score': (sentiment_counts.get(2, 0) - sentiment_counts.get(0, 0)) / total_mentions\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def generate_business_recommendations(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate actionable business recommendations\"\"\"\n",
    "        metrics = self.calculate_aspect_metrics(df)\n",
    "        recommendations = []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Sort aspects by priority (negative rate * mentions)\n",
    "        priority_aspects = []\n",
    "        for aspect, data in metrics.items():\n",
    "            priority_score = data['negative_rate'] * np.log(data['total_mentions'] + 1)\n",
    "            priority_aspects.append((aspect, priority_score, data))\n",
    "            \n",
    "        priority_aspects.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ASPECT PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        for aspect, _, data in priority_aspects:\n",
    "            print(f\"â€¢ {aspect.upper()}: {data['total_mentions']} mentions\")\n",
    "            print(f\"  â”œâ”€ Negative: {data['negative_rate']:.1f}% ({data['negative_count']} reviews)\")\n",
    "            print(f\"  â”œâ”€ Neutral:  {data['neutral_rate']:.1f}% ({data['neutral_count']} reviews)\")\n",
    "            print(f\"  â””â”€ Positive: {data['positive_rate']:.1f}% ({data['positive_count']} reviews)\")\n",
    "            \n",
    "            # Generate specific recommendations\n",
    "            if data['negative_rate'] > 30:\n",
    "                recommendations.append(f\"ðŸ”´ URGENT: Address {aspect} issues - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['negative_rate'] > 20:\n",
    "                recommendations.append(f\"ðŸŸ¡ ATTENTION: Monitor {aspect} - {data['negative_rate']:.1f}% negative feedback\")\n",
    "            elif data['positive_rate'] > 70:\n",
    "                recommendations.append(f\"ðŸŸ¢ STRENGTH: Leverage {aspect} success - {data['positive_rate']:.1f}% positive feedback\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(recommendations[:5], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "            \n",
    "        return {\n",
    "            \"metrics\": metrics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"priority_aspects\": priority_aspects\n",
    "        }\n",
    "    \n",
    "    def executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate executive summary with key metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_reviews = len(df)\n",
    "        sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        overall_sentiment_score = (\n",
    "            sentiment_dist.get(2, 0) - sentiment_dist.get(0, 0)\n",
    "        ) / 100\n",
    "        \n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Total Reviews Analyzed: {total_reviews:,}\")\n",
    "        print(f\"ðŸ“Š Overall Sentiment Score: {overall_sentiment_score:.3f} (-1 to +1)\")\n",
    "        print(f\"ðŸ“ Average Review Length: {avg_text_length:.0f} characters\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "        for sentiment in [0, 1, 2]:\n",
    "            label = self.label_map[sentiment]\n",
    "            pct = sentiment_dist.get(sentiment, 0)\n",
    "            bar_length = int(pct / 2)  # Scale for display\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"  {label.capitalize():>8}: {pct:5.1f}% |{bar}|\")\n",
    "        \n",
    "        return {\n",
    "            \"total_reviews\": total_reviews,\n",
    "            \"sentiment_distribution\": sentiment_dist.to_dict(),\n",
    "            \"overall_sentiment_score\": overall_sentiment_score,\n",
    "            \"avg_text_length\": avg_text_length\n",
    "        }\n",
    "\n",
    "class VertexAIModelDeployer:\n",
    "    \"\"\"Deploy trained model to Vertex AI endpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, vertex_config: VertexAIConfig):\n",
    "        self.vertex_config = vertex_config\n",
    "        self.model = None\n",
    "        self.endpoint = None\n",
    "        \n",
    "    def create_custom_container_image(self, model_path: str) -> str:\n",
    "        \"\"\"Create custom container image for model serving\"\"\"\n",
    "        \n",
    "        # Dockerfile content\n",
    "        dockerfile_content = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    g++ \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model artifacts and predictor\n",
    "COPY model_artifacts.pkl .\n",
    "COPY predictor.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Set environment variables\n",
    "ENV AIP_STORAGE_URI=/app\n",
    "ENV AIP_HEALTH_ROUTE=/health\n",
    "ENV AIP_PREDICT_ROUTE=/predict\n",
    "\n",
    "# Start the server\n",
    "CMD [\"python\", \"predictor.py\"]\n",
    "\"\"\"\n",
    "        \n",
    "        # Requirements.txt content\n",
    "        requirements_content = \"\"\"\n",
    "google-cloud-aiplatform==1.38.0\n",
    "xgboost==1.7.6\n",
    "scikit-learn==1.3.0\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "sentence-transformers==2.2.2\n",
    "vertexai==1.38.0\n",
    "flask==2.3.2\n",
    "\"\"\"\n",
    "        \n",
    "        # Predictor.py content\n",
    "        predictor_content = \"\"\"\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "from google.cloud import aiplatform\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global predictor instance\n",
    "predictor = None\n",
    "\n",
    "def load_predictor():\n",
    "    global predictor\n",
    "    try:\n",
    "        from vertex_ai_absa_pipeline import VertexAICustomPredictor\n",
    "        predictor = VertexAICustomPredictor('model_artifacts.pkl')\n",
    "        logger.info(\"Predictor loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load predictor: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Parse request\n",
    "        data = request.get_json()\n",
    "        instances = data.get('instances', [])\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = predictor.predict(instances)\n",
    "        \n",
    "        return jsonify({\"predictions\": predictions}), 200\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_predictor()\n",
    "    app.run(host='0.0.0.0', port=8080)\n",
    "\"\"\"\n",
    "        \n",
    "        # Create temporary directory for Docker build\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Write files\n",
    "            with open(os.path.join(temp_dir, 'Dockerfile'), 'w') as f:\n",
    "                f.write(dockerfile_content)\n",
    "            \n",
    "            with open(os.path.join(temp_dir, 'requirements.txt'), 'w') as f:\n",
    "                f.write(requirements_content)\n",
    "            \n",
    "            with open(os.path.join(temp_dir, 'predictor.py'), 'w') as f:\n",
    "                f.write(predictor_content)\n",
    "            \n",
    "            # Copy model artifacts\n",
    "            shutil.copy(model_path, os.path.join(temp_dir, 'model_artifacts.pkl'))\n",
    "            \n",
    "            # Build and push container image\n",
    "            image_uri = f\"gcr.io/{self.vertex_config.project_id}/absa-predictor:latest\"\n",
    "            \n",
    "            logger.info(f\"Building container image: {image_uri}\")\n",
    "            \n",
    "            # Build Docker image (requires Docker installed)\n",
    "            import subprocess\n",
    "            \n",
    "            subprocess.run([\n",
    "                'docker', 'build', '-t', image_uri, temp_dir\n",
    "            ], check=True)\n",
    "            \n",
    "            # Push to registry\n",
    "            subprocess.run([\n",
    "                'docker', 'push', image_uri\n",
    "            ], check=True)\n",
    "            \n",
    "            return image_uri\n",
    "    \n",
    "    def upload_model(self, model_gcs_path: str, container_image_uri: str = None) -> aiplatform.Model:\n",
    "        \"\"\"Upload model to Vertex AI Model Registry\"\"\"\n",
    "\n",
    "        logger.info(f\"Uploading model to Vertex AI Model Registry...\")\n",
    "\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=self.vertex_config.model_display_name,\n",
    "            artifact_uri=os.path.dirname(model_gcs_path),\n",
    "            serving_container_image_uri=container_image_uri or \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "            serving_container_predict_route=\"/predict\",\n",
    "            serving_container_health_route=\"/health\",\n",
    "            project=self.vertex_config.project_id,\n",
    "            location=self.vertex_config.location,\n",
    "            sync=True,\n",
    "        )\n",
    "        logger.info(f\"Model uploaded. Resource name: {model.resource_name}\")\n",
    "        return model\n",
    "\n",
    "    def deploy_model(self, model: aiplatform.Model) -> aiplatform.Endpoint:\n",
    "        \"\"\"Deploy uploaded model to Vertex AI endpoint\"\"\"\n",
    "\n",
    "        logger.info(\"Deploying model to Vertex AI endpoint...\")\n",
    "        deployed_model = model.deploy(\n",
    "            machine_type=self.vertex_config.machine_type,\n",
    "            min_replica_count=self.vertex_config.min_replica_count,\n",
    "            max_replica_count=self.vertex_config.max_replica_count,\n",
    "            accelerator_type=self.vertex_config.accelerator_type if self.vertex_config.use_gpu else None,\n",
    "            accelerator_count=self.vertex_config.accelerator_count if self.vertex_config.use_gpu else None,\n",
    "            traffic_split={\"0\": 100},  # Route all traffic to this model version\n",
    "            sync=True\n",
    "        )\n",
    "        logger.info(f\"Model deployed to endpoint: {deployed_model.resource_name}\")\n",
    "        return deployed_model\n",
    "\n",
    "# ---- Example End-to-End Usage ----\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual settings!\n",
    "    vertex_config = VertexAIConfig(\n",
    "        project_id=\"your-gcp-project\",\n",
    "        location=\"us-central1\",\n",
    "        staging_bucket=\"gs://your-staging-bucket\",\n",
    "        model_display_name=\"japanese-absa-model\",\n",
    "        endpoint_display_name=\"japanese-absa-endpoint\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "        accelerator_count=1,\n",
    "        use_gpu=True,\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=2,\n",
    "        explanation_config=False,\n",
    "    )\n",
    "\n",
    "    # 1. (Assume model was already trained & saved)\n",
    "    model_gcs_path = \"gs://your-staging-bucket/absa_models/model_artifacts.pkl\"\n",
    "    container_image_uri = \"gcr.io/your-gcp-project/absa-predictor:latest\"\n",
    "\n",
    "    # 2. Deploy\n",
    "    deployer = VertexAIModelDeployer(vertex_config)\n",
    "    # Optionally: deployer.create_custom_container_image(model_path)  # If building/pushing container here\n",
    "    model = deployer.upload_model(model_gcs_path, container_image_uri=container_image_uri)\n",
    "    endpoint = deployer.deploy_model(model)\n",
    "\n",
    "    print(f\"\\nâœ… Model deployed! Endpoint resource name:\\n{endpoint.resource_name}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
